{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMg5kKakevGd"
      },
      "source": [
        "# 確認 GPU 類型\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJgt_njFcA4R",
        "outputId": "10b6742a-3be6-46f2-d1f9-5933a1bb5c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device name Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  raise Exception(\"GPU not availalbe. CPU training will be too slow.\")\n",
        "print(\"device name\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kKsDcHu0hlX"
      },
      "source": [
        "# 是否要掛載 Google Drive\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aEAQyCz1950",
        "outputId": "f883fbd4-1202-4946-f2b5-06c1d6d7ddeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYFkur3sEb6f"
      },
      "source": [
        "#確認 ＴＰＵ規格"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCfuY-RMEctt"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O9n-3Ak0rik"
      },
      "source": [
        "# 安裝 transformers,datastes,... 相依套件\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8eh87Hoee5d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install soundfile\n",
        "!pip install jiwer\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!apt install git-lfs\n",
        "!git config --global user.email \"gary109@gmail.com\"\n",
        "!git config --global user.name \"GARY\"\n",
        "!git config --global credential.helper store\n",
        "!pip install wandb\n",
        "!wandb login 2cf656515a3b158f4f603aeba63181236de2fc1b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1JcSRcJ0_uA"
      },
      "source": [
        "# 登入 huggingface \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLI9ee6CkBQg",
        "outputId": "1d486b29-ce88-4249-a01c-2e39d56a5fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens.\n",
            "        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n",
            "        \n",
            "Token: \n",
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ],
      "source": [
        "! huggingface-cli login\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aVUBWempD48"
      },
      "source": [
        "# 下載 orchid219_classification 程式碼\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-gsL86on6AX",
        "outputId": "af97aea5-a817-4b2c-fc89-d275d04beb24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'orchid219_classification'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 78 (delta 36), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (78/78), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://gary109:Gygy844109109@gitlab.com/gary109/orchid219_classification.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E9I4PBFPZsb",
        "outputId": "5b063e39-a75f-429c-a8bf-0e0e1d7c7121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/orchid219_classification\n"
          ]
        }
      ],
      "source": [
        "%cd orchid219_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3eXyafxpNkp"
      },
      "source": [
        "# 載入 orchid219 訓練資料集\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "BDNRJpgbkBTg",
        "outputId": "e3b3dfd4-8806-4d2b-976f-532cfbdc95c6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34f646a9865c4fae9be31445599b137a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.08k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration gary109--orchid219-53e55d447bfb4b23\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset orchid219/orchid219 (download: 86.69 MiB, generated: 85.33 MiB, post-processed: Unknown size, total: 172.02 MiB) to /root/.cache/huggingface/datasets/gary109___parquet/gary109--orchid219-53e55d447bfb4b23/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2df59ce122834786b83d64fd9edfcfea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91642116e07e4c87897560030411493b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/82.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "631e48dece5543e3ae405ecd6a36284e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/8.79M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2da97d306e5c468fb435afce21a322ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/gary109___parquet/gary109--orchid219-53e55d447bfb4b23/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95d96d8873fb417c9167edc93e2a46cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['filename', 'image', 'category'],\n",
              "        num_rows: 1971\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['filename', 'image', 'category'],\n",
              "        num_rows: 219\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "# !cp /content/drive/MyDrive/datasets/orchid219.py /content\n",
        "# dataset = load_dataset(\"./orchid219_classification/datasets/orchid219.py\", use_auth_token=True)\n",
        "# dataset = load_dataset(\"gary109/orchid219\", use_auth_token=True, cache_dir='/content/drive/MyDrive/datasets/cache_orchid219')\n",
        "dataset = load_dataset(\"gary109/orchid219\", use_auth_token=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wUgz4JfEs5p",
        "outputId": "43b75182-9ed6-481a-bc76-4d087046a2b3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7FF538AAB290>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers.utils.dummy_vision_objects import ImageGPTFeatureExtractor\n",
        "import random\n",
        "from PIL import ImageDraw, ImageFont, Image\n",
        "\n",
        "# def show_examples(ds, seed: int = 1234, examples_per_class: int = 3, size=(100, 100)):\n",
        "\n",
        "#     w, h = size\n",
        "#     labels = ds['train'].features['labels'].names\n",
        "#     # labels = labels[:9]\n",
        "#     grid = Image.new('RGB', size=(examples_per_class * w, len(labels) * h))\n",
        "#     draw = ImageDraw.Draw(grid)\n",
        "#     font = ImageFont.truetype(\"./fonts/LiberationMono-Bold.ttf\", 24)\n",
        "#     for label_id, label in enumerate(labels):\n",
        "\n",
        "#         # Filter the dataset by a single label, shuffle it, and grab a few samples\n",
        "#         ds_slice = ds['train'].filter(lambda ex: ex['labels'] == label_id).shuffle(seed).select(range(examples_per_class))\n",
        "\n",
        "#         # Plot this label's examples along a row\n",
        "#         for i, example in enumerate(ds_slice):\n",
        "#             image = example['image']\n",
        "#             idx = examples_per_class * label_id + i\n",
        "#             box = (idx % examples_per_class * w, idx // examples_per_class * h)\n",
        "#             grid.paste(image.resize(size), box=box)\n",
        "#             draw.text(box, str(label), (255, 255, 255), font=font)\n",
        "\n",
        "#     return grid\n",
        "\n",
        "# show_examples(dataset, seed=random.randint(0, 1337), examples_per_class=3)\n",
        "dataset['train'][0]['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRq7IdUXrEUO"
      },
      "outputs": [],
      "source": [
        "# dataset.push_to_hub('gary109/orchid219')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3AZ8EIyzzir"
      },
      "source": [
        "# 安裝加速器\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6_ypCvRkBdI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install accelerate deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MFGLCKC5DHC",
        "outputId": "2f252f09-b3f0-4eda-c6f8-9f7630514a8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0\n",
            "Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 3\n",
            "What is the name of the function in your script that should be launched in all parallel scripts? [main]: \n",
            "How many TPU cores should be used for distributed training? [1]:\n"
          ]
        }
      ],
      "source": [
        "!accelerate config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0G_8Qs65EZZ"
      },
      "outputs": [],
      "source": [
        "!accelerate test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lYnSRfsvhd_"
      },
      "source": [
        "# FOR TPU needs\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94vK4zQPvfGQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip uninstall -y torch\n",
        "!pip install torch==1.8.2+cpu torchvision==0.9.2+cpu -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdZtjk7Uy2e_"
      },
      "source": [
        "# 開始訓練\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YchdiUKFraWJ"
      },
      "outputs": [],
      "source": [
        "# # %cd /content/transformers/examples/pytorch/image-pretraining\n",
        "# !cp /content/drive/MyDrive/datasets/run_mae.py /content\n",
        "# !cp /content/drive/MyDrive/datasets/run_mim.py /content\n",
        "# !cp /content/drive/MyDrive/datasets/run_image_classification.py /content\n",
        "# !cp /content/drive/MyDrive/datasets/run_image_classification_no_trainer.py /content\n",
        "# !cp /content/drive/MyDrive/datasets/run_image_classification_ViT-MAE.py /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW0GTGnQzZnw"
      },
      "source": [
        "## MAE (by Facebook AI).\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5cejFcGAzqd"
      },
      "source": [
        "### model_name_or_path\n",
        "- datasets_name \n",
        "    - [O] crop14-small\n",
        "    - [?] crop14-balance\n",
        "    - [?] crop14-pretrain\n",
        "- model_name_or_path\n",
        "    - [O] google/vit-base-patch16-224-in21k\n",
        "    - [?] google/vit-base-patch16-224-in21k\n",
        "    - [O] google/vit-large-patch16-224-in21k\n",
        "    - [?] google/vit-large-patch32-224-in21k\n",
        "    - [?] google/vit-huge-patch14-224-in21k\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXutErFKr48K"
      },
      "source": [
        "### [facebook/vit-mae-base] ===> orchid219_pretrain_vit-mae-base\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBJgzxj05VzQ",
        "outputId": "2797d651-b395-4557-d542-ea86e82ceea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m串流輸出內容已截斷至最後 5000 行。\u001b[0m\n",
            "                                            {'loss': 0.3944, 'learning_rate': 3.0028262447736833e-05, 'epoch': 131.94}\n",
            " 33% 4092/12400 [1:21:29<1:17:20,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 10:35:30,932 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:35:30,932 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:35:30,932 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.39it/s]\u001b[A\n",
            "                                            \n",
            " 33% 4092/12400 [1:21:33<1:17:20,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "{'eval_loss': 0.3823145925998688, 'eval_runtime': 3.899, 'eval_samples_per_second': 56.168, 'eval_steps_per_second': 1.026, 'epoch': 132.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:35:34,833 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4092\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:35:34,834 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4092/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:35:36,777 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4092/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:35:36,778 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4092/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:35:40,920 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4030] due to args.save_total_limit\n",
            " 33% 4100/12400 [1:21:48<2:10:45,  1.06it/s]{'loss': 0.3861, 'learning_rate': 2.9988275726630026e-05, 'epoch': 132.26}\n",
            " 33% 4110/12400 [1:21:54<1:27:17,  1.58it/s]{'loss': 0.3961, 'learning_rate': 2.994820907574826e-05, 'epoch': 132.58}\n",
            "{'loss': 0.3957, 'learning_rate': 2.990806278005682e-05, 'epoch': 132.9}\n",
            " 33% 4123/12400 [1:22:02<1:17:03,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 10:36:03,806 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:36:03,806 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:36:03,806 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.3889636695384979, 'eval_runtime': 3.8995, 'eval_samples_per_second': 56.161, 'eval_steps_per_second': 1.026, 'epoch': 133.0}\n",
            " 33% 4123/12400 [1:22:06<1:17:03,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:36:07,707 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4123\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:36:07,709 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4123/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:36:09,590 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4123/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:36:09,590 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4123/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:36:13,718 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4061] due to args.save_total_limit\n",
            " 33% 4130/12400 [1:22:21<2:36:24,  1.13s/it]{'loss': 0.399, 'learning_rate': 2.9867837125087434e-05, 'epoch': 133.23}\n",
            "{'loss': 0.3863, 'learning_rate': 2.9827532396936268e-05, 'epoch': 133.55}\n",
            "{'loss': 0.3903, 'learning_rate': 2.9787148882261877e-05, 'epoch': 133.87}\n",
            " 34% 4154/12400 [1:22:35<1:16:16,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 10:36:37,151 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:36:37,151 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:36:37,152 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                            \n",
            " 34% 4154/12400 [1:22:39<1:16:16,  1.80it/s]\n",
            "{'eval_loss': 0.3886607587337494, 'eval_runtime': 3.9141, 'eval_samples_per_second': 55.951, 'eval_steps_per_second': 1.022, 'epoch': 134.0}\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:36:41,067 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4154\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:36:41,069 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4154/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:36:43,096 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4154/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:36:43,097 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4154/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:36:47,414 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4092] due to args.save_total_limit\n",
            "{'loss': 0.3987, 'learning_rate': 2.974668686828316e-05, 'epoch': 134.19}\n",
            "{'loss': 0.389, 'learning_rate': 2.9706146642777334e-05, 'epoch': 134.52}\n",
            "{'loss': 0.3843, 'learning_rate': 2.9665528494077875e-05, 'epoch': 134.84}\n",
            " 34% 4185/12400 [1:23:08<1:15:36,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 10:37:10,436 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:37:10,436 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:37:10,436 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.3819936513900757, 'eval_runtime': 3.8609, 'eval_samples_per_second': 56.723, 'eval_steps_per_second': 1.036, 'epoch': 135.0}\n",
            " 34% 4185/12400 [1:23:13<1:15:36,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:37:14,299 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4185\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:37:14,300 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4185/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:37:16,218 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4185/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:37:16,219 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4185/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:37:20,590 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4123] due to args.save_total_limit\n",
            " 34% 4190/12400 [1:23:26<3:50:24,  1.68s/it]{'loss': 0.4051, 'learning_rate': 2.9624832711072472e-05, 'epoch': 135.16}\n",
            " 34% 4200/12400 [1:23:32<1:31:26,  1.49it/s]{'loss': 0.3922, 'learning_rate': 2.9584059583200956e-05, 'epoch': 135.48}\n",
            " 34% 4210/12400 [1:23:38<1:26:07,  1.58it/s]{'loss': 0.3846, 'learning_rate': 2.9543209400453288e-05, 'epoch': 135.81}\n",
            " 34% 4216/12400 [1:23:42<1:16:15,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 10:37:43,893 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:37:43,893 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:37:43,893 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                            \n",
            " 34% 4216/12400 [1:23:46<1:16:15,  1.79it/s]\n",
            "{'eval_loss': 0.39025095105171204, 'eval_runtime': 3.9138, 'eval_samples_per_second': 55.955, 'eval_steps_per_second': 1.022, 'epoch': 136.0}\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:37:47,808 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4216\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:37:47,810 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4216/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:37:49,746 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4216/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:37:49,747 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4216/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:37:54,048 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4154] due to args.save_total_limit\n",
            "{'loss': 0.3795, 'learning_rate': 2.9502282453367437e-05, 'epoch': 136.13}\n",
            " 34% 4230/12400 [1:24:05<1:28:39,  1.54it/s]{'loss': 0.3873, 'learning_rate': 2.9461279033027354e-05, 'epoch': 136.45}\n",
            "                                            {'loss': 0.3996, 'learning_rate': 2.9420199431060888e-05, 'epoch': 136.77}\n",
            " 34% 4247/12400 [1:24:15<1:14:24,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 10:38:16,811 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:38:16,811 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:38:16,811 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.3835234045982361, 'eval_runtime': 3.8667, 'eval_samples_per_second': 56.637, 'eval_steps_per_second': 1.034, 'epoch': 137.0}\n",
            " 34% 4247/12400 [1:24:19<1:14:24,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:38:20,679 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4247\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:38:20,682 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4247/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:38:22,599 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4247/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:38:22,600 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4247/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:38:26,870 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4185] due to args.save_total_limit\n",
            " 34% 4250/12400 [1:24:31<6:00:04,  2.65s/it]{'loss': 0.3959, 'learning_rate': 2.9379043939637706e-05, 'epoch': 137.1}\n",
            " 34% 4260/12400 [1:24:37<1:27:23,  1.55it/s]{'loss': 0.3898, 'learning_rate': 2.933781285146723e-05, 'epoch': 137.42}\n",
            " 34% 4270/12400 [1:24:43<1:20:45,  1.68it/s]{'loss': 0.3814, 'learning_rate': 2.929650645979655e-05, 'epoch': 137.74}\n",
            " 34% 4278/12400 [1:24:47<1:13:31,  1.84it/s][INFO|trainer.py:2550] 2022-05-10 10:38:49,434 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:38:49,434 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:38:49,434 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                            \n",
            " 34% 4278/12400 [1:24:52<1:13:31,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "{'eval_loss': 0.3827582895755768, 'eval_runtime': 3.8816, 'eval_samples_per_second': 56.42, 'eval_steps_per_second': 1.03, 'epoch': 138.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:38:53,317 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4278\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:38:53,320 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4278/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:38:55,210 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4278/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:38:55,211 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4278/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:38:59,747 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4216] due to args.save_total_limit\n",
            "                                            {'loss': 0.4073, 'learning_rate': 2.925512505840832e-05, 'epoch': 138.06}\n",
            "{'loss': 0.3812, 'learning_rate': 2.921366894161871e-05, 'epoch': 138.39}\n",
            "                                            {'loss': 0.3947, 'learning_rate': 2.9172138404275262e-05, 'epoch': 138.71}\n",
            " 35% 4309/12400 [1:25:21<1:13:33,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 10:39:22,718 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:39:22,718 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:39:22,718 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                            \n",
            " 35% 4309/12400 [1:25:25<1:13:33,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.39078399538993835, 'eval_runtime': 3.8957, 'eval_samples_per_second': 56.215, 'eval_steps_per_second': 1.027, 'epoch': 139.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 10:39:26,616 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4309\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:39:26,617 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4309/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:39:28,578 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4309/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:39:28,578 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4309/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:39:32,945 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4247] due to args.save_total_limit\n",
            "{'loss': 0.3914, 'learning_rate': 2.913053374175483e-05, 'epoch': 139.03}\n",
            "{'loss': 0.3918, 'learning_rate': 2.908885524996146e-05, 'epoch': 139.35}\n",
            "{'loss': 0.3941, 'learning_rate': 2.9047103225324313e-05, 'epoch': 139.68}\n",
            "                                            {'loss': 0.3889, 'learning_rate': 2.9005277964795506e-05, 'epoch': 140.0}\n",
            " 35% 4340/12400 [1:25:54<1:12:47,  1.85it/s][INFO|trainer.py:2550] 2022-05-10 10:39:55,919 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:39:55,919 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:39:55,919 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                            \n",
            " 35% 4340/12400 [1:25:58<1:12:47,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "{'eval_loss': 0.37962591648101807, 'eval_runtime': 3.8542, 'eval_samples_per_second': 56.821, 'eval_steps_per_second': 1.038, 'epoch': 140.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:39:59,775 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4340\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:39:59,778 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4340/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:40:01,641 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4340/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:40:01,642 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4340/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:40:06,018 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4278] due to args.save_total_limit\n",
            " 35% 4350/12400 [1:26:15<1:44:48,  1.28it/s]{'loss': 0.3927, 'learning_rate': 2.8963379765848045e-05, 'epoch': 140.32}\n",
            "                                            {'loss': 0.3931, 'learning_rate': 2.89214089264737e-05, 'epoch': 140.65}\n",
            "{'loss': 0.3867, 'learning_rate': 2.8879365745180865e-05, 'epoch': 140.97}\n",
            " 35% 4371/12400 [1:26:27<1:16:45,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 10:40:29,021 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:40:29,021 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:40:29,021 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.38933929800987244, 'eval_runtime': 3.8827, 'eval_samples_per_second': 56.404, 'eval_steps_per_second': 1.03, 'epoch': 141.0}\n",
            " 35% 4371/12400 [1:26:31<1:16:45,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:40:32,906 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4371\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:40:32,910 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4371/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:40:34,835 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4371/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:40:34,836 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4371/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:40:39,189 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4309] due to args.save_total_limit\n",
            " 35% 4380/12400 [1:26:47<1:57:21,  1.14it/s]{'loss': 0.3874, 'learning_rate': 2.8837250520992475e-05, 'epoch': 141.29}\n",
            "{'loss': 0.4025, 'learning_rate': 2.879506355344383e-05, 'epoch': 141.61}\n",
            "                                            {'loss': 0.3871, 'learning_rate': 2.8752805142580508e-05, 'epoch': 141.94}\n",
            " 36% 4402/12400 [1:27:00<1:15:02,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 10:41:02,136 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:41:02,136 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:41:02,136 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                            \n",
            " 36% 4402/12400 [1:27:04<1:15:02,  1.78it/s]\n",
            "{'eval_loss': 0.3843056559562683, 'eval_runtime': 3.9645, 'eval_samples_per_second': 55.24, 'eval_steps_per_second': 1.009, 'epoch': 142.0}\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:41:06,103 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4402\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:41:06,106 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4402/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:41:08,003 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4402/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:41:08,004 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4402/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:41:12,425 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4340] due to args.save_total_limit\n",
            " 36% 4410/12400 [1:27:20<2:06:43,  1.05it/s]{'loss': 0.3858, 'learning_rate': 2.8710475588956185e-05, 'epoch': 142.26}\n",
            " 36% 4420/12400 [1:27:25<1:21:53,  1.62it/s]{'loss': 0.3939, 'learning_rate': 2.866807519363054e-05, 'epoch': 142.58}\n",
            " 36% 4430/12400 [1:27:31<1:17:02,  1.72it/s]{'loss': 0.3928, 'learning_rate': 2.862560425816712e-05, 'epoch': 142.9}\n",
            " 36% 4433/12400 [1:27:33<1:14:24,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 10:41:35,258 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:41:35,258 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:41:35,258 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.38147300481796265, 'eval_runtime': 3.982, 'eval_samples_per_second': 54.998, 'eval_steps_per_second': 1.005, 'epoch': 143.0}\n",
            " 36% 4433/12400 [1:27:37<1:14:24,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:41:39,242 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4433\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:41:39,243 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4433/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:41:41,189 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4433/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:41:41,190 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4433/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:41:45,486 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4371] due to args.save_total_limit\n",
            "{'loss': 0.3865, 'learning_rate': 2.858306308463114e-05, 'epoch': 143.23}\n",
            "{'loss': 0.3969, 'learning_rate': 2.854045197558738e-05, 'epoch': 143.55}\n",
            " 36% 4460/12400 [1:28:04<1:16:30,  1.73it/s]{'loss': 0.3915, 'learning_rate': 2.8497771234098017e-05, 'epoch': 143.87}\n",
            " 36% 4464/12400 [1:28:06<1:13:03,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 10:42:08,396 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:42:08,396 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:42:08,396 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                            \n",
            " 36% 4464/12400 [1:28:11<1:13:03,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "{'eval_loss': 0.38846173882484436, 'eval_runtime': 3.888, 'eval_samples_per_second': 56.327, 'eval_steps_per_second': 1.029, 'epoch': 144.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:42:12,286 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4464\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:42:12,289 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4464/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:42:14,269 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4464/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:42:14,270 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4464/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:42:18,587 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4402] due to args.save_total_limit\n",
            " 36% 4470/12400 [1:28:24<2:55:02,  1.32s/it]{'loss': 0.389, 'learning_rate': 2.8455021163720496e-05, 'epoch': 144.19}\n",
            "{'loss': 0.4049, 'learning_rate': 2.841220206850531e-05, 'epoch': 144.52}\n",
            "{'loss': 0.3863, 'learning_rate': 2.83693142529939e-05, 'epoch': 144.84}\n",
            " 36% 4495/12400 [1:28:40<1:13:50,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 10:42:41,954 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:42:41,954 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:42:41,954 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.55it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.3858371675014496, 'eval_runtime': 3.9046, 'eval_samples_per_second': 56.088, 'eval_steps_per_second': 1.024, 'epoch': 145.0}\n",
            " 36% 4495/12400 [1:28:44<1:13:50,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:42:45,860 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4495\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:42:45,863 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4495/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:42:47,809 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4495/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:42:47,809 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4495/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:42:51,983 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4433] due to args.save_total_limit\n",
            "{'loss': 0.3872, 'learning_rate': 2.8326358022216467e-05, 'epoch': 145.16}\n",
            "{'loss': 0.3885, 'learning_rate': 2.82833336816898e-05, 'epoch': 145.48}\n",
            "{'loss': 0.4026, 'learning_rate': 2.8240241537415086e-05, 'epoch': 145.81}\n",
            " 36% 4526/12400 [1:29:13<1:12:41,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 10:43:14,871 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:43:14,872 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:43:14,872 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.3763074576854706, 'eval_runtime': 3.9235, 'eval_samples_per_second': 55.817, 'eval_steps_per_second': 1.019, 'epoch': 146.0}\n",
            " 36% 4526/12400 [1:29:17<1:12:41,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:43:18,797 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4526\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:43:18,799 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4526/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:43:20,769 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4526/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:43:20,771 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4526/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:43:25,178 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4464] due to args.save_total_limit\n",
            "{'loss': 0.3804, 'learning_rate': 2.8197081895875777e-05, 'epoch': 146.13}\n",
            "                                            {'loss': 0.3944, 'learning_rate': 2.8153855064035367e-05, 'epoch': 146.45}\n",
            "{'loss': 0.3813, 'learning_rate': 2.8110561349335224e-05, 'epoch': 146.77}\n",
            " 37% 4557/12400 [1:29:46<1:12:58,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 10:43:48,672 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:43:48,672 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:43:48,672 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                            \n",
            "{'eval_loss': 0.3783002495765686, 'eval_runtime': 3.9936, 'eval_samples_per_second': 54.838, 'eval_steps_per_second': 1.002, 'epoch': 147.0}\n",
            " 37% 4557/12400 [1:29:51<1:12:58,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:43:52,667 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4557\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:43:52,670 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4557/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:43:54,628 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4557/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:43:54,629 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4557/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:43:59,396 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4495] due to args.save_total_limit\n",
            " 37% 4560/12400 [1:30:03<6:04:39,  2.79s/it]{'loss': 0.3987, 'learning_rate': 2.8067201059692418e-05, 'epoch': 147.1}\n",
            " 37% 4570/12400 [1:30:10<1:31:04,  1.43it/s]{'loss': 0.3956, 'learning_rate': 2.8023774503497515e-05, 'epoch': 147.42}\n",
            " 37% 4580/12400 [1:30:16<1:19:55,  1.63it/s]{'loss': 0.3758, 'learning_rate': 2.7980281989612382e-05, 'epoch': 147.74}\n",
            " 37% 4588/12400 [1:30:20<1:11:19,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 10:44:22,549 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:44:22,549 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:44:22,549 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "                                            \n",
            " 37% 4588/12400 [1:30:25<1:11:19,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]{'eval_loss': 0.38227516412734985, 'eval_runtime': 3.8973, 'eval_samples_per_second': 56.193, 'eval_steps_per_second': 1.026, 'epoch': 148.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:44:26,448 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4588\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:44:26,450 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4588/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:44:28,406 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4588/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:44:28,407 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4588/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:44:32,764 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4526] due to args.save_total_limit\n",
            "                                            {'loss': 0.4005, 'learning_rate': 2.7936723827368003e-05, 'epoch': 148.06}\n",
            "{'loss': 0.3982, 'learning_rate': 2.7893100326562263e-05, 'epoch': 148.39}\n",
            "{'loss': 0.3877, 'learning_rate': 2.784941179745777e-05, 'epoch': 148.71}\n",
            " 37% 4619/12400 [1:30:55<1:09:52,  1.86it/s][INFO|trainer.py:2550] 2022-05-10 10:44:57,646 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:44:57,646 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:44:57,646 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                            \n",
            " 37% 4619/12400 [1:31:00<1:09:52,  1.86it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]{'eval_loss': 0.37992486357688904, 'eval_runtime': 3.9033, 'eval_samples_per_second': 56.106, 'eval_steps_per_second': 1.025, 'epoch': 149.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:45:01,551 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4619\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:45:01,553 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4619/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:45:03,566 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4619/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:45:03,567 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4619/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:45:07,952 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4557] due to args.save_total_limit\n",
            "{'loss': 0.3825, 'learning_rate': 2.78056585507796e-05, 'epoch': 149.03}\n",
            " 37% 4630/12400 [1:31:17<1:33:07,  1.39it/s]{'loss': 0.3984, 'learning_rate': 2.7761840897713156e-05, 'epoch': 149.35}\n",
            "                                            {'loss': 0.3851, 'learning_rate': 2.7717959149901893e-05, 'epoch': 149.68}\n",
            " 38% 4650/12400 [1:31:29<1:10:50,  1.82it/s]{'loss': 0.392, 'learning_rate': 2.767401361944513e-05, 'epoch': 150.0}\n",
            "[INFO|trainer.py:2550] 2022-05-10 10:45:30,979 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:45:30,979 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:45:30,979 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.38491886854171753, 'eval_runtime': 4.0026, 'eval_samples_per_second': 54.714, 'eval_steps_per_second': 0.999, 'epoch': 150.0}\n",
            " 38% 4650/12400 [1:31:33<1:10:50,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:45:34,984 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4650\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:45:34,986 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4650/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:45:36,900 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4650/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:45:36,901 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4650/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:45:41,407 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4588] due to args.save_total_limit\n",
            "{'loss': 0.3938, 'learning_rate': 2.7630004618895826e-05, 'epoch': 150.32}\n",
            " 38% 4670/12400 [1:31:56<1:17:58,  1.65it/s]{'loss': 0.3931, 'learning_rate': 2.7585932461258365e-05, 'epoch': 150.65}\n",
            "                                            {'loss': 0.3935, 'learning_rate': 2.7541797459986324e-05, 'epoch': 150.97}\n",
            " 38% 4681/12400 [1:32:02<1:12:34,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 10:46:04,157 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:46:04,157 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:46:04,157 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.3870408535003662, 'eval_runtime': 3.9239, 'eval_samples_per_second': 55.812, 'eval_steps_per_second': 1.019, 'epoch': 151.0}\n",
            " 38% 4681/12400 [1:32:06<1:12:34,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:46:08,083 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4681\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:46:08,084 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4681/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:46:10,037 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4681/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:46:10,038 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4681/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:46:14,233 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4619] due to args.save_total_limit\n",
            "{'loss': 0.3994, 'learning_rate': 2.7497599928980217e-05, 'epoch': 151.29}\n",
            " 38% 4700/12400 [1:32:28<1:18:14,  1.64it/s]{'loss': 0.3804, 'learning_rate': 2.7453340182585314e-05, 'epoch': 151.61}\n",
            "{'loss': 0.389, 'learning_rate': 2.7409018535589365e-05, 'epoch': 151.94}\n",
            " 38% 4712/12400 [1:32:35<1:12:25,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 10:46:37,468 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:46:37,468 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:46:37,468 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.59it/s]\u001b[A\n",
            "                                            \n",
            " 38% 4712/12400 [1:32:40<1:12:25,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A{'eval_loss': 0.3890523314476013, 'eval_runtime': 4.0141, 'eval_samples_per_second': 54.558, 'eval_steps_per_second': 0.996, 'epoch': 152.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:46:41,484 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4712\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:46:41,486 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4712/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:46:43,409 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4712/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:46:43,410 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4712/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:46:48,031 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4650] due to args.save_total_limit\n",
            " 38% 4720/12400 [1:32:55<2:03:30,  1.04it/s]{'loss': 0.3828, 'learning_rate': 2.736463530322038e-05, 'epoch': 152.26}\n",
            " 38% 4730/12400 [1:33:02<1:20:36,  1.59it/s]{'loss': 0.3965, 'learning_rate': 2.7320190801144388e-05, 'epoch': 152.58}\n",
            "                                            {'loss': 0.4016, 'learning_rate': 2.7275685345463163e-05, 'epoch': 152.9}\n",
            " 38% 4743/12400 [1:33:09<1:12:15,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 10:47:11,383 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:47:11,384 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:47:11,384 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                            \n",
            "{'eval_loss': 0.387823224067688, 'eval_runtime': 3.9893, 'eval_samples_per_second': 54.897, 'eval_steps_per_second': 1.003, 'epoch': 153.0}\n",
            " 38% 4743/12400 [1:33:14<1:12:15,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:47:15,375 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4743\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:47:15,376 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4743/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:47:17,299 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4743/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:47:17,300 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4743/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:47:21,734 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4681] due to args.save_total_limit\n",
            "{'loss': 0.3819, 'learning_rate': 2.7231119252712015e-05, 'epoch': 153.23}\n",
            "{'loss': 0.4027, 'learning_rate': 2.7186492839857524e-05, 'epoch': 153.55}\n",
            " 38% 4770/12400 [1:33:41<1:17:26,  1.64it/s]{'loss': 0.3923, 'learning_rate': 2.7141806424295274e-05, 'epoch': 153.87}\n",
            " 38% 4774/12400 [1:33:43<1:11:04,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 10:47:45,004 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:47:45,004 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:47:45,004 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.59it/s]\u001b[A\n",
            "                                            \n",
            "                                 {'eval_loss': 0.38689756393432617, 'eval_runtime': 3.9474, 'eval_samples_per_second': 55.48, 'eval_steps_per_second': 1.013, 'epoch': 154.0}\n",
            " 38% 4774/12400 [1:33:47<1:11:04,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:47:48,953 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4774\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:47:48,955 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4774/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:47:50,928 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4774/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:47:50,929 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4774/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:47:55,343 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4712] due to args.save_total_limit\n",
            "{'loss': 0.3879, 'learning_rate': 2.7097060323847617e-05, 'epoch': 154.19}\n",
            "                                            {'loss': 0.3872, 'learning_rate': 2.7052254856761374e-05, 'epoch': 154.52}\n",
            "{'loss': 0.3868, 'learning_rate': 2.700739034170564e-05, 'epoch': 154.84}\n",
            " 39% 4805/12400 [1:34:16<1:10:15,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 10:48:18,560 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:48:18,560 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:48:18,560 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.59it/s]\u001b[A\n",
            "                                            \n",
            "{'eval_loss': 0.37955421209335327, 'eval_runtime': 4.0204, 'eval_samples_per_second': 54.473, 'eval_steps_per_second': 0.995, 'epoch': 155.0}\n",
            " 39% 4805/12400 [1:34:21<1:10:15,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:48:22,582 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4805\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:48:22,584 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4805/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:48:24,534 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4805/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:48:24,535 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4805/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:48:28,944 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4743] due to args.save_total_limit\n",
            " 39% 4810/12400 [1:34:34<3:30:35,  1.66s/it]{'loss': 0.3914, 'learning_rate': 2.6962467097769443e-05, 'epoch': 155.16}\n",
            " 39% 4820/12400 [1:34:41<1:19:46,  1.58it/s]{'loss': 0.3863, 'learning_rate': 2.6917485444459516e-05, 'epoch': 155.48}\n",
            " 39% 4830/12400 [1:34:46<1:18:48,  1.60it/s]{'loss': 0.3855, 'learning_rate': 2.687244570169802e-05, 'epoch': 155.81}\n",
            " 39% 4836/12400 [1:34:50<1:10:27,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 10:48:52,126 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:48:52,126 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:48:52,126 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.59it/s]\u001b[A\n",
            "                                            \n",
            " 39% 4836/12400 [1:34:54<1:10:27,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A{'eval_loss': 0.3872509300708771, 'eval_runtime': 3.953, 'eval_samples_per_second': 55.402, 'eval_steps_per_second': 1.012, 'epoch': 156.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:48:56,081 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4836\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:48:56,082 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4836/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:48:58,068 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4836/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:48:58,069 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4836/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:49:02,653 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4774] due to args.save_total_limit\n",
            "                                            {'loss': 0.3957, 'learning_rate': 2.6827348189820264e-05, 'epoch': 156.13}\n",
            "                                            {'loss': 0.3931, 'learning_rate': 2.6782193229572417e-05, 'epoch': 156.45}\n",
            "{'loss': 0.3842, 'learning_rate': 2.673698114210926e-05, 'epoch': 156.77}\n",
            " 39% 4867/12400 [1:35:24<1:09:38,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 10:49:26,522 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:49:26,522 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:49:26,522 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                            \n",
            " 39% 4867/12400 [1:35:29<1:09:38,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.39437413215637207, 'eval_runtime': 3.9857, 'eval_samples_per_second': 54.946, 'eval_steps_per_second': 1.004, 'epoch': 157.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 10:49:30,510 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4867\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:49:30,511 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4867/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:49:32,458 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4867/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:49:32,458 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4867/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:49:36,836 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4805] due to args.save_total_limit\n",
            "{'loss': 0.3924, 'learning_rate': 2.6691712248991844e-05, 'epoch': 157.1}\n",
            "                                            {'loss': 0.3836, 'learning_rate': 2.664638687218527e-05, 'epoch': 157.42}\n",
            "{'loss': 0.3952, 'learning_rate': 2.6601005334056356e-05, 'epoch': 157.74}\n",
            " 40% 4898/12400 [1:35:58<1:08:46,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 10:50:00,119 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:50:00,119 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:50:00,119 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                            \n",
            " 40% 4898/12400 [1:36:02<1:08:46,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]{'eval_loss': 0.3838280141353607, 'eval_runtime': 4.0199, 'eval_samples_per_second': 54.478, 'eval_steps_per_second': 0.995, 'epoch': 158.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:50:04,141 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4898\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:50:04,143 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4898/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:50:06,091 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4898/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:50:06,091 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4898/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:50:10,343 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4836] due to args.save_total_limit\n",
            " 40% 4900/12400 [1:36:14<7:23:07,  3.54s/it]{'loss': 0.39, 'learning_rate': 2.6555567957371345e-05, 'epoch': 158.06}\n",
            "{'loss': 0.402, 'learning_rate': 2.651007506529364e-05, 'epoch': 158.39}\n",
            "{'loss': 0.3976, 'learning_rate': 2.6464526981381458e-05, 'epoch': 158.71}\n",
            " 40% 4929/12400 [1:36:31<1:07:54,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 10:50:33,232 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:50:33,232 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:50:33,232 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                            \n",
            " 40% 4929/12400 [1:36:35<1:07:54,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]{'eval_loss': 0.3865478038787842, 'eval_runtime': 3.93, 'eval_samples_per_second': 55.726, 'eval_steps_per_second': 1.018, 'epoch': 159.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:50:37,164 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4929\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:50:37,165 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4929/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:50:39,108 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4929/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:50:39,109 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4929/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:50:43,470 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4867] due to args.save_total_limit\n",
            " 40% 4930/12400 [1:36:46<10:04:02,  4.85s/it]{'loss': 0.373, 'learning_rate': 2.6418924029585578e-05, 'epoch': 159.03}\n",
            "{'loss': 0.3888, 'learning_rate': 2.6373266534247015e-05, 'epoch': 159.35}\n",
            "{'loss': 0.3971, 'learning_rate': 2.6327554820094693e-05, 'epoch': 159.68}\n",
            "{'loss': 0.3907, 'learning_rate': 2.6281789212243176e-05, 'epoch': 160.0}\n",
            " 40% 4960/12400 [1:37:05<1:08:01,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 10:51:06,486 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:51:06,487 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:51:06,487 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                            \n",
            " 40% 4960/12400 [1:37:09<1:08:01,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]{'eval_loss': 0.37987738847732544, 'eval_runtime': 3.9036, 'eval_samples_per_second': 56.102, 'eval_steps_per_second': 1.025, 'epoch': 160.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:51:10,392 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4960\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:51:10,393 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4960/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:51:12,363 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4960/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:51:12,364 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4960/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:51:16,550 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4898] due to args.save_total_limit\n",
            "{'loss': 0.3852, 'learning_rate': 2.623597003619033e-05, 'epoch': 160.32}\n",
            "{'loss': 0.3987, 'learning_rate': 2.6190097617814997e-05, 'epoch': 160.65}\n",
            "{'loss': 0.3934, 'learning_rate': 2.6144172283374723e-05, 'epoch': 160.97}\n",
            " 40% 4991/12400 [1:37:38<1:11:19,  1.73it/s][INFO|trainer.py:2550] 2022-05-10 10:51:39,972 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:51:39,972 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:51:39,972 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                            \n",
            " 40% 4991/12400 [1:37:42<1:11:19,  1.73it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "{'eval_loss': 0.3807092308998108, 'eval_runtime': 3.8954, 'eval_samples_per_second': 56.22, 'eval_steps_per_second': 1.027, 'epoch': 161.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:51:43,869 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-4991\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:51:43,870 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-4991/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:51:45,825 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-4991/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:51:45,825 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-4991/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:51:50,107 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4929] due to args.save_total_limit\n",
            " 40% 5000/12400 [1:37:58<1:51:20,  1.11it/s]{'loss': 0.3971, 'learning_rate': 2.6098194359503382e-05, 'epoch': 161.29}\n",
            "{'loss': 0.3783, 'learning_rate': 2.6052164173208892e-05, 'epoch': 161.61}\n",
            " 40% 5020/12400 [1:38:11<1:11:03,  1.73it/s]{'loss': 0.3885, 'learning_rate': 2.600608205187087e-05, 'epoch': 161.94}\n",
            " 40% 5022/12400 [1:38:12<1:10:09,  1.75it/s][INFO|trainer.py:2550] 2022-05-10 10:52:13,750 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:52:13,750 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:52:13,751 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.378606915473938, 'eval_runtime': 3.9714, 'eval_samples_per_second': 55.144, 'eval_steps_per_second': 1.007, 'epoch': 162.0}\n",
            " 40% 5022/12400 [1:38:16<1:10:09,  1.75it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:52:17,724 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5022\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:52:17,725 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5022/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:52:19,714 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5022/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:52:19,715 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5022/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:52:24,214 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4960] due to args.save_total_limit\n",
            "                                            {'loss': 0.3902, 'learning_rate': 2.5959948323238316e-05, 'epoch': 162.26}\n",
            " 41% 5040/12400 [1:38:38<1:16:08,  1.61it/s]{'loss': 0.3957, 'learning_rate': 2.5913763315427268e-05, 'epoch': 162.58}\n",
            " 41% 5050/12400 [1:38:43<1:11:35,  1.71it/s]{'loss': 0.3876, 'learning_rate': 2.5867527356918477e-05, 'epoch': 162.9}\n",
            " 41% 5053/12400 [1:38:45<1:09:00,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 10:52:47,122 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:52:47,122 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:52:47,122 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                            \n",
            "{'eval_loss': 0.38227975368499756, 'eval_runtime': 3.9759, 'eval_samples_per_second': 55.081, 'eval_steps_per_second': 1.006, 'epoch': 163.0}\n",
            " 41% 5053/12400 [1:38:49<1:09:00,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:52:51,100 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5053\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:52:51,101 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5053/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:52:53,138 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5053/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:52:53,139 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5053/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:52:57,373 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-4991] due to args.save_total_limit\n",
            "{'loss': 0.3954, 'learning_rate': 2.5821240776555085e-05, 'epoch': 163.23}\n",
            " 41% 5070/12400 [1:39:11<1:24:48,  1.44it/s]{'loss': 0.3894, 'learning_rate': 2.577490390354024e-05, 'epoch': 163.55}\n",
            " 41% 5080/12400 [1:39:17<1:14:56,  1.63it/s]{'loss': 0.3935, 'learning_rate': 2.5728517067434826e-05, 'epoch': 163.87}\n",
            " 41% 5084/12400 [1:39:19<1:08:33,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 10:53:20,900 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:53:20,900 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:53:20,900 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.41it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.38774019479751587, 'eval_runtime': 4.0069, 'eval_samples_per_second': 54.655, 'eval_steps_per_second': 0.998, 'epoch': 164.0}\n",
            " 41% 5084/12400 [1:39:23<1:08:33,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:53:24,909 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5084\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:53:24,910 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5084/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:53:26,871 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5084/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:53:26,872 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5084/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:53:31,122 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5022] due to args.save_total_limit\n",
            "{'loss': 0.3815, 'learning_rate': 2.5682080598155034e-05, 'epoch': 164.19}\n",
            " 41% 5100/12400 [1:39:43<1:16:09,  1.60it/s]{'loss': 0.4004, 'learning_rate': 2.5635594825970097e-05, 'epoch': 164.52}\n",
            "                                            {'loss': 0.394, 'learning_rate': 2.5589060081499883e-05, 'epoch': 164.84}\n",
            " 41% 5115/12400 [1:39:52<1:07:17,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 10:53:54,038 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:53:54,038 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:53:54,038 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.59it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.3882550001144409, 'eval_runtime': 3.9524, 'eval_samples_per_second': 55.41, 'eval_steps_per_second': 1.012, 'epoch': 165.0}\n",
            " 41% 5115/12400 [1:39:56<1:07:17,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:53:57,992 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5115\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:53:57,994 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5115/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:53:59,960 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5115/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:53:59,961 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5115/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:54:04,400 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5053] due to args.save_total_limit\n",
            "{'loss': 0.3824, 'learning_rate': 2.5542476695712567e-05, 'epoch': 165.16}\n",
            " 41% 5130/12400 [1:40:16<1:17:52,  1.56it/s]{'loss': 0.3815, 'learning_rate': 2.5495844999922287e-05, 'epoch': 165.48}\n",
            "                                            {'loss': 0.3998, 'learning_rate': 2.5449165325786772e-05, 'epoch': 165.81}\n",
            " 42% 5146/12400 [1:40:26<1:07:45,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 10:54:27,728 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:54:27,729 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:54:27,729 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                            \n",
            " 42% 5146/12400 [1:40:30<1:07:45,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.38926100730895996, 'eval_runtime': 3.9816, 'eval_samples_per_second': 55.003, 'eval_steps_per_second': 1.005, 'epoch': 166.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 10:54:31,712 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5146\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:54:31,713 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5146/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:54:33,724 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5146/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:54:33,725 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5146/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:54:38,291 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5084] due to args.save_total_limit\n",
            " 42% 5150/12400 [1:40:43<4:14:24,  2.11s/it]{'loss': 0.3935, 'learning_rate': 2.540243800530496e-05, 'epoch': 166.13}\n",
            "{'loss': 0.3979, 'learning_rate': 2.535566337081471e-05, 'epoch': 166.45}\n",
            "                                            {'loss': 0.3788, 'learning_rate': 2.5308841754990355e-05, 'epoch': 166.77}\n",
            " 42% 5177/12400 [1:41:00<1:06:58,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 10:55:01,795 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:55:01,795 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:55:01,795 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.82it/s]\u001b[A\n",
            "                                            \n",
            " 42% 5177/12400 [1:41:04<1:06:58,  1.80it/s]\n",
            "{'eval_loss': 0.38684186339378357, 'eval_runtime': 3.9946, 'eval_samples_per_second': 54.825, 'eval_steps_per_second': 1.001, 'epoch': 167.0}\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:55:05,792 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5177\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:55:05,793 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5177/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:55:07,788 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5177/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:55:07,789 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5177/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:55:12,245 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5115] due to args.save_total_limit\n",
            "{'loss': 0.3794, 'learning_rate': 2.5261973490840376e-05, 'epoch': 167.1}\n",
            " 42% 5190/12400 [1:41:22<1:25:17,  1.41it/s]{'loss': 0.388, 'learning_rate': 2.5215058911705055e-05, 'epoch': 167.42}\n",
            "{'loss': 0.3913, 'learning_rate': 2.5168098351254048e-05, 'epoch': 167.74}\n",
            " 42% 5208/12400 [1:41:33<1:06:09,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 10:55:35,434 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:55:35,434 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:55:35,434 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                            \n",
            " 42% 5208/12400 [1:41:38<1:06:09,  1.81it/s]{'eval_loss': 0.3823331594467163, 'eval_runtime': 3.9952, 'eval_samples_per_second': 54.815, 'eval_steps_per_second': 1.001, 'epoch': 168.0}\n",
            "\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:55:39,431 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5208\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:55:39,433 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5208/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:55:41,378 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5208/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:55:41,379 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5208/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:55:45,888 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5146] due to args.save_total_limit\n",
            "{'loss': 0.4013, 'learning_rate': 2.512109214348406e-05, 'epoch': 168.06}\n",
            "{'loss': 0.385, 'learning_rate': 2.507404062271645e-05, 'epoch': 168.39}\n",
            "{'loss': 0.3968, 'learning_rate': 2.5026944123594863e-05, 'epoch': 168.71}\n",
            " 42% 5239/12400 [1:42:07<1:05:52,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 10:56:09,280 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:56:09,281 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:56:09,281 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                            \n",
            " 42% 5239/12400 [1:42:11<1:05:52,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.38305968046188354, 'eval_runtime': 3.9516, 'eval_samples_per_second': 55.421, 'eval_steps_per_second': 1.012, 'epoch': 169.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 10:56:13,234 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5239\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:56:13,235 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5239/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:56:15,206 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5239/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:56:15,206 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5239/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:56:19,644 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5177] due to args.save_total_limit\n",
            "{'loss': 0.3889, 'learning_rate': 2.4979802981082823e-05, 'epoch': 169.03}\n",
            "{'loss': 0.3979, 'learning_rate': 2.493261753046139e-05, 'epoch': 169.35}\n",
            "{'loss': 0.3758, 'learning_rate': 2.488538810732674e-05, 'epoch': 169.68}\n",
            "                                            {'loss': 0.3927, 'learning_rate': 2.4838115047587815e-05, 'epoch': 170.0}\n",
            " 42% 5270/12400 [1:42:41<1:04:44,  1.84it/s][INFO|trainer.py:2550] 2022-05-10 10:56:42,585 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:56:42,585 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:56:42,586 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.38873907923698425, 'eval_runtime': 3.9825, 'eval_samples_per_second': 54.99, 'eval_steps_per_second': 1.004, 'epoch': 170.0}\n",
            " 42% 5270/12400 [1:42:45<1:04:44,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:56:46,570 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5270\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:56:46,571 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5270/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:56:48,492 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5270/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:56:48,493 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5270/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:56:52,848 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5208] due to args.save_total_limit\n",
            " 43% 5280/12400 [1:43:01<1:32:34,  1.28it/s]{'loss': 0.3874, 'learning_rate': 2.4790798687463884e-05, 'epoch': 170.32}\n",
            "{'loss': 0.3946, 'learning_rate': 2.474343936348221e-05, 'epoch': 170.65}\n",
            "{'loss': 0.3852, 'learning_rate': 2.4696037412475616e-05, 'epoch': 170.97}\n",
            " 43% 5301/12400 [1:43:14<1:07:53,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 10:57:15,968 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:57:15,968 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:57:15,968 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                            {'eval_loss': 0.3791761100292206, 'eval_runtime': 4.0936, 'eval_samples_per_second': 53.498, 'eval_steps_per_second': 0.977, 'epoch': 171.0}\n",
            "\n",
            " 43% 5301/12400 [1:43:18<1:07:53,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:57:20,064 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5301\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:57:20,065 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5301/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:57:22,032 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5301/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:57:22,033 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5301/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:57:26,305 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5239] due to args.save_total_limit\n",
            " 43% 5310/12400 [1:43:34<1:44:21,  1.13it/s]{'loss': 0.3757, 'learning_rate': 2.464859317158008e-05, 'epoch': 171.29}\n",
            "                                            {'loss': 0.4008, 'learning_rate': 2.460110697823239e-05, 'epoch': 171.61}\n",
            " 43% 5330/12400 [1:43:46<1:06:40,  1.77it/s]{'loss': 0.3892, 'learning_rate': 2.4553579170167687e-05, 'epoch': 171.94}\n",
            " 43% 5332/12400 [1:43:47<1:06:56,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 10:57:49,532 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:57:49,533 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:57:49,533 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.3793460726737976, 'eval_runtime': 3.9945, 'eval_samples_per_second': 54.825, 'eval_steps_per_second': 1.001, 'epoch': 172.0}\n",
            " 43% 5332/12400 [1:43:52<1:06:56,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:57:53,529 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5332\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:57:53,530 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5332/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:57:55,496 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5332/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:57:55,497 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5332/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:58:00,011 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5270] due to args.save_total_limit\n",
            "                                            {'loss': 0.3886, 'learning_rate': 2.4506010085417106e-05, 'epoch': 172.26}\n",
            "{'loss': 0.3873, 'learning_rate': 2.4458400062305334e-05, 'epoch': 172.58}\n",
            "                                            {'loss': 0.386, 'learning_rate': 2.441074943944825e-05, 'epoch': 172.9}\n",
            " 43% 5363/12400 [1:44:21<1:05:35,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 10:58:22,910 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:58:22,910 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:58:22,910 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                            \n",
            " 43% 5363/12400 [1:44:25<1:05:35,  1.79it/s]\n",
            "{'eval_loss': 0.37847593426704407, 'eval_runtime': 3.9619, 'eval_samples_per_second': 55.277, 'eval_steps_per_second': 1.01, 'epoch': 173.0}\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:58:26,874 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5363\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:58:26,876 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5363/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:58:28,794 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5363/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:58:28,795 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5363/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:58:33,281 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5301] due to args.save_total_limit\n",
            " 43% 5370/12400 [1:44:39<2:10:39,  1.12s/it]{'loss': 0.3855, 'learning_rate': 2.4363058555750466e-05, 'epoch': 173.23}\n",
            " 43% 5380/12400 [1:44:47<1:16:45,  1.52it/s]{'loss': 0.3992, 'learning_rate': 2.4315327750402935e-05, 'epoch': 173.55}\n",
            "{'loss': 0.3822, 'learning_rate': 2.4267557362880577e-05, 'epoch': 173.87}\n",
            " 44% 5394/12400 [1:44:54<1:05:02,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 10:58:56,511 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:58:56,511 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:58:56,511 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                            \n",
            " 44% 5394/12400 [1:44:59<1:05:02,  1.80it/s]\n",
            "{'eval_loss': 0.382831335067749, 'eval_runtime': 4.0317, 'eval_samples_per_second': 54.32, 'eval_steps_per_second': 0.992, 'epoch': 174.0}\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:59:00,544 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5394\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:59:00,546 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5394/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:59:02,510 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5394/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:59:02,511 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5394/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:59:07,006 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5332] due to args.save_total_limit\n",
            " 44% 5400/12400 [1:45:13<2:35:43,  1.33s/it]{'loss': 0.3873, 'learning_rate': 2.4219747732939805e-05, 'epoch': 174.19}\n",
            "{'loss': 0.3928, 'learning_rate': 2.4171899200616134e-05, 'epoch': 174.52}\n",
            "                                            {'loss': 0.3914, 'learning_rate': 2.412401210622178e-05, 'epoch': 174.84}\n",
            " 44% 5425/12400 [1:45:28<1:04:28,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 10:59:29,986 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 10:59:29,986 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 10:59:29,986 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                            \n",
            " 44% 5425/12400 [1:45:32<1:04:28,  1.80it/s]\n",
            "{'eval_loss': 0.3829290270805359, 'eval_runtime': 3.8784, 'eval_samples_per_second': 56.466, 'eval_steps_per_second': 1.031, 'epoch': 175.0}\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 10:59:33,866 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5425\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 10:59:33,867 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5425/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 10:59:35,821 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5425/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 10:59:35,822 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5425/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 10:59:40,267 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5363] due to args.save_total_limit\n",
            "                                            {'loss': 0.3893, 'learning_rate': 2.4076086790343214e-05, 'epoch': 175.16}\n",
            "                                            {'loss': 0.386, 'learning_rate': 2.402812359383875e-05, 'epoch': 175.48}\n",
            " 44% 5450/12400 [1:45:58<1:12:03,  1.61it/s]{'loss': 0.3826, 'learning_rate': 2.398012285783612e-05, 'epoch': 175.81}\n",
            " 44% 5456/12400 [1:46:01<1:04:34,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:00:03,392 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:00:03,392 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:00:03,392 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.38411465287208557, 'eval_runtime': 3.9823, 'eval_samples_per_second': 54.993, 'eval_steps_per_second': 1.004, 'epoch': 176.0}\n",
            " 44% 5456/12400 [1:46:06<1:04:34,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:00:07,377 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5456\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:00:07,378 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5456/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:00:09,303 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5456/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:00:09,304 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5456/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:00:13,580 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5394] due to args.save_total_limit\n",
            "                                            {'loss': 0.3865, 'learning_rate': 2.393208492373004e-05, 'epoch': 176.13}\n",
            "{'loss': 0.3926, 'learning_rate': 2.38840101331798e-05, 'epoch': 176.45}\n",
            " 44% 5480/12400 [1:46:31<1:11:13,  1.62it/s]{'loss': 0.3911, 'learning_rate': 2.3835898828106814e-05, 'epoch': 176.77}\n",
            " 44% 5487/12400 [1:46:34<1:03:54,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:00:36,501 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:00:36,502 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:00:36,502 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                            \n",
            " 44% 5487/12400 [1:46:39<1:03:54,  1.80it/s]\n",
            "{'eval_loss': 0.3801950216293335, 'eval_runtime': 3.9524, 'eval_samples_per_second': 55.409, 'eval_steps_per_second': 1.012, 'epoch': 177.0}\n",
            "100% 4/4 [00:00<00:00,  8.98it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:00:40,456 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5487\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:00:40,457 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5487/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:00:42,365 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5487/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:00:42,366 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5487/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:00:46,752 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5425] due to args.save_total_limit\n",
            "{'loss': 0.3946, 'learning_rate': 2.378775135069222e-05, 'epoch': 177.1}\n",
            "                                            {'loss': 0.3864, 'learning_rate': 2.3739568043374394e-05, 'epoch': 177.42}\n",
            " 44% 5510/12400 [1:47:03<1:08:56,  1.67it/s]{'loss': 0.3794, 'learning_rate': 2.3691349248846556e-05, 'epoch': 177.74}\n",
            " 44% 5518/12400 [1:47:07<1:02:31,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 11:01:09,567 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:01:09,567 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:01:09,567 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.37427249550819397, 'eval_runtime': 3.9047, 'eval_samples_per_second': 56.086, 'eval_steps_per_second': 1.024, 'epoch': 178.0}\n",
            " 44% 5518/12400 [1:47:12<1:02:31,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:01:13,474 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5518\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:01:13,475 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5518/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:01:15,427 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5518/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:01:15,429 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5518/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:01:19,842 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5456] due to args.save_total_limit\n",
            " 45% 5520/12400 [1:47:24<6:50:32,  3.58s/it]{'loss': 0.3914, 'learning_rate': 2.364309531005433e-05, 'epoch': 178.06}\n",
            "                                            {'loss': 0.3874, 'learning_rate': 2.3594806570193282e-05, 'epoch': 178.39}\n",
            " 45% 5540/12400 [1:47:36<1:10:32,  1.62it/s]{'loss': 0.3861, 'learning_rate': 2.35464833727065e-05, 'epoch': 178.71}\n",
            " 45% 5549/12400 [1:47:41<1:02:32,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 11:01:42,991 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:01:42,991 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:01:42,991 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                            \n",
            "{'eval_loss': 0.37818676233291626, 'eval_runtime': 3.9423, 'eval_samples_per_second': 55.552, 'eval_steps_per_second': 1.015, 'epoch': 179.0}\n",
            " 45% 5549/12400 [1:47:45<1:02:32,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:01:46,935 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5549\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:01:46,937 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5549/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:01:48,860 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5549/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:01:48,861 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5549/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:01:53,585 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5487] due to args.save_total_limit\n",
            " 45% 5550/12400 [1:47:57<9:26:37,  4.96s/it]{'loss': 0.39, 'learning_rate': 2.349812606128215e-05, 'epoch': 179.03}\n",
            "{'loss': 0.3893, 'learning_rate': 2.3449734979851006e-05, 'epoch': 179.35}\n",
            "{'loss': 0.3859, 'learning_rate': 2.3401310472584052e-05, 'epoch': 179.68}\n",
            "{'loss': 0.388, 'learning_rate': 2.3352852883889983e-05, 'epoch': 180.0}\n",
            " 45% 5580/12400 [1:48:15<1:02:34,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:02:16,860 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:02:16,860 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:02:16,860 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.3751426339149475, 'eval_runtime': 3.935, 'eval_samples_per_second': 55.654, 'eval_steps_per_second': 1.017, 'epoch': 180.0}\n",
            " 45% 5580/12400 [1:48:19<1:02:34,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:02:20,797 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5580\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:02:20,799 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5580/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:02:22,730 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5580/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:02:22,731 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5580/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:02:27,162 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5518] due to args.save_total_limit\n",
            " 45% 5590/12400 [1:48:36<1:28:37,  1.28it/s]{'loss': 0.3965, 'learning_rate': 2.330436255841279e-05, 'epoch': 180.32}\n",
            "{'loss': 0.3797, 'learning_rate': 2.3255839841029288e-05, 'epoch': 180.65}\n",
            "                                            {'loss': 0.3945, 'learning_rate': 2.3207285076846677e-05, 'epoch': 180.97}\n",
            " 45% 5611/12400 [1:48:48<1:04:58,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 11:02:50,087 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:02:50,087 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:02:50,087 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                            \n",
            " 45% 5611/12400 [1:48:52<1:04:58,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "{'eval_loss': 0.37704259157180786, 'eval_runtime': 3.9728, 'eval_samples_per_second': 55.125, 'eval_steps_per_second': 1.007, 'epoch': 181.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:02:54,061 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5611\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:02:54,063 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5611/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:02:56,030 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5611/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:02:56,031 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5611/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:03:00,309 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5549] due to args.save_total_limit\n",
            " 45% 5620/12400 [1:49:08<1:40:28,  1.12it/s]{'loss': 0.3918, 'learning_rate': 2.3158698611200096e-05, 'epoch': 181.29}\n",
            "{'loss': 0.3846, 'learning_rate': 2.3110080789650134e-05, 'epoch': 181.61}\n",
            " 45% 5640/12400 [1:49:21<1:03:59,  1.76it/s]{'loss': 0.391, 'learning_rate': 2.3061431957980396e-05, 'epoch': 181.94}\n",
            " 46% 5642/12400 [1:49:21<1:03:48,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:03:23,486 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:03:23,486 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:03:23,486 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                            \n",
            "{'eval_loss': 0.3778311014175415, 'eval_runtime': 4.077, 'eval_samples_per_second': 53.716, 'eval_steps_per_second': 0.981, 'epoch': 182.0}\n",
            " 46% 5642/12400 [1:49:26<1:03:48,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:03:27,565 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5642\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:03:27,567 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5642/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:03:29,484 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5642/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:03:29,485 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5642/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:03:33,916 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5580] due to args.save_total_limit\n",
            "{'loss': 0.3927, 'learning_rate': 2.3012752462195054e-05, 'epoch': 182.26}\n",
            "                                            {'loss': 0.3736, 'learning_rate': 2.296404264851635e-05, 'epoch': 182.58}\n",
            "                                            {'loss': 0.3981, 'learning_rate': 2.291530286338217e-05, 'epoch': 182.9}\n",
            " 46% 5673/12400 [1:49:55<1:02:55,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:03:57,115 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:03:57,116 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:03:57,116 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                            \n",
            " 46% 5673/12400 [1:49:59<1:02:55,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "{'eval_loss': 0.3756818175315857, 'eval_runtime': 4.0295, 'eval_samples_per_second': 54.349, 'eval_steps_per_second': 0.993, 'epoch': 183.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:04:01,147 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5673\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:04:01,149 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5673/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:04:03,084 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5673/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:04:03,086 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5673/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:04:07,501 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5611] due to args.save_total_limit\n",
            " 46% 5680/12400 [1:50:14<2:05:17,  1.12s/it]{'loss': 0.3825, 'learning_rate': 2.2866533453443563e-05, 'epoch': 183.23}\n",
            "{'loss': 0.3919, 'learning_rate': 2.281773476556228e-05, 'epoch': 183.55}\n",
            " 46% 5700/12400 [1:50:27<1:09:33,  1.61it/s]{'loss': 0.3849, 'learning_rate': 2.2768907146808293e-05, 'epoch': 183.87}\n",
            " 46% 5704/12400 [1:50:29<1:02:58,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:04:31,108 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:04:31,108 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:04:31,108 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                            \n",
            " 46% 5704/12400 [1:50:33<1:02:58,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]{'eval_loss': 0.38247430324554443, 'eval_runtime': 3.9302, 'eval_samples_per_second': 55.723, 'eval_steps_per_second': 1.018, 'epoch': 184.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:04:35,040 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5704\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:04:35,042 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5704/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:04:37,036 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5704/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:04:37,037 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5704/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:04:41,355 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5642] due to args.save_total_limit\n",
            "{'loss': 0.4022, 'learning_rate': 2.272005094445735e-05, 'epoch': 184.19}\n",
            "{'loss': 0.3921, 'learning_rate': 2.26711665059885e-05, 'epoch': 184.52}\n",
            " 46% 5730/12400 [1:51:01<1:13:59,  1.50it/s]{'loss': 0.3889, 'learning_rate': 2.2622254179081606e-05, 'epoch': 184.84}\n",
            " 46% 5735/12400 [1:51:03<1:02:42,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:05:05,585 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:05:05,585 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:05:05,585 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.38047322630882263, 'eval_runtime': 3.9334, 'eval_samples_per_second': 55.678, 'eval_steps_per_second': 1.017, 'epoch': 185.0}\n",
            " 46% 5735/12400 [1:51:08<1:02:42,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:05:09,520 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5735\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:05:09,521 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5735/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:05:11,479 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5735/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:05:11,480 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5735/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:05:15,678 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5673] due to args.save_total_limit\n",
            " 46% 5740/12400 [1:51:21<3:02:33,  1.64s/it]{'loss': 0.3834, 'learning_rate': 2.257331431161486e-05, 'epoch': 185.16}\n",
            "                                            {'loss': 0.3905, 'learning_rate': 2.2524347251662375e-05, 'epoch': 185.48}\n",
            " 46% 5760/12400 [1:51:34<1:14:50,  1.48it/s]{'loss': 0.3877, 'learning_rate': 2.2475353347491634e-05, 'epoch': 185.81}\n",
            " 46% 5766/12400 [1:51:37<1:02:34,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:05:39,219 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:05:39,219 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:05:39,219 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                            \n",
            " 46% 5766/12400 [1:51:42<1:02:34,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]{'eval_loss': 0.38273152709007263, 'eval_runtime': 4.0691, 'eval_samples_per_second': 53.821, 'eval_steps_per_second': 0.983, 'epoch': 186.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:05:43,290 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5766\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:05:43,292 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5766/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:05:45,235 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5766/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:05:45,236 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5766/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:05:49,889 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5704] due to args.save_total_limit\n",
            " 47% 5770/12400 [1:51:54<3:53:36,  2.11s/it]{'loss': 0.3896, 'learning_rate': 2.2426332947561047e-05, 'epoch': 186.13}\n",
            "{'loss': 0.3799, 'learning_rate': 2.237728640051747e-05, 'epoch': 186.45}\n",
            " 47% 5790/12400 [1:52:07<1:07:18,  1.64it/s]{'loss': 0.3943, 'learning_rate': 2.2328214055193747e-05, 'epoch': 186.77}\n",
            " 47% 5797/12400 [1:52:11<1:00:36,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:06:12,868 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:06:12,868 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:06:12,868 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                            \n",
            " 47% 5797/12400 [1:52:15<1:00:36,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]{'eval_loss': 0.3874945044517517, 'eval_runtime': 3.956, 'eval_samples_per_second': 55.36, 'eval_steps_per_second': 1.011, 'epoch': 187.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:06:16,826 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5797\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:06:16,828 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5797/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:06:18,766 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5797/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:06:18,767 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5797/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:06:23,191 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5735] due to args.save_total_limit\n",
            "{'loss': 0.3845, 'learning_rate': 2.2279116260606168e-05, 'epoch': 187.1}\n",
            " 47% 5810/12400 [1:52:33<1:16:58,  1.43it/s]{'loss': 0.3923, 'learning_rate': 2.222999336595205e-05, 'epoch': 187.42}\n",
            "{'loss': 0.3863, 'learning_rate': 2.2180845720607228e-05, 'epoch': 187.74}\n",
            " 47% 5828/12400 [1:52:44<1:00:23,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:06:46,572 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:06:46,572 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:06:46,572 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.44it/s]\u001b[A\n",
            "                                            \n",
            " 47% 5828/12400 [1:52:49<1:00:23,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "{'eval_loss': 0.3859885632991791, 'eval_runtime': 3.9361, 'eval_samples_per_second': 55.639, 'eval_steps_per_second': 1.016, 'epoch': 188.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:06:50,510 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5828\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:06:50,511 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5828/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:06:52,503 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5828/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:06:52,504 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5828/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:06:56,720 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5766] due to args.save_total_limit\n",
            "                                            {'loss': 0.3876, 'learning_rate': 2.213167367412356e-05, 'epoch': 188.06}\n",
            " 47% 5840/12400 [1:53:06<1:14:46,  1.46it/s]{'loss': 0.3811, 'learning_rate': 2.208247757622646e-05, 'epoch': 188.39}\n",
            " 47% 5850/12400 [1:53:13<1:04:19,  1.70it/s]{'loss': 0.3943, 'learning_rate': 2.2033257776812402e-05, 'epoch': 188.71}\n",
            " 47% 5859/12400 [1:53:18<59:30,  1.83it/s]  [INFO|trainer.py:2550] 2022-05-10 11:07:19,714 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:07:19,714 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:07:19,714 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.38037505745887756, 'eval_runtime': 3.9521, 'eval_samples_per_second': 55.414, 'eval_steps_per_second': 1.012, 'epoch': 189.0}\n",
            " 47% 5859/12400 [1:53:22<59:30,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:07:23,668 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5859\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:07:23,670 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5859/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:07:25,655 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5859/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:07:25,656 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5859/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:07:30,174 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5797] due to args.save_total_limit\n",
            " 47% 5860/12400 [1:53:33<8:58:14,  4.94s/it]{'loss': 0.386, 'learning_rate': 2.1984014625946435e-05, 'epoch': 189.03}\n",
            "                                            {'loss': 0.3828, 'learning_rate': 2.1934748473859673e-05, 'epoch': 189.35}\n",
            "                                            {'loss': 0.3854, 'learning_rate': 2.1885459670946843e-05, 'epoch': 189.68}\n",
            " 48% 5890/12400 [1:53:51<59:56,  1.81it/s]  {'loss': 0.3964, 'learning_rate': 2.183614856776376e-05, 'epoch': 190.0}\n",
            " 48% 5890/12400 [1:53:52<59:56,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:07:53,386 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:07:53,387 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:07:53,387 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            " 48% 5890/12400 [1:53:56<59:56,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  7.58it/s]{'eval_loss': 0.38511958718299866, 'eval_runtime': 3.9935, 'eval_samples_per_second': 54.839, 'eval_steps_per_second': 1.002, 'epoch': 190.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:07:57,382 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5890\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:07:57,384 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5890/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:07:59,305 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5890/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:07:59,306 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5890/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:08:03,884 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5828] due to args.save_total_limit\n",
            "{'loss': 0.3861, 'learning_rate': 2.178681551502485e-05, 'epoch': 190.32}\n",
            "{'loss': 0.3837, 'learning_rate': 2.173746086360063e-05, 'epoch': 190.65}\n",
            "                                            {'loss': 0.397, 'learning_rate': 2.168808496451527e-05, 'epoch': 190.97}\n",
            " 48% 5921/12400 [1:54:25<1:01:56,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 11:08:26,890 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:08:26,890 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:08:26,890 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                            \n",
            " 48% 5921/12400 [1:54:29<1:01:56,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.3858051598072052, 'eval_runtime': 3.9008, 'eval_samples_per_second': 56.143, 'eval_steps_per_second': 1.025, 'epoch': 191.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 11:08:30,792 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5921\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:08:30,794 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5921/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:08:32,740 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5921/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:08:32,741 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5921/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:08:37,286 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5859] due to args.save_total_limit\n",
            "                                            {'loss': 0.3759, 'learning_rate': 2.163868816894404e-05, 'epoch': 191.29}\n",
            "                                            {'loss': 0.3902, 'learning_rate': 2.1589270828210813e-05, 'epoch': 191.61}\n",
            "{'loss': 0.3918, 'learning_rate': 2.1539833293785612e-05, 'epoch': 191.94}\n",
            " 48% 5952/12400 [1:54:58<1:00:34,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:09:00,228 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:09:00,228 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:09:00,228 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                            \n",
            " 48% 5952/12400 [1:55:02<1:00:34,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]{'eval_loss': 0.3707527816295624, 'eval_runtime': 3.9488, 'eval_samples_per_second': 55.46, 'eval_steps_per_second': 1.013, 'epoch': 192.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:09:04,179 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5952\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:09:04,181 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5952/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:09:06,093 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5952/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:09:06,094 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5952/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:09:10,477 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-1736] due to args.save_total_limit\n",
            "                                            {'loss': 0.3881, 'learning_rate': 2.1490375917282077e-05, 'epoch': 192.26}\n",
            "                                            {'loss': 0.3914, 'learning_rate': 2.144089905045496e-05, 'epoch': 192.58}\n",
            " 48% 5980/12400 [1:55:29<1:02:24,  1.71it/s]{'loss': 0.3839, 'learning_rate': 2.1391403045197646e-05, 'epoch': 192.9}\n",
            " 48% 5983/12400 [1:55:31<59:59,  1.78it/s]  [INFO|trainer.py:2550] 2022-05-10 11:09:33,254 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:09:33,254 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:09:33,254 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                          \n",
            " 48% 5983/12400 [1:55:35<59:59,  1.78it/s]\n",
            "{'eval_loss': 0.3838147819042206, 'eval_runtime': 3.9499, 'eval_samples_per_second': 55.444, 'eval_steps_per_second': 1.013, 'epoch': 193.0}\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:09:37,206 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-5983\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:09:37,207 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-5983/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:09:39,121 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-5983/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:09:39,122 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-5983/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:09:45,903 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5890] due to args.save_total_limit\n",
            " 48% 5990/12400 [1:55:53<2:06:54,  1.19s/it]{'loss': 0.3868, 'learning_rate': 2.1341888253539638e-05, 'epoch': 193.23}\n",
            "{'loss': 0.3917, 'learning_rate': 2.1292355027644035e-05, 'epoch': 193.55}\n",
            "                                            {'loss': 0.3883, 'learning_rate': 2.1242803719805072e-05, 'epoch': 193.87}\n",
            " 48% 6014/12400 [1:56:07<59:16,  1.80it/s]  [INFO|trainer.py:2550] 2022-05-10 11:10:08,985 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:10:08,986 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:10:08,986 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                          \n",
            " 48% 6014/12400 [1:56:11<59:16,  1.80it/s]\n",
            "{'eval_loss': 0.3827399015426636, 'eval_runtime': 3.9838, 'eval_samples_per_second': 54.973, 'eval_steps_per_second': 1.004, 'epoch': 194.0}\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:10:12,971 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6014\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:10:12,973 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6014/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:10:14,910 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6014/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:10:14,911 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6014/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:10:19,488 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5921] due to args.save_total_limit\n",
            "{'loss': 0.3838, 'learning_rate': 2.1193234682445567e-05, 'epoch': 194.19}\n",
            "{'loss': 0.4008, 'learning_rate': 2.114364826811444e-05, 'epoch': 194.52}\n",
            " 49% 6040/12400 [1:56:38<1:03:56,  1.66it/s]{'loss': 0.377, 'learning_rate': 2.109404482948421e-05, 'epoch': 194.84}\n",
            " 49% 6045/12400 [1:56:40<58:30,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:10:42,457 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:10:42,458 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:10:42,458 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                          \n",
            " 49% 6045/12400 [1:56:45<58:30,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "{'eval_loss': 0.38253679871559143, 'eval_runtime': 3.9504, 'eval_samples_per_second': 55.437, 'eval_steps_per_second': 1.013, 'epoch': 195.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:10:46,410 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6045\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:10:46,411 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6045/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:10:48,346 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6045/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:10:48,347 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6045/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:10:55,304 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5983] due to args.save_total_limit\n",
            "{'loss': 0.3827, 'learning_rate': 2.1044424719348467e-05, 'epoch': 195.16}\n",
            " 49% 6060/12400 [1:57:06<1:07:01,  1.58it/s]{'loss': 0.3937, 'learning_rate': 2.0994788290619388e-05, 'epoch': 195.48}\n",
            "{'loss': 0.3879, 'learning_rate': 2.0945135896325182e-05, 'epoch': 195.81}\n",
            " 49% 6076/12400 [1:57:16<59:06,  1.78it/s]  [INFO|trainer.py:2550] 2022-05-10 11:11:18,420 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:11:18,420 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:11:18,420 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            " 49% 6076/12400 [1:57:21<59:06,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A{'eval_loss': 0.3805723786354065, 'eval_runtime': 3.9369, 'eval_samples_per_second': 55.627, 'eval_steps_per_second': 1.016, 'epoch': 196.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:11:22,359 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6076\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:11:22,360 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6076/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:11:24,297 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6076/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:11:24,298 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6076/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:11:28,855 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6014] due to args.save_total_limit\n",
            " 49% 6080/12400 [1:57:34<3:41:10,  2.10s/it]{'loss': 0.3854, 'learning_rate': 2.0895467889607645e-05, 'epoch': 196.13}\n",
            "                                            {'loss': 0.3871, 'learning_rate': 2.0845784623719598e-05, 'epoch': 196.45}\n",
            "{'loss': 0.3928, 'learning_rate': 2.079608645202238e-05, 'epoch': 196.77}\n",
            " 49% 6107/12400 [1:57:50<57:44,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:11:52,330 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:11:52,330 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:11:52,330 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.37969160079956055, 'eval_runtime': 3.9378, 'eval_samples_per_second': 55.615, 'eval_steps_per_second': 1.016, 'epoch': 197.0}\n",
            " 49% 6107/12400 [1:57:55<57:44,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:11:56,270 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6107\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:11:56,271 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6107/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:11:58,195 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6107/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:11:58,196 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6107/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:12:02,529 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6045] due to args.save_total_limit\n",
            "{'loss': 0.3855, 'learning_rate': 2.0746373727983362e-05, 'epoch': 197.1}\n",
            "                                            {'loss': 0.3909, 'learning_rate': 2.0696646805173403e-05, 'epoch': 197.42}\n",
            "{'loss': 0.3889, 'learning_rate': 2.064690603726435e-05, 'epoch': 197.74}\n",
            " 50% 6138/12400 [1:58:23<57:36,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:12:25,542 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:12:25,543 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:12:25,543 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                          \n",
            " 50% 6138/12400 [1:58:28<57:36,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.3804059326648712, 'eval_runtime': 3.9759, 'eval_samples_per_second': 55.082, 'eval_steps_per_second': 1.006, 'epoch': 198.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 11:12:29,520 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6138\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:12:29,522 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6138/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:12:31,501 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6138/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:12:31,501 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6138/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:12:36,096 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6076] due to args.save_total_limit\n",
            "{'loss': 0.3881, 'learning_rate': 2.059715177802653e-05, 'epoch': 198.06}\n",
            "{'loss': 0.3867, 'learning_rate': 2.05473843813262e-05, 'epoch': 198.39}\n",
            "                                            {'loss': 0.3865, 'learning_rate': 2.049760420112309e-05, 'epoch': 198.71}\n",
            " 50% 6169/12400 [1:58:57<57:07,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:12:59,207 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:12:59,207 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:12:59,207 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.36it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3802568316459656, 'eval_runtime': 3.982, 'eval_samples_per_second': 54.998, 'eval_steps_per_second': 1.005, 'epoch': 199.0}\n",
            " 50% 6169/12400 [1:59:01<57:07,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:13:03,191 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6169\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:13:03,192 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6169/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:13:05,273 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6169/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:13:05,274 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6169/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:13:10,146 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6107] due to args.save_total_limit\n",
            " 50% 6170/12400 [1:59:15<9:55:47,  5.74s/it]{'loss': 0.3879, 'learning_rate': 2.044781159146781e-05, 'epoch': 199.03}\n",
            "{'loss': 0.3958, 'learning_rate': 2.0398006906499403e-05, 'epoch': 199.35}\n",
            "{'loss': 0.3819, 'learning_rate': 2.0348190500442785e-05, 'epoch': 199.68}\n",
            "{'loss': 0.388, 'learning_rate': 2.0298362727606233e-05, 'epoch': 200.0}\n",
            " 50% 6200/12400 [1:59:34<57:33,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:13:35,637 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:13:35,637 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:13:35,637 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                          \n",
            " 50% 6200/12400 [1:59:38<57:33,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:13:39,604 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6200\n",
            "{'eval_loss': 0.3813236951828003, 'eval_runtime': 3.9649, 'eval_samples_per_second': 55.235, 'eval_steps_per_second': 1.009, 'epoch': 200.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:13:39,606 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6200/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:13:41,625 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6200/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:13:41,626 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6200/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:13:46,099 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6138] due to args.save_total_limit\n",
            "                                            {'loss': 0.4007, 'learning_rate': 2.024852394237886e-05, 'epoch': 200.32}\n",
            "{'loss': 0.3844, 'learning_rate': 2.019867449922813e-05, 'epoch': 200.65}\n",
            "                                          {'loss': 0.3796, 'learning_rate': 2.0148814752697273e-05, 'epoch': 200.97}\n",
            " 50% 6231/12400 [2:00:07<58:56,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 11:14:09,395 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:14:09,395 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:14:09,395 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.39085283875465393, 'eval_runtime': 3.9491, 'eval_samples_per_second': 55.456, 'eval_steps_per_second': 1.013, 'epoch': 201.0}\n",
            " 50% 6231/12400 [2:00:12<58:56,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:14:13,346 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6231\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:14:13,348 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6231/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:14:15,306 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6231/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:14:15,307 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6231/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:14:19,786 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6169] due to args.save_total_limit\n",
            "                                            {'loss': 0.3877, 'learning_rate': 2.0098945057402818e-05, 'epoch': 201.29}\n",
            " 50% 6250/12400 [2:00:34<1:02:45,  1.63it/s]{'loss': 0.3841, 'learning_rate': 2.0049065768032063e-05, 'epoch': 201.61}\n",
            "{'loss': 0.3828, 'learning_rate': 1.9999177239340523e-05, 'epoch': 201.94}\n",
            " 50% 6262/12400 [2:00:41<58:06,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 11:14:42,897 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:14:42,898 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:14:42,898 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.49it/s]\u001b[A\n",
            "                                          \n",
            " 50% 6262/12400 [2:00:45<58:06,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "{'eval_loss': 0.38136011362075806, 'eval_runtime': 3.9216, 'eval_samples_per_second': 55.844, 'eval_steps_per_second': 1.02, 'epoch': 202.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:14:46,821 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6262\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:14:46,823 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6262/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:14:48,817 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6262/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:14:48,818 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6262/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:14:53,351 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6200] due to args.save_total_limit\n",
            "{'loss': 0.3937, 'learning_rate': 1.9949279826149436e-05, 'epoch': 202.26}\n",
            " 51% 6280/12400 [2:01:07<1:03:05,  1.62it/s]{'loss': 0.3903, 'learning_rate': 1.9899373883343224e-05, 'epoch': 202.58}\n",
            "{'loss': 0.3878, 'learning_rate': 1.984945976586698e-05, 'epoch': 202.9}\n",
            " 51% 6293/12400 [2:01:14<57:05,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:15:16,332 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:15:16,332 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:15:16,333 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                          \n",
            "                                 {'eval_loss': 0.38352474570274353, 'eval_runtime': 3.9374, 'eval_samples_per_second': 55.621, 'eval_steps_per_second': 1.016, 'epoch': 203.0}\n",
            " 51% 6293/12400 [2:01:19<57:05,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:15:20,272 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6293\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:15:20,273 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6293/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:15:22,319 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6293/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:15:22,320 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6293/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:15:26,902 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6231] due to args.save_total_limit\n",
            "                                            {'loss': 0.3829, 'learning_rate': 1.979953782872392e-05, 'epoch': 203.23}\n",
            "{'loss': 0.3893, 'learning_rate': 1.9749608426972907e-05, 'epoch': 203.55}\n",
            " 51% 6320/12400 [2:01:46<1:00:45,  1.67it/s]{'loss': 0.3822, 'learning_rate': 1.9699671915725864e-05, 'epoch': 203.87}\n",
            " 51% 6324/12400 [2:01:48<56:27,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:15:50,162 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:15:50,162 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:15:50,162 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.58it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3766252398490906, 'eval_runtime': 3.9974, 'eval_samples_per_second': 54.785, 'eval_steps_per_second': 1.001, 'epoch': 204.0}\n",
            " 51% 6324/12400 [2:01:52<56:27,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:15:54,161 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6324\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:15:54,163 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6324/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:15:56,131 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6324/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:15:56,132 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6324/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:16:00,791 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6262] due to args.save_total_limit\n",
            " 51% 6330/12400 [2:02:07<2:17:28,  1.36s/it]{'loss': 0.3907, 'learning_rate': 1.9649728650145298e-05, 'epoch': 204.19}\n",
            "{'loss': 0.3932, 'learning_rate': 1.959977898544174e-05, 'epoch': 204.52}\n",
            "{'loss': 0.384, 'learning_rate': 1.9549823276871255e-05, 'epoch': 204.84}\n",
            " 51% 6355/12400 [2:02:22<57:22,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 11:16:24,600 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:16:24,600 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:16:24,600 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                          \n",
            " 51% 6355/12400 [2:02:27<57:22,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "{'eval_loss': 0.38520026206970215, 'eval_runtime': 3.974, 'eval_samples_per_second': 55.108, 'eval_steps_per_second': 1.007, 'epoch': 205.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:16:28,576 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6355\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:16:28,577 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6355/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:16:30,574 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6355/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:16:30,575 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6355/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:16:34,983 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6293] due to args.save_total_limit\n",
            "{'loss': 0.3798, 'learning_rate': 1.9499861879732867e-05, 'epoch': 205.16}\n",
            " 51% 6370/12400 [2:02:47<1:06:56,  1.50it/s]{'loss': 0.3968, 'learning_rate': 1.9449895149366076e-05, 'epoch': 205.48}\n",
            "                                            {'loss': 0.3889, 'learning_rate': 1.9399923441148315e-05, 'epoch': 205.81}\n",
            " 52% 6386/12400 [2:02:57<56:41,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:16:58,881 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:16:58,881 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:16:58,881 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                          \n",
            " 52% 6386/12400 [2:03:01<56:41,  1.77it/s]\n",
            "{'eval_loss': 0.37119394540786743, 'eval_runtime': 3.9164, 'eval_samples_per_second': 55.919, 'eval_steps_per_second': 1.021, 'epoch': 206.0}\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:17:02,799 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6386\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:17:02,800 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6386/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:17:04,768 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6386/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:17:04,769 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6386/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:17:09,282 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6324] due to args.save_total_limit\n",
            " 52% 6390/12400 [2:03:14<3:28:39,  2.08s/it]{'loss': 0.3866, 'learning_rate': 1.934994711049241e-05, 'epoch': 206.13}\n",
            "{'loss': 0.3938, 'learning_rate': 1.9299966512844062e-05, 'epoch': 206.45}\n",
            "                                            {'loss': 0.3783, 'learning_rate': 1.924998200367934e-05, 'epoch': 206.77}\n",
            " 52% 6417/12400 [2:03:30<55:16,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:17:32,376 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:17:32,376 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:17:32,376 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.38327306509017944, 'eval_runtime': 3.9358, 'eval_samples_per_second': 55.643, 'eval_steps_per_second': 1.016, 'epoch': 207.0}\n",
            " 52% 6417/12400 [2:03:35<55:16,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.68it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:17:36,314 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6417\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:17:36,316 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6417/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:17:38,248 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6417/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:17:38,249 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6417/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:17:42,712 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6355] due to args.save_total_limit\n",
            "{'loss': 0.3909, 'learning_rate': 1.9199993938502104e-05, 'epoch': 207.1}\n",
            "{'loss': 0.3943, 'learning_rate': 1.9150002672841526e-05, 'epoch': 207.42}\n",
            "{'loss': 0.3715, 'learning_rate': 1.9100008562249546e-05, 'epoch': 207.74}\n",
            " 52% 6448/12400 [2:04:04<54:53,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:18:05,967 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:18:05,967 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:18:05,967 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                          \n",
            " 52% 6448/12400 [2:04:08<54:53,  1.81it/s]\n",
            "{'eval_loss': 0.3801875710487366, 'eval_runtime': 3.9379, 'eval_samples_per_second': 55.613, 'eval_steps_per_second': 1.016, 'epoch': 208.0}\n",
            "100% 4/4 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:18:09,906 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6448\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:18:09,908 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6448/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:18:11,872 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6448/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:18:11,873 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6448/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:18:16,436 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6386] due to args.save_total_limit\n",
            "{'loss': 0.397, 'learning_rate': 1.9050011962298315e-05, 'epoch': 208.06}\n",
            "                                            {'loss': 0.3861, 'learning_rate': 1.90000132285777e-05, 'epoch': 208.39}\n",
            " 52% 6470/12400 [2:04:32<1:02:34,  1.58it/s]{'loss': 0.3938, 'learning_rate': 1.895001271669275e-05, 'epoch': 208.71}\n",
            " 52% 6479/12400 [2:04:38<55:07,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:18:39,905 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:18:39,905 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:18:39,905 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.37594619393348694, 'eval_runtime': 3.9798, 'eval_samples_per_second': 55.028, 'eval_steps_per_second': 1.005, 'epoch': 209.0}\n",
            " 52% 6479/12400 [2:04:42<55:07,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.68it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:18:43,886 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6479\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:18:43,888 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6479/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:18:45,851 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6479/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:18:45,852 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6479/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:18:50,581 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6417] due to args.save_total_limit\n",
            "{'loss': 0.3754, 'learning_rate': 1.890001078226116e-05, 'epoch': 209.03}\n",
            "                                            {'loss': 0.379, 'learning_rate': 1.885000778091073e-05, 'epoch': 209.35}\n",
            "{'loss': 0.3957, 'learning_rate': 1.8800004068276853e-05, 'epoch': 209.68}\n",
            " 52% 6510/12400 [2:05:12<53:49,  1.82it/s]{'loss': 0.3851, 'learning_rate': 1.875e-05, 'epoch': 210.0}\n",
            "[INFO|trainer.py:2550] 2022-05-10 11:19:13,987 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:19:13,987 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:19:13,987 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            " 52% 6510/12400 [2:05:16<53:49,  1.82it/s]\n",
            "{'eval_loss': 0.38646310567855835, 'eval_runtime': 3.9614, 'eval_samples_per_second': 55.283, 'eval_steps_per_second': 1.01, 'epoch': 210.0}\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:19:17,950 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6510\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:19:17,952 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6510/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:19:19,927 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6510/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:19:19,928 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6510/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:19:24,417 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6448] due to args.save_total_limit\n",
            " 53% 6520/12400 [2:05:33<1:18:37,  1.25it/s]{'loss': 0.3925, 'learning_rate': 1.869999593172315e-05, 'epoch': 210.32}\n",
            "{'loss': 0.3822, 'learning_rate': 1.8649992219089267e-05, 'epoch': 210.65}\n",
            "{'loss': 0.3908, 'learning_rate': 1.8599989217738843e-05, 'epoch': 210.97}\n",
            " 53% 6541/12400 [2:05:46<57:33,  1.70it/s][INFO|trainer.py:2550] 2022-05-10 11:19:48,248 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:19:48,248 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:19:48,248 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            " 53% 6541/12400 [2:05:51<57:33,  1.70it/s]\n",
            "{'eval_loss': 0.3765077590942383, 'eval_runtime': 4.0474, 'eval_samples_per_second': 54.108, 'eval_steps_per_second': 0.988, 'epoch': 211.0}\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:19:52,297 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6541\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:19:52,299 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6541/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:19:54,281 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6541/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:19:54,282 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6541/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:19:58,887 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6479] due to args.save_total_limit\n",
            "{'loss': 0.3775, 'learning_rate': 1.8549987283307243e-05, 'epoch': 211.29}\n",
            "{'loss': 0.3852, 'learning_rate': 1.8499986771422298e-05, 'epoch': 211.61}\n",
            "{'loss': 0.388, 'learning_rate': 1.8449988037701682e-05, 'epoch': 211.94}\n",
            " 53% 6572/12400 [2:06:20<55:16,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 11:20:22,086 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:20:22,086 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:20:22,086 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                          \n",
            " 53% 6572/12400 [2:06:24<55:16,  1.76it/s]\n",
            "{'eval_loss': 0.37859082221984863, 'eval_runtime': 3.9343, 'eval_samples_per_second': 55.665, 'eval_steps_per_second': 1.017, 'epoch': 212.0}\n",
            "100% 4/4 [00:00<00:00,  8.69it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:20:26,023 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6572\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:20:26,025 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6572/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:20:27,966 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6572/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:20:27,967 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6572/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:20:32,232 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6510] due to args.save_total_limit\n",
            "                                            {'loss': 0.3891, 'learning_rate': 1.839999143775045e-05, 'epoch': 212.26}\n",
            "{'loss': 0.382, 'learning_rate': 1.834999732715847e-05, 'epoch': 212.58}\n",
            "{'loss': 0.3893, 'learning_rate': 1.8300006061497893e-05, 'epoch': 212.9}\n",
            " 53% 6603/12400 [2:06:53<54:33,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:20:55,231 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:20:55,232 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:20:55,232 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3785838186740875, 'eval_runtime': 3.9388, 'eval_samples_per_second': 55.6, 'eval_steps_per_second': 1.016, 'epoch': 213.0}\n",
            " 53% 6603/12400 [2:06:57<54:33,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:20:59,172 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6603\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:20:59,174 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6603/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:21:01,121 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6603/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:21:01,122 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6603/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:21:06,457 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6541] due to args.save_total_limit\n",
            "                                            {'loss': 0.3878, 'learning_rate': 1.8250017996320667e-05, 'epoch': 213.23}\n",
            " 53% 6620/12400 [2:07:20<1:05:38,  1.47it/s]{'loss': 0.3724, 'learning_rate': 1.8200033487155935e-05, 'epoch': 213.55}\n",
            "                                            {'loss': 0.3988, 'learning_rate': 1.8150052889507594e-05, 'epoch': 213.87}\n",
            " 54% 6634/12400 [2:07:28<54:38,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 11:21:29,982 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:21:29,982 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:21:29,982 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 0.3903608024120331, 'eval_runtime': 3.9812, 'eval_samples_per_second': 55.009, 'eval_steps_per_second': 1.005, 'epoch': 214.0}\n",
            " 54% 6634/12400 [2:07:32<54:38,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:21:33,965 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6634\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:21:33,967 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6634/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:21:35,990 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6634/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:21:35,991 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6634/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:21:40,536 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6572] due to args.save_total_limit\n",
            "{'loss': 0.3848, 'learning_rate': 1.8100076558851682e-05, 'epoch': 214.19}\n",
            "{'loss': 0.378, 'learning_rate': 1.805010485063392e-05, 'epoch': 214.52}\n",
            " 54% 6660/12400 [2:07:59<58:57,  1.62it/s]{'loss': 0.3926, 'learning_rate': 1.8000138120267133e-05, 'epoch': 214.84}\n",
            " 54% 6665/12400 [2:08:02<53:12,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:22:03,678 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:22:03,678 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:22:03,678 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.55it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3830386996269226, 'eval_runtime': 4.0467, 'eval_samples_per_second': 54.118, 'eval_steps_per_second': 0.988, 'epoch': 215.0}\n",
            " 54% 6665/12400 [2:08:06<53:12,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:22:07,727 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6665\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:22:07,728 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6665/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:22:09,736 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6665/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:22:09,737 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6665/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:22:14,939 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6603] due to args.save_total_limit\n",
            "                                            {'loss': 0.3899, 'learning_rate': 1.7950176723128745e-05, 'epoch': 215.16}\n",
            "{'loss': 0.3911, 'learning_rate': 1.790022101455826e-05, 'epoch': 215.48}\n",
            "{'loss': 0.3777, 'learning_rate': 1.7850271349854702e-05, 'epoch': 215.81}\n",
            " 54% 6696/12400 [2:08:36<53:20,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:22:38,581 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:22:38,581 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:22:38,581 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                          \n",
            " 54% 6696/12400 [2:08:41<53:20,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]{'eval_loss': 0.37601566314697266, 'eval_runtime': 3.9506, 'eval_samples_per_second': 55.435, 'eval_steps_per_second': 1.013, 'epoch': 216.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:22:42,533 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6696\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:22:42,535 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6696/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:22:44,516 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6696/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:22:44,518 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6696/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:22:49,119 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6634] due to args.save_total_limit\n",
            " 54% 6700/12400 [2:08:54<3:20:38,  2.11s/it]{'loss': 0.3973, 'learning_rate': 1.7800328084274137e-05, 'epoch': 216.13}\n",
            " 54% 6710/12400 [2:09:01<1:05:05,  1.46it/s]{'loss': 0.3874, 'learning_rate': 1.775039157302709e-05, 'epoch': 216.45}\n",
            " 54% 6720/12400 [2:09:07<58:01,  1.63it/s]{'loss': 0.3824, 'learning_rate': 1.770046217127608e-05, 'epoch': 216.77}\n",
            " 54% 6727/12400 [2:09:10<52:31,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:23:12,616 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:23:12,616 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:23:12,616 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3806438744068146, 'eval_runtime': 4.009, 'eval_samples_per_second': 54.627, 'eval_steps_per_second': 0.998, 'epoch': 217.0}\n",
            " 54% 6727/12400 [2:09:15<52:31,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:23:16,627 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6727\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:23:16,628 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6727/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:23:18,594 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6727/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:23:18,596 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6727/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:23:23,125 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6665] due to args.save_total_limit\n",
            "{'loss': 0.3799, 'learning_rate': 1.7650540234133025e-05, 'epoch': 217.1}\n",
            "                                            {'loss': 0.3858, 'learning_rate': 1.7600626116656772e-05, 'epoch': 217.42}\n",
            "                                          {'loss': 0.3968, 'learning_rate': 1.7550720173850564e-05, 'epoch': 217.74}\n",
            " 55% 6758/12400 [2:09:44<51:55,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:23:46,355 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:23:46,355 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:23:46,355 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            " 55% 6758/12400 [2:09:49<51:55,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.3735736310482025, 'eval_runtime': 3.9608, 'eval_samples_per_second': 55.291, 'eval_steps_per_second': 1.01, 'epoch': 218.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 11:23:50,318 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6758\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:23:50,319 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6758/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:23:52,298 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6758/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:23:52,299 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6758/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:23:56,719 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6696] due to args.save_total_limit\n",
            "{'loss': 0.3794, 'learning_rate': 1.7500822760659473e-05, 'epoch': 218.06}\n",
            "                                            {'loss': 0.3854, 'learning_rate': 1.7450934231967937e-05, 'epoch': 218.39}\n",
            "                                          {'loss': 0.3803, 'learning_rate': 1.7401054942597176e-05, 'epoch': 218.71}\n",
            " 55% 6789/12400 [2:10:18<52:14,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:24:20,194 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:24:20,194 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:24:20,194 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3810652494430542, 'eval_runtime': 4.0016, 'eval_samples_per_second': 54.728, 'eval_steps_per_second': 1.0, 'epoch': 219.0}\n",
            " 55% 6789/12400 [2:10:22<52:14,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:24:24,197 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6789\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:24:24,199 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6789/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:24:26,287 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6789/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:24:26,289 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6789/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:24:30,703 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6727] due to args.save_total_limit\n",
            " 55% 6790/12400 [2:10:33<7:42:30,  4.95s/it]{'loss': 0.3955, 'learning_rate': 1.7351185247302727e-05, 'epoch': 219.03}\n",
            " 55% 6800/12400 [2:10:40<1:10:42,  1.32it/s]{'loss': 0.3896, 'learning_rate': 1.7301325500771877e-05, 'epoch': 219.35}\n",
            " 55% 6810/12400 [2:10:46<1:01:09,  1.52it/s]{'loss': 0.3862, 'learning_rate': 1.7251476057621138e-05, 'epoch': 219.68}\n",
            "{'loss': 0.378, 'learning_rate': 1.7201637272393767e-05, 'epoch': 220.0}\n",
            " 55% 6820/12400 [2:10:53<51:41,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:24:54,296 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:24:54,296 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:24:54,296 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                          \n",
            " 55% 6820/12400 [2:10:57<51:41,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.3781127333641052, 'eval_runtime': 3.9855, 'eval_samples_per_second': 54.95, 'eval_steps_per_second': 1.004, 'epoch': 220.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 11:24:58,284 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6820\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:24:58,285 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6820/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:25:00,226 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6820/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:25:00,227 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6820/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:25:04,490 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6758] due to args.save_total_limit\n",
            "{'loss': 0.3844, 'learning_rate': 1.715180949955721e-05, 'epoch': 220.32}\n",
            "                                          {'loss': 0.3905, 'learning_rate': 1.7101993093500597e-05, 'epoch': 220.65}\n",
            "                                          {'loss': 0.3881, 'learning_rate': 1.7052188408532186e-05, 'epoch': 220.97}\n",
            " 55% 6851/12400 [2:11:26<53:08,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 11:25:27,855 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:25:27,855 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:25:27,856 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3767673969268799, 'eval_runtime': 4.0211, 'eval_samples_per_second': 54.462, 'eval_steps_per_second': 0.995, 'epoch': 221.0}\n",
            " 55% 6851/12400 [2:11:30<53:08,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:25:31,878 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6851\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:25:31,880 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6851/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:25:33,801 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6851/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:25:33,802 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6851/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:25:38,350 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6789] due to args.save_total_limit\n",
            " 55% 6860/12400 [2:11:46<1:24:19,  1.10it/s]{'loss': 0.3807, 'learning_rate': 1.700239579887691e-05, 'epoch': 221.29}\n",
            "{'loss': 0.3865, 'learning_rate': 1.6952615618673802e-05, 'epoch': 221.61}\n",
            " 55% 6880/12400 [2:11:59<52:24,  1.76it/s]{'loss': 0.3906, 'learning_rate': 1.6902848221973473e-05, 'epoch': 221.94}\n",
            " 56% 6882/12400 [2:12:00<52:14,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 11:26:01,829 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:26:01,829 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:26:01,829 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3814207911491394, 'eval_runtime': 4.025, 'eval_samples_per_second': 54.41, 'eval_steps_per_second': 0.994, 'epoch': 222.0}\n",
            " 56% 6882/12400 [2:12:04<52:14,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:26:05,856 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6882\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:26:05,857 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6882/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:26:07,830 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6882/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:26:07,831 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6882/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:26:12,196 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6820] due to args.save_total_limit\n",
            "                                            {'loss': 0.3822, 'learning_rate': 1.6853093962735646e-05, 'epoch': 222.26}\n",
            " 56% 6900/12400 [2:12:26<59:57,  1.53it/s]  {'loss': 0.3838, 'learning_rate': 1.680335319482659e-05, 'epoch': 222.58}\n",
            "                                          {'loss': 0.3906, 'learning_rate': 1.6753626272016638e-05, 'epoch': 222.9}\n",
            " 56% 6913/12400 [2:12:34<51:46,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:26:35,711 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:26:35,711 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:26:35,711 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            " 56% 6913/12400 [2:12:38<51:46,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A{'eval_loss': 0.380254864692688, 'eval_runtime': 3.9594, 'eval_samples_per_second': 55.312, 'eval_steps_per_second': 1.01, 'epoch': 223.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:26:39,672 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6913\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:26:39,674 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6913/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:26:41,642 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6913/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:26:41,643 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6913/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:26:46,087 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6851] due to args.save_total_limit\n",
            " 56% 6920/12400 [2:12:52<1:41:00,  1.11s/it]{'loss': 0.3916, 'learning_rate': 1.6703913547977614e-05, 'epoch': 223.23}\n",
            "                                          {'loss': 0.3846, 'learning_rate': 1.66542153762804e-05, 'epoch': 223.55}\n",
            "{'loss': 0.3866, 'learning_rate': 1.6604532110392355e-05, 'epoch': 223.87}\n",
            " 56% 6944/12400 [2:13:07<50:54,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:27:09,073 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:27:09,074 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:27:09,074 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.52it/s]\u001b[A\n",
            "                                          \n",
            " 56% 6944/12400 [2:13:11<50:54,  1.79it/s]\n",
            "{'eval_loss': 0.3775097131729126, 'eval_runtime': 3.9498, 'eval_samples_per_second': 55.446, 'eval_steps_per_second': 1.013, 'epoch': 224.0}\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:27:13,025 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6944\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:27:13,027 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6944/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:27:14,946 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6944/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:27:14,947 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6944/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:27:19,633 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6882] due to args.save_total_limit\n",
            "                                            {'loss': 0.3816, 'learning_rate': 1.6554864103674818e-05, 'epoch': 224.19}\n",
            " 56% 6960/12400 [2:13:32<1:00:38,  1.50it/s]{'loss': 0.389, 'learning_rate': 1.6505211709380616e-05, 'epoch': 224.52}\n",
            "                                          {'loss': 0.3811, 'learning_rate': 1.6455575280651526e-05, 'epoch': 224.84}\n",
            " 56% 6975/12400 [2:13:41<50:09,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:27:43,013 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:27:43,013 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:27:43,013 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                          \n",
            " 56% 6975/12400 [2:13:45<50:09,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.3855433464050293, 'eval_runtime': 3.9192, 'eval_samples_per_second': 55.879, 'eval_steps_per_second': 1.021, 'epoch': 225.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 11:27:46,934 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-6975\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:27:46,935 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-6975/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:27:48,897 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-6975/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:27:48,898 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-6975/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:27:53,317 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6913] due to args.save_total_limit\n",
            " 56% 6980/12400 [2:13:58<2:29:06,  1.65s/it]{'loss': 0.3954, 'learning_rate': 1.640595517051579e-05, 'epoch': 225.16}\n",
            "                                          {'loss': 0.3818, 'learning_rate': 1.6356351731885558e-05, 'epoch': 225.48}\n",
            "{'loss': 0.3922, 'learning_rate': 1.6306765317554433e-05, 'epoch': 225.81}\n",
            " 56% 7006/12400 [2:14:14<50:09,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:28:16,507 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:28:16,507 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:28:16,507 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                          \n",
            " 56% 7006/12400 [2:14:19<50:09,  1.79it/s]\n",
            "{'eval_loss': 0.3816854953765869, 'eval_runtime': 4.0489, 'eval_samples_per_second': 54.089, 'eval_steps_per_second': 0.988, 'epoch': 226.0}\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:28:20,558 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7006\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:28:20,559 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7006/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:28:22,495 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7006/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:28:22,496 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7006/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:28:27,031 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6944] due to args.save_total_limit\n",
            "{'loss': 0.3868, 'learning_rate': 1.625719628019493e-05, 'epoch': 226.13}\n",
            "{'loss': 0.3794, 'learning_rate': 1.6207644972355962e-05, 'epoch': 226.45}\n",
            "                                          {'loss': 0.3826, 'learning_rate': 1.6158111746460365e-05, 'epoch': 226.77}\n",
            " 57% 7037/12400 [2:14:48<49:07,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:28:50,248 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:28:50,248 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:28:50,248 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 0.37474381923675537, 'eval_runtime': 3.9396, 'eval_samples_per_second': 55.59, 'eval_steps_per_second': 1.015, 'epoch': 227.0}\n",
            " 57% 7037/12400 [2:14:52<49:07,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:28:54,189 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7037\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:28:54,191 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7037/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:28:56,154 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7037/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:28:56,155 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7037/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:29:00,371 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-6975] due to args.save_total_limit\n",
            " 57% 7040/12400 [2:15:05<3:57:22,  2.66s/it]{'loss': 0.3907, 'learning_rate': 1.6108596954802348e-05, 'epoch': 227.1}\n",
            " 57% 7050/12400 [2:15:11<1:03:08,  1.41it/s]{'loss': 0.3891, 'learning_rate': 1.6059100949545042e-05, 'epoch': 227.42}\n",
            "{'loss': 0.3903, 'learning_rate': 1.600962408271792e-05, 'epoch': 227.74}\n",
            " 57% 7068/12400 [2:15:22<48:45,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:29:23,697 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:29:23,698 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:29:23,698 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                          \n",
            " 57% 7068/12400 [2:15:26<48:45,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "{'eval_loss': 0.37959325313568115, 'eval_runtime': 3.9885, 'eval_samples_per_second': 54.907, 'eval_steps_per_second': 1.003, 'epoch': 228.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:29:27,688 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7068\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:29:27,690 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7068/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:29:29,623 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7068/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:29:29,624 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7068/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:29:34,161 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7006] due to args.save_total_limit\n",
            "{'loss': 0.3885, 'learning_rate': 1.5960166706214388e-05, 'epoch': 228.06}\n",
            " 57% 7080/12400 [2:15:44<1:01:25,  1.44it/s]{'loss': 0.3956, 'learning_rate': 1.591072917178919e-05, 'epoch': 228.39}\n",
            "{'loss': 0.382, 'learning_rate': 1.5861311831055958e-05, 'epoch': 228.71}\n",
            " 57% 7099/12400 [2:15:55<49:28,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:29:57,499 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:29:57,499 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:29:57,499 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.40it/s]\u001b[A\n",
            "                                          \n",
            " 57% 7099/12400 [2:16:00<49:28,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]{'eval_loss': 0.37788090109825134, 'eval_runtime': 4.0543, 'eval_samples_per_second': 54.017, 'eval_steps_per_second': 0.987, 'epoch': 229.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:30:01,555 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7099\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:30:01,557 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7099/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:30:03,592 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7099/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:30:03,593 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7099/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:30:07,987 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7037] due to args.save_total_limit\n",
            "{'loss': 0.383, 'learning_rate': 1.5811915035484725e-05, 'epoch': 229.03}\n",
            " 57% 7110/12400 [2:16:17<1:05:10,  1.35it/s]{'loss': 0.3842, 'learning_rate': 1.5762539136399362e-05, 'epoch': 229.35}\n",
            "{'loss': 0.3794, 'learning_rate': 1.571318448497515e-05, 'epoch': 229.68}\n",
            " 57% 7130/12400 [2:16:30<48:44,  1.80it/s]{'loss': 0.3907, 'learning_rate': 1.5663851432236234e-05, 'epoch': 230.0}\n",
            "[INFO|trainer.py:2550] 2022-05-10 11:30:31,432 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:30:31,432 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:30:31,432 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                          \n",
            " 57% 7130/12400 [2:16:34<48:44,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:30:35,433 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7130\n",
            "{'eval_loss': 0.387419193983078, 'eval_runtime': 3.9991, 'eval_samples_per_second': 54.763, 'eval_steps_per_second': 1.0, 'epoch': 230.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:30:35,434 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7130/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:30:37,384 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7130/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:30:37,385 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7130/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:30:41,592 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7068] due to args.save_total_limit\n",
            "{'loss': 0.3827, 'learning_rate': 1.5614540329053157e-05, 'epoch': 230.32}\n",
            "                                          {'loss': 0.3945, 'learning_rate': 1.5565251526140327e-05, 'epoch': 230.65}\n",
            "{'loss': 0.3798, 'learning_rate': 1.5515985374053565e-05, 'epoch': 230.97}\n",
            " 58% 7161/12400 [2:17:02<50:24,  1.73it/s][INFO|trainer.py:2550] 2022-05-10 11:31:04,632 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:31:04,632 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:31:04,632 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.53it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 0.37852999567985535, 'eval_runtime': 4.0073, 'eval_samples_per_second': 54.65, 'eval_steps_per_second': 0.998, 'epoch': 231.0}\n",
            " 58% 7161/12400 [2:17:07<50:24,  1.73it/s]\n",
            "100% 4/4 [00:00<00:00,  8.71it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:31:08,641 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7161\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:31:08,643 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7161/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:31:10,602 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7161/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:31:10,603 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7161/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:31:15,139 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7099] due to args.save_total_limit\n",
            "                                            {'loss': 0.3814, 'learning_rate': 1.54667422231876e-05, 'epoch': 231.29}\n",
            "{'loss': 0.395, 'learning_rate': 1.5417522423773537e-05, 'epoch': 231.61}\n",
            " 58% 7190/12400 [2:17:36<49:30,  1.75it/s]{'loss': 0.3777, 'learning_rate': 1.536832632587644e-05, 'epoch': 231.94}\n",
            " 58% 7192/12400 [2:17:36<49:27,  1.75it/s][INFO|trainer.py:2550] 2022-05-10 11:31:38,631 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:31:38,631 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:31:38,631 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                          \n",
            " 58% 7192/12400 [2:17:41<49:27,  1.75it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]{'eval_loss': 0.383610337972641, 'eval_runtime': 4.0032, 'eval_samples_per_second': 54.706, 'eval_steps_per_second': 0.999, 'epoch': 232.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:31:42,636 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7192\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:31:42,638 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7192/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:31:44,585 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7192/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:31:44,586 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7192/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:31:48,958 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7130] due to args.save_total_limit\n",
            "                                            {'loss': 0.3818, 'learning_rate': 1.531915427939277e-05, 'epoch': 232.26}\n",
            " 58% 7210/12400 [2:18:03<57:13,  1.51it/s]{'loss': 0.3826, 'learning_rate': 1.527000663404795e-05, 'epoch': 232.58}\n",
            " 58% 7220/12400 [2:18:09<50:46,  1.70it/s]{'loss': 0.3929, 'learning_rate': 1.5220883739393834e-05, 'epoch': 232.9}\n",
            " 58% 7223/12400 [2:18:10<48:32,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:32:12,578 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:32:12,578 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:32:12,578 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                          \n",
            " 58% 7223/12400 [2:18:15<48:32,  1.78it/s]\n",
            "{'eval_loss': 0.378485769033432, 'eval_runtime': 3.9868, 'eval_samples_per_second': 54.931, 'eval_steps_per_second': 1.003, 'epoch': 233.0}\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:32:16,566 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7223\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:32:16,568 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7223/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:32:18,505 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7223/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:32:18,506 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7223/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:32:23,068 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7161] due to args.save_total_limit\n",
            "                                            {'loss': 0.3749, 'learning_rate': 1.517178594480625e-05, 'epoch': 233.23}\n",
            " 58% 7240/12400 [2:18:36<56:23,  1.52it/s]{'loss': 0.3949, 'learning_rate': 1.5122713599482524e-05, 'epoch': 233.55}\n",
            "{'loss': 0.3941, 'learning_rate': 1.5073667052438953e-05, 'epoch': 233.87}\n",
            " 58% 7254/12400 [2:18:44<48:02,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:32:46,395 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:32:46,395 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:32:46,396 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                          \n",
            " 58% 7254/12400 [2:18:49<48:02,  1.79it/s]\n",
            "{'eval_loss': 0.38028791546821594, 'eval_runtime': 3.9259, 'eval_samples_per_second': 55.783, 'eval_steps_per_second': 1.019, 'epoch': 234.0}\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:32:50,323 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7254\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:32:50,325 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7254/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:32:52,281 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7254/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:32:52,282 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7254/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:32:56,735 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7192] due to args.save_total_limit\n",
            " 59% 7260/12400 [2:19:02<1:54:02,  1.33s/it]{'loss': 0.3902, 'learning_rate': 1.5024646652508366e-05, 'epoch': 234.19}\n",
            "{'loss': 0.3786, 'learning_rate': 1.4975652748337619e-05, 'epoch': 234.52}\n",
            " 59% 7280/12400 [2:19:15<52:23,  1.63it/s]{'loss': 0.3892, 'learning_rate': 1.4926685688385138e-05, 'epoch': 234.84}\n",
            " 59% 7285/12400 [2:19:18<47:24,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:33:19,769 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:33:19,769 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:33:19,769 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                          \n",
            " 59% 7285/12400 [2:19:22<47:24,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.3723814785480499, 'eval_runtime': 3.9408, 'eval_samples_per_second': 55.573, 'eval_steps_per_second': 1.015, 'epoch': 235.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 11:33:23,712 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7285\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:33:23,714 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7285/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:33:25,623 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7285/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:33:25,624 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7285/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:33:29,852 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7223] due to args.save_total_limit\n",
            "                                            {'loss': 0.3916, 'learning_rate': 1.4877745820918401e-05, 'epoch': 235.16}\n",
            "                                          {'loss': 0.394, 'learning_rate': 1.4828833494011493e-05, 'epoch': 235.48}\n",
            " 59% 7310/12400 [2:19:48<55:19,  1.53it/s]{'loss': 0.3716, 'learning_rate': 1.4779949055542649e-05, 'epoch': 235.81}\n",
            " 59% 7316/12400 [2:19:51<47:28,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:33:53,193 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:33:53,194 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:33:53,194 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.39it/s]\u001b[A\n",
            "                                          \n",
            " 59% 7316/12400 [2:19:55<47:28,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.3835638761520386, 'eval_runtime': 3.9542, 'eval_samples_per_second': 55.384, 'eval_steps_per_second': 1.012, 'epoch': 236.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 11:33:57,149 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7316\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:33:57,151 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7316/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:33:59,082 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7316/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:33:59,084 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7316/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:34:03,668 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7254] due to args.save_total_limit\n",
            " 59% 7320/12400 [2:20:09<2:56:32,  2.09s/it]{'loss': 0.3824, 'learning_rate': 1.4731092853191707e-05, 'epoch': 236.13}\n",
            " 59% 7330/12400 [2:20:14<56:36,  1.49it/s]  {'loss': 0.387, 'learning_rate': 1.4682265234437723e-05, 'epoch': 236.45}\n",
            " 59% 7340/12400 [2:20:21<51:29,  1.64it/s]{'loss': 0.3876, 'learning_rate': 1.4633466546556434e-05, 'epoch': 236.77}\n",
            " 59% 7347/12400 [2:20:25<46:27,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:34:26,756 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:34:26,756 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:34:26,756 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                          \n",
            " 59% 7347/12400 [2:20:29<46:27,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]{'eval_loss': 0.38379979133605957, 'eval_runtime': 3.9485, 'eval_samples_per_second': 55.464, 'eval_steps_per_second': 1.013, 'epoch': 237.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:34:30,706 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7347\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:34:30,708 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7347/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:34:32,642 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7347/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:34:32,643 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7347/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:34:37,033 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7285] due to args.save_total_limit\n",
            "{'loss': 0.3915, 'learning_rate': 1.4584697136617828e-05, 'epoch': 237.1}\n",
            "                                          {'loss': 0.3858, 'learning_rate': 1.4535957351483654e-05, 'epoch': 237.42}\n",
            "{'loss': 0.3816, 'learning_rate': 1.4487247537804946e-05, 'epoch': 237.74}\n",
            " 60% 7378/12400 [2:20:58<46:19,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:35:00,384 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:35:00,384 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:35:00,384 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                          \n",
            " 60% 7378/12400 [2:21:03<46:19,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.74it/s]{'eval_loss': 0.37947437167167664, 'eval_runtime': 3.9536, 'eval_samples_per_second': 55.392, 'eval_steps_per_second': 1.012, 'epoch': 238.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:35:04,339 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7378\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:35:04,341 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7378/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:35:06,374 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7378/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:35:06,375 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7378/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:35:11,010 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7316] due to args.save_total_limit\n",
            "{'loss': 0.3681, 'learning_rate': 1.4438568042019599e-05, 'epoch': 238.06}\n",
            "                                          {'loss': 0.3931, 'learning_rate': 1.438991921034986e-05, 'epoch': 238.39}\n",
            " 60% 7400/12400 [2:21:27<51:06,  1.63it/s]{'loss': 0.3842, 'learning_rate': 1.4341301388799902e-05, 'epoch': 238.71}\n",
            " 60% 7409/12400 [2:21:32<46:08,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:35:34,182 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:35:34,182 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:35:34,182 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            " 60% 7409/12400 [2:21:36<46:08,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  7.56it/s]\u001b[A{'eval_loss': 0.38864952325820923, 'eval_runtime': 4.0429, 'eval_samples_per_second': 54.169, 'eval_steps_per_second': 0.989, 'epoch': 239.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:35:38,227 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7409\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:35:38,228 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7409/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:35:40,183 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7409/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:35:40,184 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7409/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:35:44,713 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7347] due to args.save_total_limit\n",
            " 60% 7410/12400 [2:21:47<6:50:16,  4.93s/it]{'loss': 0.3917, 'learning_rate': 1.4292714923153316e-05, 'epoch': 239.03}\n",
            " 60% 7420/12400 [2:21:53<1:00:39,  1.37it/s]{'loss': 0.385, 'learning_rate': 1.4244160158970712e-05, 'epoch': 239.35}\n",
            " 60% 7430/12400 [2:22:00<52:25,  1.58it/s]{'loss': 0.3779, 'learning_rate': 1.4195637441587215e-05, 'epoch': 239.68}\n",
            "                                          {'loss': 0.3844, 'learning_rate': 1.4147147116110015e-05, 'epoch': 240.0}\n",
            " 60% 7440/12400 [2:22:06<45:20,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:36:07,841 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:36:07,841 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:36:07,841 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.37955889105796814, 'eval_runtime': 3.9565, 'eval_samples_per_second': 55.353, 'eval_steps_per_second': 1.011, 'epoch': 240.0}\n",
            " 60% 7440/12400 [2:22:10<45:20,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:36:11,799 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7440\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:36:11,801 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7440/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:36:13,732 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7440/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:36:13,733 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7440/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:36:18,277 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7378] due to args.save_total_limit\n",
            "{'loss': 0.3905, 'learning_rate': 1.4098689527415947e-05, 'epoch': 240.32}\n",
            "{'loss': 0.3829, 'learning_rate': 1.4050265020148987e-05, 'epoch': 240.65}\n",
            " 60% 7470/12400 [2:22:39<46:40,  1.76it/s]{'loss': 0.3862, 'learning_rate': 1.4001873938717852e-05, 'epoch': 240.97}\n",
            " 60% 7471/12400 [2:22:40<47:42,  1.72it/s][INFO|trainer.py:2550] 2022-05-10 11:36:41,834 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:36:41,834 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:36:41,834 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            " 60% 7471/12400 [2:22:44<47:42,  1.72it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]{'eval_loss': 0.3738228380680084, 'eval_runtime': 3.9247, 'eval_samples_per_second': 55.801, 'eval_steps_per_second': 1.019, 'epoch': 241.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:36:45,760 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7471\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:36:45,762 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7471/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:36:47,686 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7471/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:36:47,687 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7471/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:36:52,254 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7409] due to args.save_total_limit\n",
            "                                            {'loss': 0.3824, 'learning_rate': 1.3953516627293495e-05, 'epoch': 241.29}\n",
            "                                          {'loss': 0.385, 'learning_rate': 1.3905193429806716e-05, 'epoch': 241.61}\n",
            "{'loss': 0.3871, 'learning_rate': 1.3856904689945671e-05, 'epoch': 241.94}\n",
            " 60% 7502/12400 [2:23:13<46:09,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:37:15,320 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:37:15,320 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:37:15,320 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3813706934452057, 'eval_runtime': 3.9294, 'eval_samples_per_second': 55.734, 'eval_steps_per_second': 1.018, 'epoch': 242.0}\n",
            " 60% 7502/12400 [2:23:17<46:09,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:37:19,251 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7502\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:37:19,253 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7502/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:37:21,140 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7502/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:37:21,141 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7502/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:37:25,362 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7440] due to args.save_total_limit\n",
            " 61% 7510/12400 [2:23:33<1:17:15,  1.05it/s]{'loss': 0.3941, 'learning_rate': 1.380865075115344e-05, 'epoch': 242.26}\n",
            "{'loss': 0.3884, 'learning_rate': 1.3760431956625605e-05, 'epoch': 242.58}\n",
            "{'loss': 0.3822, 'learning_rate': 1.3712248649307775e-05, 'epoch': 242.9}\n",
            " 61% 7533/12400 [2:23:46<45:37,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:37:48,525 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:37:48,525 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:37:48,525 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                          \n",
            " 61% 7533/12400 [2:23:51<45:37,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:37:52,493 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7533\n",
            "{'eval_loss': 0.3802444040775299, 'eval_runtime': 3.9668, 'eval_samples_per_second': 55.208, 'eval_steps_per_second': 1.008, 'epoch': 243.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:37:52,495 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7533/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:37:54,439 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7533/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:37:54,440 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7533/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:37:58,829 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7471] due to args.save_total_limit\n",
            " 61% 7540/12400 [2:24:05<1:29:43,  1.11s/it]{'loss': 0.381, 'learning_rate': 1.3664101171893183e-05, 'epoch': 243.23}\n",
            " 61% 7550/12400 [2:24:11<53:14,  1.52it/s]{'loss': 0.3893, 'learning_rate': 1.3615989866820206e-05, 'epoch': 243.55}\n",
            " 61% 7560/12400 [2:24:18<48:28,  1.66it/s]{'loss': 0.3766, 'learning_rate': 1.3567915076269962e-05, 'epoch': 243.87}\n",
            " 61% 7564/12400 [2:24:20<44:57,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:38:22,089 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:38:22,089 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:38:22,089 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                          \n",
            " 61% 7564/12400 [2:24:24<44:57,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.3832857310771942, 'eval_runtime': 3.9224, 'eval_samples_per_second': 55.832, 'eval_steps_per_second': 1.02, 'epoch': 244.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 11:38:26,013 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7564\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:38:26,014 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7564/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:38:27,935 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7564/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:38:27,936 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7564/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:38:32,367 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7502] due to args.save_total_limit\n",
            " 61% 7570/12400 [2:24:39<1:47:11,  1.33s/it]{'loss': 0.3907, 'learning_rate': 1.3519877142163887e-05, 'epoch': 244.19}\n",
            "{'loss': 0.3744, 'learning_rate': 1.347187640616125e-05, 'epoch': 244.52}\n",
            " 61% 7590/12400 [2:24:51<48:46,  1.64it/s]{'loss': 0.3947, 'learning_rate': 1.3423913209656783e-05, 'epoch': 244.84}\n",
            " 61% 7595/12400 [2:24:53<44:27,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:38:55,529 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:38:55,530 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:38:55,530 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                          \n",
            " 61% 7595/12400 [2:24:58<44:27,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.37628173828125, 'eval_runtime': 3.8879, 'eval_samples_per_second': 56.329, 'eval_steps_per_second': 1.029, 'epoch': 245.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 11:38:59,419 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7595\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:38:59,421 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7595/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:39:01,328 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7595/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:39:01,328 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7595/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:39:05,597 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7533] due to args.save_total_limit\n",
            "{'loss': 0.3847, 'learning_rate': 1.3375987893778215e-05, 'epoch': 245.16}\n",
            " 61% 7610/12400 [2:25:17<50:39,  1.58it/s]{'loss': 0.3775, 'learning_rate': 1.3328100799383866e-05, 'epoch': 245.48}\n",
            "{'loss': 0.3868, 'learning_rate': 1.32802522670602e-05, 'epoch': 245.81}\n",
            " 62% 7626/12400 [2:25:27<44:20,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:39:28,735 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:39:28,735 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:39:28,735 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.35it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3826502561569214, 'eval_runtime': 3.8738, 'eval_samples_per_second': 56.534, 'eval_steps_per_second': 1.033, 'epoch': 246.0}\n",
            " 62% 7626/12400 [2:25:31<44:20,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:39:32,610 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7626\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:39:32,612 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7626/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:39:34,535 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7626/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:39:34,536 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7626/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:39:38,935 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7564] due to args.save_total_limit\n",
            " 62% 7630/12400 [2:25:44<2:43:36,  2.06s/it]{'loss': 0.3886, 'learning_rate': 1.3232442637119422e-05, 'epoch': 246.13}\n",
            " 62% 7640/12400 [2:25:50<51:35,  1.54it/s]{'loss': 0.3833, 'learning_rate': 1.3184672249597065e-05, 'epoch': 246.45}\n",
            " 62% 7650/12400 [2:25:56<48:35,  1.63it/s]{'loss': 0.3886, 'learning_rate': 1.3136941444249536e-05, 'epoch': 246.77}\n",
            " 62% 7657/12400 [2:26:00<43:19,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:40:01,838 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:40:01,838 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:40:01,838 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                          \n",
            " 62% 7657/12400 [2:26:04<43:19,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "{'eval_loss': 0.3799389600753784, 'eval_runtime': 3.9481, 'eval_samples_per_second': 55.469, 'eval_steps_per_second': 1.013, 'epoch': 247.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:40:05,788 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7657\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:40:05,790 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7657/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:40:07,707 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7657/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:40:07,709 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7657/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:40:12,124 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7595] due to args.save_total_limit\n",
            "{'loss': 0.3731, 'learning_rate': 1.3089250560551753e-05, 'epoch': 247.1}\n",
            "{'loss': 0.3894, 'learning_rate': 1.304159993769466e-05, 'epoch': 247.42}\n",
            "{'loss': 0.3815, 'learning_rate': 1.2993989914582893e-05, 'epoch': 247.74}\n",
            " 62% 7688/12400 [2:26:33<43:17,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:40:35,486 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:40:35,486 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:40:35,486 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                          \n",
            "                                 {'eval_loss': 0.3811244070529938, 'eval_runtime': 3.9058, 'eval_samples_per_second': 56.071, 'eval_steps_per_second': 1.024, 'epoch': 248.0}\n",
            " 62% 7688/12400 [2:26:38<43:17,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:40:39,393 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7688\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:40:39,395 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7688/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:40:41,305 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7688/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:40:41,306 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7688/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:40:45,693 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7626] due to args.save_total_limit\n",
            " 62% 7690/12400 [2:26:49<4:38:58,  3.55s/it]{'loss': 0.3987, 'learning_rate': 1.2946420829832315e-05, 'epoch': 248.06}\n",
            "                                          {'loss': 0.3747, 'learning_rate': 1.289889302176761e-05, 'epoch': 248.39}\n",
            "{'loss': 0.3738, 'learning_rate': 1.2851406828419918e-05, 'epoch': 248.71}\n",
            " 62% 7719/12400 [2:27:06<42:29,  1.84it/s][INFO|trainer.py:2550] 2022-05-10 11:41:08,616 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:41:08,616 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:41:08,616 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                          \n",
            " 62% 7719/12400 [2:27:11<42:29,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A{'eval_loss': 0.37576064467430115, 'eval_runtime': 3.9028, 'eval_samples_per_second': 56.114, 'eval_steps_per_second': 1.025, 'epoch': 249.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:41:12,521 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7719\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:41:12,522 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7719/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:41:14,445 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7719/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:41:14,446 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7719/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:41:19,031 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7657] due to args.save_total_limit\n",
            " 62% 7720/12400 [2:27:21<6:20:51,  4.88s/it]{'loss': 0.3986, 'learning_rate': 1.2803962587524382e-05, 'epoch': 249.03}\n",
            " 62% 7730/12400 [2:27:28<57:39,  1.35it/s]{'loss': 0.3798, 'learning_rate': 1.2756560636517785e-05, 'epoch': 249.35}\n",
            "{'loss': 0.3918, 'learning_rate': 1.2709201312536107e-05, 'epoch': 249.68}\n",
            "                                          {'loss': 0.3824, 'learning_rate': 1.2661884952412185e-05, 'epoch': 250.0}\n",
            " 62% 7750/12400 [2:27:40<42:32,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:41:42,006 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:41:42,006 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:41:42,006 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3870919346809387, 'eval_runtime': 3.946, 'eval_samples_per_second': 55.499, 'eval_steps_per_second': 1.014, 'epoch': 250.0}\n",
            " 62% 7750/12400 [2:27:44<42:32,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:41:45,954 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7750\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:41:45,956 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7750/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:41:47,934 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7750/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:41:47,935 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7750/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:41:52,658 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7688] due to args.save_total_limit\n",
            "                                            {'loss': 0.3802, 'learning_rate': 1.261461189267326e-05, 'epoch': 250.32}\n",
            "{'loss': 0.3904, 'learning_rate': 1.2567382469538613e-05, 'epoch': 250.65}\n",
            " 63% 7780/12400 [2:28:13<42:34,  1.81it/s]{'loss': 0.3905, 'learning_rate': 1.2520197018917179e-05, 'epoch': 250.97}\n",
            " 63% 7781/12400 [2:28:13<43:52,  1.75it/s][INFO|trainer.py:2550] 2022-05-10 11:42:15,591 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:42:15,591 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:42:15,591 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                          \n",
            " 63% 7781/12400 [2:28:18<43:52,  1.75it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "{'eval_loss': 0.3758484721183777, 'eval_runtime': 3.8916, 'eval_samples_per_second': 56.275, 'eval_steps_per_second': 1.028, 'epoch': 251.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:42:19,485 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7781\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:42:19,486 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7781/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:42:21,412 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7781/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:42:21,413 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7781/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:42:25,655 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7719] due to args.save_total_limit\n",
            "{'loss': 0.3859, 'learning_rate': 1.2473055876405137e-05, 'epoch': 251.29}\n",
            "                                          {'loss': 0.3801, 'learning_rate': 1.2425959377283549e-05, 'epoch': 251.61}\n",
            "                                          {'loss': 0.3852, 'learning_rate': 1.2378907856515939e-05, 'epoch': 251.94}\n",
            " 63% 7812/12400 [2:28:46<42:33,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 11:42:48,366 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:42:48,367 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:42:48,367 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3776552379131317, 'eval_runtime': 3.8873, 'eval_samples_per_second': 56.337, 'eval_steps_per_second': 1.029, 'epoch': 252.0}\n",
            " 63% 7812/12400 [2:28:50<42:33,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:42:52,256 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7812\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:42:52,257 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7812/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:42:54,185 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7812/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:42:54,186 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7812/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:42:58,603 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7750] due to args.save_total_limit\n",
            "                                            {'loss': 0.3809, 'learning_rate': 1.2331901648745954e-05, 'epoch': 252.26}\n",
            " 63% 7830/12400 [2:29:12<47:42,  1.60it/s]{'loss': 0.3836, 'learning_rate': 1.2284941088294949e-05, 'epoch': 252.58}\n",
            "{'loss': 0.3828, 'learning_rate': 1.223802650915962e-05, 'epoch': 252.9}\n",
            " 63% 7843/12400 [2:29:19<42:36,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:43:21,513 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:43:21,514 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:43:21,514 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3767320513725281, 'eval_runtime': 3.94, 'eval_samples_per_second': 55.584, 'eval_steps_per_second': 1.015, 'epoch': 253.0}\n",
            " 63% 7843/12400 [2:29:24<42:36,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:43:25,455 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7843\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:43:25,457 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7843/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:43:27,384 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7843/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:43:27,385 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7843/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:43:31,794 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7781] due to args.save_total_limit\n",
            " 63% 7850/12400 [2:29:39<1:24:37,  1.12s/it]{'loss': 0.3989, 'learning_rate': 1.2191158245009648e-05, 'epoch': 253.23}\n",
            "{'loss': 0.3746, 'learning_rate': 1.2144336629185284e-05, 'epoch': 253.55}\n",
            " 63% 7870/12400 [2:29:51<43:33,  1.73it/s]{'loss': 0.3902, 'learning_rate': 1.2097561994695036e-05, 'epoch': 253.87}\n",
            " 64% 7874/12400 [2:29:53<41:26,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:43:54,799 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:43:54,800 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:43:54,800 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                          \n",
            " 64% 7874/12400 [2:29:57<41:26,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.383342444896698, 'eval_runtime': 3.8624, 'eval_samples_per_second': 56.7, 'eval_steps_per_second': 1.036, 'epoch': 254.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 11:43:58,663 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7874\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:43:58,665 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7874/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:44:00,587 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7874/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:44:00,588 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7874/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:44:05,028 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7812] due to args.save_total_limit\n",
            " 64% 7880/12400 [2:30:11<1:39:26,  1.32s/it]{'loss': 0.3801, 'learning_rate': 1.205083467421323e-05, 'epoch': 254.19}\n",
            " 64% 7890/12400 [2:30:17<48:31,  1.55it/s]{'loss': 0.3906, 'learning_rate': 1.2004155000077708e-05, 'epoch': 254.52}\n",
            "                                          {'loss': 0.3885, 'learning_rate': 1.1957523304287435e-05, 'epoch': 254.84}\n",
            " 64% 7905/12400 [2:30:26<41:30,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:44:28,023 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:44:28,023 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:44:28,023 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                          \n",
            " 64% 7905/12400 [2:30:30<41:30,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A{'eval_loss': 0.3838275969028473, 'eval_runtime': 3.9112, 'eval_samples_per_second': 55.993, 'eval_steps_per_second': 1.023, 'epoch': 255.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:44:31,936 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7905\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:44:31,937 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7905/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:44:33,931 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7905/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:44:33,932 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7905/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:44:38,146 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7843] due to args.save_total_limit\n",
            "                                            {'loss': 0.3738, 'learning_rate': 1.1910939918500117e-05, 'epoch': 255.16}\n",
            "{'loss': 0.3854, 'learning_rate': 1.1864405174029907e-05, 'epoch': 255.48}\n",
            "                                          {'loss': 0.3925, 'learning_rate': 1.1817919401844964e-05, 'epoch': 255.81}\n",
            " 64% 7936/12400 [2:30:59<41:34,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:45:01,002 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:45:01,002 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:45:01,002 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3750818073749542, 'eval_runtime': 3.8827, 'eval_samples_per_second': 56.404, 'eval_steps_per_second': 1.03, 'epoch': 256.0}\n",
            " 64% 7936/12400 [2:31:03<41:34,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:45:04,887 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7936\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:45:04,889 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7936/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:45:06,829 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7936/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:45:06,830 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7936/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:45:11,170 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7874] due to args.save_total_limit\n",
            "{'loss': 0.3844, 'learning_rate': 1.1771482932565175e-05, 'epoch': 256.13}\n",
            "                                          {'loss': 0.3844, 'learning_rate': 1.1725096096459751e-05, 'epoch': 256.45}\n",
            "{'loss': 0.3844, 'learning_rate': 1.1678759223444915e-05, 'epoch': 256.77}\n",
            " 64% 7967/12400 [2:31:32<40:25,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 11:45:34,178 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:45:34,178 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:45:34,178 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            " 64% 7967/12400 [2:31:36<40:25,  1.83it/s]\n",
            "{'eval_loss': 0.3801381587982178, 'eval_runtime': 3.9209, 'eval_samples_per_second': 55.854, 'eval_steps_per_second': 1.02, 'epoch': 257.0}\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:45:38,101 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7967\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:45:38,103 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7967/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:45:40,050 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7967/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:45:40,051 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7967/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:45:44,699 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7905] due to args.save_total_limit\n",
            "{'loss': 0.3853, 'learning_rate': 1.163247264308152e-05, 'epoch': 257.1}\n",
            "                                          {'loss': 0.3916, 'learning_rate': 1.158623668457273e-05, 'epoch': 257.42}\n",
            "                                          {'loss': 0.3858, 'learning_rate': 1.1540051676761689e-05, 'epoch': 257.74}\n",
            " 64% 7998/12400 [2:32:06<40:06,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 11:46:07,837 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:46:07,837 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:46:07,837 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                          \n",
            " 64% 7998/12400 [2:32:10<40:06,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.3784677982330322, 'eval_runtime': 3.9009, 'eval_samples_per_second': 56.141, 'eval_steps_per_second': 1.025, 'epoch': 258.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 11:46:11,740 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-7998\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:46:11,742 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-7998/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:46:13,679 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-7998/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:46:13,680 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-7998/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:46:18,117 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7936] due to args.save_total_limit\n",
            "{'loss': 0.3848, 'learning_rate': 1.1493917948129128e-05, 'epoch': 258.06}\n",
            "{'loss': 0.3788, 'learning_rate': 1.144783582679111e-05, 'epoch': 258.39}\n",
            "                                          {'loss': 0.3909, 'learning_rate': 1.1401805640496615e-05, 'epoch': 258.71}\n",
            " 65% 8029/12400 [2:32:39<40:06,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:46:41,289 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:46:41,289 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:46:41,289 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 0.37909644842147827, 'eval_runtime': 3.8845, 'eval_samples_per_second': 56.378, 'eval_steps_per_second': 1.03, 'epoch': 259.0}\n",
            " 65% 8029/12400 [2:32:43<40:06,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:46:45,175 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8029\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:46:45,178 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8029/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:46:47,163 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8029/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:46:47,164 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8029/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:46:51,405 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7967] due to args.save_total_limit\n",
            "{'loss': 0.3876, 'learning_rate': 1.1355827716625279e-05, 'epoch': 259.03}\n",
            "                                          {'loss': 0.3859, 'learning_rate': 1.1309902382185002e-05, 'epoch': 259.35}\n",
            "{'loss': 0.3757, 'learning_rate': 1.1264029963809672e-05, 'epoch': 259.68}\n",
            "{'loss': 0.3805, 'learning_rate': 1.1218210787756825e-05, 'epoch': 260.0}\n",
            " 65% 8060/12400 [2:33:12<39:07,  1.85it/s][INFO|trainer.py:2550] 2022-05-10 11:47:14,240 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:47:14,240 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:47:14,240 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.60it/s]\u001b[A\n",
            "                                          \n",
            " 65% 8060/12400 [2:33:16<39:07,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]{'eval_loss': 0.37344807386398315, 'eval_runtime': 3.8753, 'eval_samples_per_second': 56.512, 'eval_steps_per_second': 1.032, 'epoch': 260.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:47:18,117 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8060\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:47:18,119 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8060/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:47:20,029 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8060/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:47:20,029 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8060/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:47:24,758 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-7998] due to args.save_total_limit\n",
            " 65% 8070/12400 [2:33:33<57:08,  1.26it/s]  {'loss': 0.387, 'learning_rate': 1.1172445179905307e-05, 'epoch': 260.32}\n",
            "                                          {'loss': 0.3753, 'learning_rate': 1.1126733465752987e-05, 'epoch': 260.65}\n",
            "{'loss': 0.3922, 'learning_rate': 1.1081075970414417e-05, 'epoch': 260.97}\n",
            " 65% 8091/12400 [2:33:45<40:25,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:47:47,233 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:47:47,233 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:47:47,233 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                          \n",
            " 65% 8091/12400 [2:33:49<40:25,  1.78it/s]\n",
            "{'eval_loss': 0.3751293420791626, 'eval_runtime': 3.8877, 'eval_samples_per_second': 56.332, 'eval_steps_per_second': 1.029, 'epoch': 261.0}\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:47:51,122 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8091\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:47:51,125 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8091/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:47:53,040 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8091/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:47:53,041 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8091/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:47:57,378 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8029] due to args.save_total_limit\n",
            " 65% 8100/12400 [2:34:05<1:01:58,  1.16it/s]{'loss': 0.3804, 'learning_rate': 1.1035473018618544e-05, 'epoch': 261.29}\n",
            "{'loss': 0.3848, 'learning_rate': 1.0989924934706365e-05, 'epoch': 261.61}\n",
            " 65% 8120/12400 [2:34:17<40:18,  1.77it/s]{'loss': 0.3843, 'learning_rate': 1.0944432042628652e-05, 'epoch': 261.94}\n",
            " 66% 8122/12400 [2:34:18<40:13,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:48:20,254 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:48:20,255 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:48:20,255 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3699621260166168, 'eval_runtime': 3.8763, 'eval_samples_per_second': 56.497, 'eval_steps_per_second': 1.032, 'epoch': 262.0}\n",
            " 66% 8122/12400 [2:34:22<40:13,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:48:24,133 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8122\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:48:24,136 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8122/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:48:26,012 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8122/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:48:26,013 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8122/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:48:30,477 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-5952] due to args.save_total_limit\n",
            "{'loss': 0.3901, 'learning_rate': 1.0898994665943643e-05, 'epoch': 262.26}\n",
            "{'loss': 0.3841, 'learning_rate': 1.0853613127814725e-05, 'epoch': 262.58}\n",
            "                                          {'loss': 0.3848, 'learning_rate': 1.0808287751008153e-05, 'epoch': 262.9}\n",
            " 66% 8153/12400 [2:34:51<39:42,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:48:53,356 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:48:53,356 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:48:53,356 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            " 66% 8153/12400 [2:34:56<39:42,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]{'eval_loss': 0.37798672914505005, 'eval_runtime': 3.9516, 'eval_samples_per_second': 55.421, 'eval_steps_per_second': 1.012, 'epoch': 263.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:48:57,310 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8153\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:48:57,311 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8153/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:48:59,288 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8153/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:48:59,289 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8153/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:49:03,605 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8060] due to args.save_total_limit\n",
            "                                            {'loss': 0.3826, 'learning_rate': 1.0763018857890742e-05, 'epoch': 263.23}\n",
            " 66% 8170/12400 [2:35:17<47:43,  1.48it/s]{'loss': 0.3784, 'learning_rate': 1.0717806770427578e-05, 'epoch': 263.55}\n",
            "{'loss': 0.3845, 'learning_rate': 1.0672651810179738e-05, 'epoch': 263.87}\n",
            " 66% 8184/12400 [2:35:25<39:42,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 11:49:27,137 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:49:27,138 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:49:27,138 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.60it/s]\u001b[A\n",
            "                                          \n",
            " 66% 8184/12400 [2:35:29<39:42,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:49:31,069 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8184\n",
            "{'eval_loss': 0.37862449884414673, 'eval_runtime': 3.9302, 'eval_samples_per_second': 55.722, 'eval_steps_per_second': 1.018, 'epoch': 264.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:49:31,072 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8184/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:49:32,955 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8184/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:49:32,956 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8184/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:49:37,361 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8091] due to args.save_total_limit\n",
            " 66% 8190/12400 [2:35:43<1:32:59,  1.33s/it]{'loss': 0.376, 'learning_rate': 1.0627554298301972e-05, 'epoch': 264.19}\n",
            "                                          {'loss': 0.3989, 'learning_rate': 1.0582514555540484e-05, 'epoch': 264.52}\n",
            " 66% 8210/12400 [2:35:55<40:49,  1.71it/s]{'loss': 0.3874, 'learning_rate': 1.0537532902230554e-05, 'epoch': 264.84}\n",
            " 66% 8215/12400 [2:35:58<38:23,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:50:00,072 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:50:00,073 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:50:00,073 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                          \n",
            " 66% 8215/12400 [2:36:02<38:23,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "{'eval_loss': 0.38073208928108215, 'eval_runtime': 3.8995, 'eval_samples_per_second': 56.162, 'eval_steps_per_second': 1.026, 'epoch': 265.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:50:03,974 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8215\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:50:03,976 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8215/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:50:05,874 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8215/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:50:05,875 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8215/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:50:10,160 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8153] due to args.save_total_limit\n",
            "                                            {'loss': 0.3752, 'learning_rate': 1.0492609658294353e-05, 'epoch': 265.16}\n",
            " 66% 8230/12400 [2:36:22<45:13,  1.54it/s]{'loss': 0.3852, 'learning_rate': 1.044774514323862e-05, 'epoch': 265.48}\n",
            "{'loss': 0.3816, 'learning_rate': 1.0402939676152385e-05, 'epoch': 265.81}\n",
            " 66% 8246/12400 [2:36:31<38:39,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:50:33,370 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:50:33,371 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:50:33,371 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 0.3752702474594116, 'eval_runtime': 3.8777, 'eval_samples_per_second': 56.477, 'eval_steps_per_second': 1.032, 'epoch': 266.0}\n",
            " 66% 8246/12400 [2:36:35<38:39,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:50:37,250 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8246\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:50:37,253 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8246/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:50:39,230 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8246/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:50:39,232 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8246/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:50:43,528 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8184] due to args.save_total_limit\n",
            "{'loss': 0.3885, 'learning_rate': 1.0358193575704722e-05, 'epoch': 266.13}\n",
            "{'loss': 0.3836, 'learning_rate': 1.0313507160142473e-05, 'epoch': 266.45}\n",
            "                                          {'loss': 0.3831, 'learning_rate': 1.0268880747287982e-05, 'epoch': 266.77}\n",
            " 67% 8277/12400 [2:37:04<37:46,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:51:06,577 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:51:06,577 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:51:06,577 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                          \n",
            " 67% 8277/12400 [2:37:09<37:46,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.96it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.37706977128982544, 'eval_runtime': 3.8854, 'eval_samples_per_second': 56.365, 'eval_steps_per_second': 1.029, 'epoch': 267.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 11:51:10,464 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8277\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:51:10,465 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8277/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:51:12,368 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8277/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:51:12,369 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8277/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:51:16,583 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8215] due to args.save_total_limit\n",
            "                                            {'loss': 0.3865, 'learning_rate': 1.0224314654536837e-05, 'epoch': 267.1}\n",
            " 67% 8290/12400 [2:37:27<46:53,  1.46it/s]{'loss': 0.3855, 'learning_rate': 1.0179809198855614e-05, 'epoch': 267.42}\n",
            "{'loss': 0.3813, 'learning_rate': 1.0135364696779607e-05, 'epoch': 267.74}\n",
            " 67% 8308/12400 [2:37:37<37:17,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 11:51:39,486 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:51:39,487 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:51:39,487 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3834283947944641, 'eval_runtime': 3.8784, 'eval_samples_per_second': 56.466, 'eval_steps_per_second': 1.031, 'epoch': 268.0}\n",
            " 67% 8308/12400 [2:37:42<37:17,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:51:43,367 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8308\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:51:43,368 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8308/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:51:45,270 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8308/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:51:45,271 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8308/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:51:49,972 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8246] due to args.save_total_limit\n",
            "{'loss': 0.3891, 'learning_rate': 1.0090981464410635e-05, 'epoch': 268.06}\n",
            "                                          {'loss': 0.3814, 'learning_rate': 1.0046659817414688e-05, 'epoch': 268.39}\n",
            " 67% 8330/12400 [2:38:06<40:47,  1.66it/s]{'loss': 0.3755, 'learning_rate': 1.0002400071019778e-05, 'epoch': 268.71}\n",
            " 67% 8339/12400 [2:38:11<36:41,  1.84it/s][INFO|trainer.py:2550] 2022-05-10 11:52:12,680 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:52:12,680 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:52:12,680 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.86it/s]\u001b[A\n",
            "                                          \n",
            " 67% 8339/12400 [2:38:15<36:41,  1.84it/s]\n",
            "{'eval_loss': 0.3782910108566284, 'eval_runtime': 3.8638, 'eval_samples_per_second': 56.68, 'eval_steps_per_second': 1.035, 'epoch': 269.0}\n",
            "100% 4/4 [00:00<00:00,  8.96it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:52:16,545 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8339\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:52:16,547 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8339/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:52:18,450 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8339/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:52:18,451 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8339/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:52:23,005 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8277] due to args.save_total_limit\n",
            "                                            {'loss': 0.3914, 'learning_rate': 9.958202540013681e-06, 'epoch': 269.03}\n",
            "{'loss': 0.3801, 'learning_rate': 9.914067538741628e-06, 'epoch': 269.35}\n",
            "{'loss': 0.3881, 'learning_rate': 9.86999538110417e-06, 'epoch': 269.68}\n",
            " 68% 8370/12400 [2:38:45<36:58,  1.82it/s]{'loss': 0.385, 'learning_rate': 9.82598638055487e-06, 'epoch': 270.0}\n",
            "[INFO|trainer.py:2550] 2022-05-10 11:52:46,268 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:52:46,268 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:52:46,268 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                          \n",
            " 68% 8370/12400 [2:38:48<36:58,  1.82it/s]\n",
            "{'eval_loss': 0.37746474146842957, 'eval_runtime': 3.8817, 'eval_samples_per_second': 56.419, 'eval_steps_per_second': 1.03, 'epoch': 270.0}\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:52:50,151 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8370\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:52:50,153 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8370/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:52:52,077 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8370/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:52:52,078 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8370/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:52:56,563 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8308] due to args.save_total_limit\n",
            " 68% 8380/12400 [2:39:05<52:42,  1.27it/s]{'loss': 0.3806, 'learning_rate': 9.782040850098107e-06, 'epoch': 270.32}\n",
            "                                          {'loss': 0.3868, 'learning_rate': 9.73815910228684e-06, 'epoch': 270.65}\n",
            " 68% 8400/12400 [2:39:17<36:39,  1.82it/s]{'loss': 0.3852, 'learning_rate': 9.694341449220397e-06, 'epoch': 270.97}\n",
            " 68% 8401/12400 [2:39:17<37:47,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 11:53:19,441 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:53:19,441 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:53:19,441 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                          \n",
            " 68% 8401/12400 [2:39:22<37:47,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]{'eval_loss': 0.3693092167377472, 'eval_runtime': 3.9756, 'eval_samples_per_second': 55.086, 'eval_steps_per_second': 1.006, 'epoch': 271.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:53:23,418 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8401\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:53:23,420 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8401/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:53:25,328 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8401/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:53:25,329 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8401/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:53:29,936 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8122] due to args.save_total_limit\n",
            "{'loss': 0.3836, 'learning_rate': 9.650588202542233e-06, 'epoch': 271.29}\n",
            "                                          {'loss': 0.3824, 'learning_rate': 9.606899673437733e-06, 'epoch': 271.61}\n",
            "                                          {'loss': 0.3847, 'learning_rate': 9.563276172632e-06, 'epoch': 271.94}\n",
            " 68% 8432/12400 [2:39:51<37:39,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 11:53:52,993 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:53:52,994 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:53:52,994 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "                                          \n",
            " 68% 8432/12400 [2:39:55<37:39,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]{'eval_loss': 0.3803301155567169, 'eval_runtime': 3.931, 'eval_samples_per_second': 55.712, 'eval_steps_per_second': 1.018, 'epoch': 272.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:53:56,926 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8432\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:53:56,928 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8432/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:53:58,797 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8432/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:53:58,798 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8432/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:54:03,081 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8339] due to args.save_total_limit\n",
            " 68% 8440/12400 [2:40:10<1:02:49,  1.05it/s]{'loss': 0.3877, 'learning_rate': 9.519718010387613e-06, 'epoch': 272.26}\n",
            " 68% 8450/12400 [2:40:17<40:49,  1.61it/s]{'loss': 0.3823, 'learning_rate': 9.476225496502488e-06, 'epoch': 272.58}\n",
            "                                          {'loss': 0.3915, 'learning_rate': 9.432798940307587e-06, 'epoch': 272.9}\n",
            " 68% 8463/12400 [2:40:24<36:43,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:54:26,012 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:54:26,012 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:54:26,012 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.30it/s]\u001b[A\n",
            "                                          \n",
            " 68% 8463/12400 [2:40:28<36:43,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A{'eval_loss': 0.3880590796470642, 'eval_runtime': 3.9102, 'eval_samples_per_second': 56.007, 'eval_steps_per_second': 1.023, 'epoch': 273.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:54:29,924 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8463\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:54:29,927 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8463/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:54:31,842 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8463/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:54:31,843 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8463/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:54:36,362 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8370] due to args.save_total_limit\n",
            " 68% 8470/12400 [2:40:42<1:12:17,  1.10s/it]{'loss': 0.3779, 'learning_rate': 9.389438650664773e-06, 'epoch': 273.23}\n",
            "{'loss': 0.397, 'learning_rate': 9.346144935964641e-06, 'epoch': 273.55}\n",
            " 68% 8490/12400 [2:40:55<40:14,  1.62it/s]{'loss': 0.3841, 'learning_rate': 9.302918104124222e-06, 'epoch': 273.87}\n",
            " 68% 8494/12400 [2:40:57<36:38,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:54:59,622 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:54:59,622 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:54:59,622 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                          \n",
            " 68% 8494/12400 [2:41:02<36:38,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "{'eval_loss': 0.3748118579387665, 'eval_runtime': 4.0134, 'eval_samples_per_second': 54.567, 'eval_steps_per_second': 0.997, 'epoch': 274.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:55:03,637 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8494\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:55:03,639 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8494/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:55:05,593 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8494/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:55:05,594 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8494/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:55:09,929 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8432] due to args.save_total_limit\n",
            " 69% 8500/12400 [2:41:16<1:26:55,  1.34s/it]{'loss': 0.3709, 'learning_rate': 9.25975846258491e-06, 'epoch': 274.19}\n",
            "{'loss': 0.3836, 'learning_rate': 9.216666318310199e-06, 'epoch': 274.52}\n",
            "{'loss': 0.3785, 'learning_rate': 9.173641977783532e-06, 'epoch': 274.84}\n",
            " 69% 8525/12400 [2:41:31<36:17,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 11:55:33,333 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:55:33,333 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:55:33,333 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3811144232749939, 'eval_runtime': 3.9544, 'eval_samples_per_second': 55.382, 'eval_steps_per_second': 1.012, 'epoch': 275.0}\n",
            " 69% 8525/12400 [2:41:36<36:17,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:55:37,289 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8525\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:55:37,291 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8525/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:55:39,209 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8525/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:55:39,210 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8525/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:55:46,011 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8463] due to args.save_total_limit\n",
            "                                            {'loss': 0.3833, 'learning_rate': 9.130685747006101e-06, 'epoch': 275.16}\n",
            "{'loss': 0.3901, 'learning_rate': 9.087797931494695e-06, 'epoch': 275.48}\n",
            " 69% 8550/12400 [2:42:04<40:24,  1.59it/s]{'loss': 0.3746, 'learning_rate': 9.044978836279509e-06, 'epoch': 275.81}\n",
            " 69% 8556/12400 [2:42:07<35:43,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:56:09,214 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:56:09,215 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:56:09,215 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.49it/s]\u001b[A\n",
            "                                          \n",
            " 69% 8556/12400 [2:42:11<35:43,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]{'eval_loss': 0.37972214818000793, 'eval_runtime': 3.9203, 'eval_samples_per_second': 55.864, 'eval_steps_per_second': 1.02, 'epoch': 276.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:56:13,137 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8556\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:56:13,138 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8556/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:56:15,040 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8556/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:56:15,041 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8556/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:56:19,199 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8494] due to args.save_total_limit\n",
            " 69% 8560/12400 [2:42:23<2:09:58,  2.03s/it]{'loss': 0.3807, 'learning_rate': 9.002228765901973e-06, 'epoch': 276.13}\n",
            "{'loss': 0.383, 'learning_rate': 8.959548024412624e-06, 'epoch': 276.45}\n",
            "                                          {'loss': 0.3816, 'learning_rate': 8.916936915368856e-06, 'epoch': 276.77}\n",
            " 69% 8587/12400 [2:42:40<35:06,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 11:56:42,102 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:56:42,102 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:56:42,102 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.58it/s]\u001b[A\n",
            "                                          \n",
            " 69% 8587/12400 [2:42:44<35:06,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]{'eval_loss': 0.3824007213115692, 'eval_runtime': 3.9004, 'eval_samples_per_second': 56.148, 'eval_steps_per_second': 1.026, 'epoch': 277.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:56:46,004 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8587\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:56:46,006 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8587/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:56:47,911 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8587/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:56:47,912 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8587/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:56:52,446 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8525] due to args.save_total_limit\n",
            " 69% 8590/12400 [2:42:56<2:50:03,  2.68s/it]{'loss': 0.3915, 'learning_rate': 8.874395741832874e-06, 'epoch': 277.1}\n",
            "                                          {'loss': 0.3792, 'learning_rate': 8.831924806369458e-06, 'epoch': 277.42}\n",
            " 69% 8610/12400 [2:43:09<38:48,  1.63it/s]{'loss': 0.3849, 'learning_rate': 8.789524411043817e-06, 'epoch': 277.74}\n",
            " 70% 8618/12400 [2:43:13<34:23,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 11:57:15,334 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:57:15,334 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:57:15,335 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                          \n",
            " 70% 8618/12400 [2:43:17<34:23,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.38009825348854065, 'eval_runtime': 3.8633, 'eval_samples_per_second': 56.687, 'eval_steps_per_second': 1.035, 'epoch': 278.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 11:57:19,199 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8618\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:57:19,201 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8618/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:57:21,134 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8618/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:57:21,136 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8618/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:57:25,509 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8556] due to args.save_total_limit\n",
            "{'loss': 0.3871, 'learning_rate': 8.747194857419494e-06, 'epoch': 278.06}\n",
            "{'loss': 0.387, 'learning_rate': 8.704936446556165e-06, 'epoch': 278.39}\n",
            " 70% 8640/12400 [2:43:42<38:51,  1.61it/s]{'loss': 0.3801, 'learning_rate': 8.66274947900752e-06, 'epoch': 278.71}\n",
            " 70% 8649/12400 [2:43:46<34:19,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 11:57:48,625 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:57:48,625 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:57:48,625 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.52it/s]\u001b[A\n",
            "                                          \n",
            " 70% 8649/12400 [2:43:51<34:19,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:57:52,554 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8649\n",
            "{'eval_loss': 0.38286399841308594, 'eval_runtime': 3.9273, 'eval_samples_per_second': 55.763, 'eval_steps_per_second': 1.019, 'epoch': 279.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:57:52,556 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8649/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:57:54,474 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8649/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:57:54,475 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8649/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:57:58,790 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8587] due to args.save_total_limit\n",
            " 70% 8650/12400 [2:44:02<5:01:13,  4.82s/it]{'loss': 0.3873, 'learning_rate': 8.620634254819132e-06, 'epoch': 279.03}\n",
            "{'loss': 0.391, 'learning_rate': 8.578591073526304e-06, 'epoch': 279.35}\n",
            "{'loss': 0.3724, 'learning_rate': 8.536620234151959e-06, 'epoch': 279.68}\n",
            "                                          {'loss': 0.3908, 'learning_rate': 8.494722035204499e-06, 'epoch': 280.0}\n",
            " 70% 8680/12400 [2:44:20<33:56,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 11:58:22,055 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:58:22,055 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:58:22,055 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                          \n",
            " 70% 8680/12400 [2:44:24<33:56,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.380816251039505, 'eval_runtime': 3.9077, 'eval_samples_per_second': 56.043, 'eval_steps_per_second': 1.024, 'epoch': 280.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 11:58:25,964 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8680\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:58:25,966 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8680/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:58:27,848 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8680/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:58:27,849 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8680/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:58:32,087 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8618] due to args.save_total_limit\n",
            "                                          {'loss': 0.3909, 'learning_rate': 8.452896774675689e-06, 'epoch': 280.32}\n",
            "{'loss': 0.39, 'learning_rate': 8.41114475003853e-06, 'epoch': 280.65}\n",
            " 70% 8710/12400 [2:44:52<34:08,  1.80it/s]{'loss': 0.3807, 'learning_rate': 8.369466258245172e-06, 'epoch': 280.97}\n",
            " 70% 8711/12400 [2:44:53<35:09,  1.75it/s][INFO|trainer.py:2550] 2022-05-10 11:58:55,123 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:58:55,123 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:58:55,123 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.37908676266670227, 'eval_runtime': 3.872, 'eval_samples_per_second': 56.56, 'eval_steps_per_second': 1.033, 'epoch': 281.0}\n",
            " 70% 8711/12400 [2:44:57<35:09,  1.75it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:58:58,997 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8711\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:58:58,998 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8711/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:59:00,940 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8711/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:59:00,941 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8711/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:59:05,763 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8649] due to args.save_total_limit\n",
            " 70% 8720/12400 [2:45:13<54:08,  1.13it/s]{'loss': 0.3783, 'learning_rate': 8.327861595724733e-06, 'epoch': 281.29}\n",
            " 70% 8730/12400 [2:45:19<37:55,  1.61it/s]{'loss': 0.3766, 'learning_rate': 8.286331058381286e-06, 'epoch': 281.61}\n",
            " 70% 8740/12400 [2:45:26<33:50,  1.80it/s]{'loss': 0.392, 'learning_rate': 8.24487494159168e-06, 'epoch': 281.94}\n",
            " 70% 8742/12400 [2:45:26<34:05,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 11:59:28,648 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 11:59:28,648 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 11:59:28,648 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.81it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3817860186100006, 'eval_runtime': 3.8741, 'eval_samples_per_second': 56.529, 'eval_steps_per_second': 1.032, 'epoch': 282.0}\n",
            " 70% 8742/12400 [2:45:31<34:05,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 11:59:32,524 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8742\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 11:59:32,526 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8742/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 11:59:34,479 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8742/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 11:59:34,480 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8742/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 11:59:38,851 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8680] due to args.save_total_limit\n",
            "                                          {'loss': 0.383, 'learning_rate': 8.20349354020345e-06, 'epoch': 282.26}\n",
            " 71% 8760/12400 [2:45:52<37:33,  1.62it/s]{'loss': 0.3933, 'learning_rate': 8.16218714853277e-06, 'epoch': 282.58}\n",
            "                                          {'loss': 0.3733, 'learning_rate': 8.120956060362296e-06, 'epoch': 282.9}\n",
            " 71% 8773/12400 [2:45:59<33:32,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:00:01,552 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:00:01,553 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:00:01,553 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3737018406391144, 'eval_runtime': 3.9257, 'eval_samples_per_second': 55.786, 'eval_steps_per_second': 1.019, 'epoch': 283.0}\n",
            " 71% 8773/12400 [2:46:04<33:32,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:00:05,480 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8773\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:00:05,482 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8773/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:00:07,505 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8773/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:00:07,506 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8773/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:00:11,818 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8711] due to args.save_total_limit\n",
            "{'loss': 0.3888, 'learning_rate': 8.079800568939114e-06, 'epoch': 283.23}\n",
            " 71% 8790/12400 [2:46:25<39:33,  1.52it/s]{'loss': 0.3901, 'learning_rate': 8.03872096697264e-06, 'epoch': 283.55}\n",
            " 71% 8800/12400 [2:46:31<36:03,  1.66it/s]{'loss': 0.3738, 'learning_rate': 7.997717546632563e-06, 'epoch': 283.87}\n",
            " 71% 8804/12400 [2:46:33<33:26,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:00:35,127 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:00:35,127 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:00:35,128 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            " 71% 8804/12400 [2:46:37<33:26,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]{'eval_loss': 0.3776929974555969, 'eval_runtime': 3.9536, 'eval_samples_per_second': 55.392, 'eval_steps_per_second': 1.012, 'epoch': 284.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:00:39,083 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8804\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:00:39,084 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8804/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:00:41,015 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8804/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:00:41,016 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8804/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:00:45,413 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8742] due to args.save_total_limit\n",
            "{'loss': 0.3898, 'learning_rate': 7.956790599546712e-06, 'epoch': 284.19}\n",
            " 71% 8820/12400 [2:46:58<37:53,  1.57it/s]{'loss': 0.3822, 'learning_rate': 7.915940416799037e-06, 'epoch': 284.52}\n",
            "                                          {'loss': 0.389, 'learning_rate': 7.875167288927536e-06, 'epoch': 284.84}\n",
            " 71% 8835/12400 [2:47:06<32:51,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:01:08,636 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:01:08,636 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:01:08,637 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                          \n",
            " 71% 8835/12400 [2:47:11<32:51,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:01:12,610 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8835\n",
            "{'eval_loss': 0.3855108916759491, 'eval_runtime': 3.9716, 'eval_samples_per_second': 55.141, 'eval_steps_per_second': 1.007, 'epoch': 285.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:01:12,612 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8835/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:01:14,555 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8835/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:01:14,557 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8835/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:01:18,966 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8773] due to args.save_total_limit\n",
            "                                            {'loss': 0.3786, 'learning_rate': 7.83447150592212e-06, 'epoch': 285.16}\n",
            " 71% 8850/12400 [2:47:31<38:14,  1.55it/s]\n",
            "                                          {'loss': 0.3866, 'learning_rate': 7.753313131716837e-06, 'epoch': 285.81}\n",
            " 72% 8866/12400 [2:47:40<32:48,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:01:41,996 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:01:41,996 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:01:41,996 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.49it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.37539276480674744, 'eval_runtime': 3.9152, 'eval_samples_per_second': 55.936, 'eval_steps_per_second': 1.022, 'epoch': 286.0}\n",
            " 72% 8866/12400 [2:47:44<32:48,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:01:45,913 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8866\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:01:45,914 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8866/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:01:47,818 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8866/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:01:47,819 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8866/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:01:52,181 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8804] due to args.save_total_limit\n",
            "{'loss': 0.3954, 'learning_rate': 7.712851117738121e-06, 'epoch': 286.13}\n",
            " 72% 8880/12400 [2:48:03<38:23,  1.53it/s]{'loss': 0.3829, 'learning_rate': 7.672467603063728e-06, 'epoch': 286.45}\n",
            "{'loss': 0.3881, 'learning_rate': 7.632162874912565e-06, 'epoch': 286.77}\n",
            " 72% 8897/12400 [2:48:13<32:00,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:02:15,099 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:02:15,099 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:02:15,099 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.21it/s]\u001b[A\n",
            "                                          \n",
            " 72% 8897/12400 [2:48:17<32:00,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "{'eval_loss': 0.37288641929626465, 'eval_runtime': 3.9507, 'eval_samples_per_second': 55.433, 'eval_steps_per_second': 1.012, 'epoch': 287.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:02:19,051 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8897\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:02:19,053 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8897/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:02:20,971 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8897/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:02:20,972 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8897/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:02:25,298 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8835] due to args.save_total_limit\n",
            " 72% 8900/12400 [2:48:29<2:35:13,  2.66s/it]{'loss': 0.3789, 'learning_rate': 7.591937219943181e-06, 'epoch': 287.1}\n",
            " 72% 8910/12400 [2:48:35<40:13,  1.45it/s]{'loss': 0.3765, 'learning_rate': 7.5517909242517365e-06, 'epoch': 287.42}\n",
            " 72% 8920/12400 [2:48:42<35:08,  1.65it/s]{'loss': 0.3909, 'learning_rate': 7.511724273369974e-06, 'epoch': 287.74}\n",
            " 72% 8928/12400 [2:48:46<31:33,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 12:02:48,247 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:02:48,248 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:02:48,248 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.37920376658439636, 'eval_runtime': 3.9279, 'eval_samples_per_second': 55.755, 'eval_steps_per_second': 1.018, 'epoch': 288.0}\n",
            " 72% 8928/12400 [2:48:50<31:33,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:02:52,177 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8928\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:02:52,179 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8928/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:02:54,113 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8928/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:02:54,114 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8928/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:02:58,534 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8866] due to args.save_total_limit\n",
            "                                            {'loss': 0.3867, 'learning_rate': 7.4717375522631586e-06, 'epoch': 288.06}\n",
            " 72% 8932/12400 [2:49:03<2:02:16,  2.12s/it]\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ReadTimeout), entering retry loop.\n",
            "{'loss': 0.3812, 'learning_rate': 7.431831045328113e-06, 'epoch': 288.39}\n",
            " 72% 8950/12400 [2:49:15<35:11,  1.63it/s]{'loss': 0.3778, 'learning_rate': 7.392005036391109e-06, 'epoch': 288.71}\n",
            " 72% 8959/12400 [2:49:19<31:14,  1.84it/s][INFO|trainer.py:2550] 2022-05-10 12:03:21,628 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:03:21,629 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:03:21,629 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.32it/s]\u001b[A\n",
            "                                          \n",
            " 72% 8959/12400 [2:49:24<31:14,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.67it/s]{'eval_loss': 0.37331852316856384, 'eval_runtime': 3.9214, 'eval_samples_per_second': 55.847, 'eval_steps_per_second': 1.02, 'epoch': 289.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:03:25,552 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8959\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:03:25,553 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8959/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:03:27,477 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8959/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:03:27,478 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8959/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:03:32,155 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8897] due to args.save_total_limit\n",
            " 72% 8960/12400 [2:49:35<4:43:06,  4.94s/it]{'loss': 0.3939, 'learning_rate': 7.352259808705908e-06, 'epoch': 289.03}\n",
            "{'loss': 0.3797, 'learning_rate': 7.31259564495176e-06, 'epoch': 289.35}\n",
            " 72% 8980/12400 [2:49:48<38:22,  1.49it/s]{'loss': 0.3735, 'learning_rate': 7.27301282723132e-06, 'epoch': 289.68}\n",
            " 72% 8990/12400 [2:49:53<31:22,  1.81it/s]{'loss': 0.3884, 'learning_rate': 7.2335116370687274e-06, 'epoch': 290.0}\n",
            " 72% 8990/12400 [2:49:54<31:22,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:03:55,548 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:03:55,548 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:03:55,548 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            " 72% 8990/12400 [2:49:58<31:22,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:03:59,460 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-8990\n",
            "{'eval_loss': 0.3804386556148529, 'eval_runtime': 3.9104, 'eval_samples_per_second': 56.005, 'eval_steps_per_second': 1.023, 'epoch': 290.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:03:59,462 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-8990/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:04:01,460 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-8990/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:04:01,461 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-8990/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:04:05,957 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8928] due to args.save_total_limit\n",
            "{'loss': 0.3766, 'learning_rate': 7.19409235540754e-06, 'epoch': 290.32}\n",
            "                                          {'loss': 0.3871, 'learning_rate': 7.154755262608768e-06, 'epoch': 290.65}\n",
            "{'loss': 0.387, 'learning_rate': 7.115500638448866e-06, 'epoch': 290.97}\n",
            " 73% 9021/12400 [2:50:27<32:58,  1.71it/s][INFO|trainer.py:2550] 2022-05-10 12:04:29,514 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:04:29,515 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:04:29,515 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.59it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 0.3822777569293976, 'eval_runtime': 4.079, 'eval_samples_per_second': 53.69, 'eval_steps_per_second': 0.981, 'epoch': 291.0}\n",
            " 73% 9021/12400 [2:50:32<32:58,  1.71it/s]\n",
            "100% 4/4 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:04:33,596 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9021\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:04:33,598 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9021/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:04:35,626 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9021/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:04:35,628 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9021/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:04:40,126 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8959] due to args.save_total_limit\n",
            " 73% 9030/12400 [2:50:48<54:23,  1.03it/s]{'loss': 0.3821, 'learning_rate': 7.076328762117756e-06, 'epoch': 291.29}\n",
            "                                          {'loss': 0.3856, 'learning_rate': 7.037239912216824e-06, 'epoch': 291.61}\n",
            " 73% 9050/12400 [2:51:01<31:41,  1.76it/s]{'loss': 0.3787, 'learning_rate': 6.998234366756954e-06, 'epoch': 291.94}\n",
            " 73% 9052/12400 [2:51:02<31:37,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 12:05:03,917 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:05:03,917 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:05:03,917 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                          \n",
            " 73% 9052/12400 [2:51:06<31:37,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.3861502707004547, 'eval_runtime': 3.9221, 'eval_samples_per_second': 55.838, 'eval_steps_per_second': 1.02, 'epoch': 292.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 12:05:07,841 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9052\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:05:07,843 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9052/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:05:09,782 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9052/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:05:09,783 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9052/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:05:15,023 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8990] due to args.save_total_limit\n",
            "{'loss': 0.3904, 'learning_rate': 6.959312403156561e-06, 'epoch': 292.26}\n",
            "{'loss': 0.3745, 'learning_rate': 6.920474298239564e-06, 'epoch': 292.58}\n",
            " 73% 9080/12400 [2:51:35<32:02,  1.73it/s]{'loss': 0.3842, 'learning_rate': 6.881720328233498e-06, 'epoch': 292.9}\n",
            " 73% 9083/12400 [2:51:36<30:58,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 12:05:38,236 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:05:38,236 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:05:38,236 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.37987369298934937, 'eval_runtime': 3.9215, 'eval_samples_per_second': 55.845, 'eval_steps_per_second': 1.02, 'epoch': 293.0}\n",
            " 73% 9083/12400 [2:51:40<30:58,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:05:42,159 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9083\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:05:42,160 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9083/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:05:44,130 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9083/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:05:44,131 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9083/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:05:48,428 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9021] due to args.save_total_limit\n",
            " 73% 9090/12400 [2:51:55<1:00:51,  1.10s/it]{'loss': 0.3832, 'learning_rate': 6.843050768767486e-06, 'epoch': 293.23}\n",
            "                                          {'loss': 0.3778, 'learning_rate': 6.804465894870284e-06, 'epoch': 293.55}\n",
            "{'loss': 0.3881, 'learning_rate': 6.765965980968375e-06, 'epoch': 293.87}\n",
            " 74% 9114/12400 [2:52:09<30:27,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:06:11,530 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:06:11,530 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:06:11,530 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                          \n",
            " 74% 9114/12400 [2:52:14<30:27,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]{'eval_loss': 0.3755342662334442, 'eval_runtime': 3.9749, 'eval_samples_per_second': 55.096, 'eval_steps_per_second': 1.006, 'epoch': 294.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:06:15,507 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9114\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:06:15,509 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9114/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:06:17,442 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9114/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:06:17,443 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9114/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:06:21,908 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9052] due to args.save_total_limit\n",
            "{'loss': 0.3866, 'learning_rate': 6.727551300883962e-06, 'epoch': 294.19}\n",
            "{'loss': 0.3848, 'learning_rate': 6.689222127833039e-06, 'epoch': 294.52}\n",
            "{'loss': 0.3816, 'learning_rate': 6.650978734423461e-06, 'epoch': 294.84}\n",
            " 74% 9145/12400 [2:52:43<30:23,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 12:06:44,906 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:06:44,906 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:06:44,906 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.60it/s]\u001b[A\n",
            "                                          \n",
            " 74% 9145/12400 [2:52:47<30:23,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.71it/s]\u001b[A\n",
            "{'eval_loss': 0.37927740812301636, 'eval_runtime': 3.95, 'eval_samples_per_second': 55.443, 'eval_steps_per_second': 1.013, 'epoch': 295.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:06:48,858 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9145\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:06:48,860 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9145/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:06:50,794 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9145/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:06:50,795 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9145/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:06:55,403 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9083] due to args.save_total_limit\n",
            "{'loss': 0.3782, 'learning_rate': 6.612821392652985e-06, 'epoch': 295.16}\n",
            "                                          {'loss': 0.385, 'learning_rate': 6.574750373907349e-06, 'epoch': 295.48}\n",
            " 74% 9170/12400 [2:53:14<33:59,  1.58it/s]{'loss': 0.3807, 'learning_rate': 6.536765948958336e-06, 'epoch': 295.81}\n",
            " 74% 9176/12400 [2:53:16<29:59,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:07:18,592 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:07:18,592 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:07:18,592 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3823188543319702, 'eval_runtime': 3.9367, 'eval_samples_per_second': 55.63, 'eval_steps_per_second': 1.016, 'epoch': 296.0}\n",
            " 74% 9176/12400 [2:53:21<29:59,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:07:22,530 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9176\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:07:22,532 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9176/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:07:24,474 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9176/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:07:24,475 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9176/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:07:28,848 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9114] due to args.save_total_limit\n",
            " 74% 9180/12400 [2:53:33<1:51:10,  2.07s/it]{'loss': 0.3877, 'learning_rate': 6.49886838796185e-06, 'epoch': 296.13}\n",
            "                                          {'loss': 0.3787, 'learning_rate': 6.461057960455989e-06, 'epoch': 296.45}\n",
            "{'loss': 0.3865, 'learning_rate': 6.4233349353591565e-06, 'epoch': 296.77}\n",
            " 74% 9207/12400 [2:53:50<29:21,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:07:51,827 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:07:51,828 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:07:51,828 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            "                                 {'eval_loss': 0.38172033429145813, 'eval_runtime': 4.1116, 'eval_samples_per_second': 53.264, 'eval_steps_per_second': 0.973, 'epoch': 297.0}\n",
            " 74% 9207/12400 [2:53:54<29:21,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:07:55,941 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9207\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:07:55,943 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9207/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:07:57,918 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9207/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:07:57,919 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9207/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:08:02,400 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9145] due to args.save_total_limit\n",
            " 74% 9210/12400 [2:54:07<2:25:56,  2.75s/it]{'loss': 0.3839, 'learning_rate': 6.38569958096808e-06, 'epoch': 297.1}\n",
            "{'loss': 0.3776, 'learning_rate': 6.348152164955999e-06, 'epoch': 297.42}\n",
            "                                          {'loss': 0.385, 'learning_rate': 6.3106929543706845e-06, 'epoch': 297.74}\n",
            " 74% 9238/12400 [2:54:24<29:15,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:08:25,755 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:08:25,756 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:08:25,756 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 0.3781501054763794, 'eval_runtime': 3.9141, 'eval_samples_per_second': 55.952, 'eval_steps_per_second': 1.022, 'epoch': 298.0}\n",
            " 74% 9238/12400 [2:54:28<29:15,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:08:29,671 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9238\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:08:29,673 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9238/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:08:31,652 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9238/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:08:31,653 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9238/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:08:36,345 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9176] due to args.save_total_limit\n",
            " 75% 9240/12400 [2:54:39<3:11:19,  3.63s/it]{'loss': 0.3899, 'learning_rate': 6.273322215632548e-06, 'epoch': 298.06}\n",
            "{'loss': 0.3773, 'learning_rate': 6.236040214532806e-06, 'epoch': 298.39}\n",
            "{'loss': 0.3772, 'learning_rate': 6.198847216231519e-06, 'epoch': 298.71}\n",
            " 75% 9269/12400 [2:54:57<28:51,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:08:59,497 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:08:59,497 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:08:59,497 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3688541352748871, 'eval_runtime': 3.8952, 'eval_samples_per_second': 56.224, 'eval_steps_per_second': 1.027, 'epoch': 299.0}\n",
            " 75% 9269/12400 [2:55:02<28:51,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:09:03,394 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9269\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:09:03,395 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9269/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:09:05,349 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9269/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:09:05,350 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9269/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:09:09,934 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-8401] due to args.save_total_limit\n",
            "{'loss': 0.3832, 'learning_rate': 6.161743485255742e-06, 'epoch': 299.03}\n",
            "{'loss': 0.382, 'learning_rate': 6.124729285497636e-06, 'epoch': 299.35}\n",
            "                                          {'loss': 0.3783, 'learning_rate': 6.087804880212592e-06, 'epoch': 299.68}\n",
            " 75% 9300/12400 [2:55:31<28:10,  1.83it/s]{'loss': 0.39, 'learning_rate': 6.050970532017358e-06, 'epoch': 300.0}\n",
            " 75% 9300/12400 [2:55:31<28:10,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 12:09:32,992 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:09:32,992 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:09:32,992 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.40it/s]\u001b[A\n",
            "                                          \n",
            " 75% 9300/12400 [2:55:35<28:10,  1.83it/s]\n",
            "{'eval_loss': 0.3808518648147583, 'eval_runtime': 3.9885, 'eval_samples_per_second': 54.908, 'eval_steps_per_second': 1.003, 'epoch': 300.0}\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:09:36,982 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9300\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:09:36,984 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9300/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:09:38,958 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9300/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:09:38,959 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9300/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:09:43,315 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9207] due to args.save_total_limit\n",
            "                                          {'loss': 0.3852, 'learning_rate': 6.014226502888159e-06, 'epoch': 300.32}\n",
            "{'loss': 0.3838, 'learning_rate': 5.977573054158873e-06, 'epoch': 300.65}\n",
            "{'loss': 0.3809, 'learning_rate': 5.941010446519107e-06, 'epoch': 300.97}\n",
            " 75% 9331/12400 [2:56:07<29:29,  1.73it/s][INFO|trainer.py:2550] 2022-05-10 12:10:08,858 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:10:08,858 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:10:08,858 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                          \n",
            " 75% 9331/12400 [2:56:11<29:29,  1.73it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "{'eval_loss': 0.3829798996448517, 'eval_runtime': 4.0047, 'eval_samples_per_second': 54.685, 'eval_steps_per_second': 0.999, 'epoch': 301.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:10:12,865 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9331\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:10:12,866 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9331/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:10:14,829 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9331/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:10:14,830 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9331/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:10:19,047 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9238] due to args.save_total_limit\n",
            "{'loss': 0.3825, 'learning_rate': 5.9045389400124185e-06, 'epoch': 301.29}\n",
            " 75% 9350/12400 [2:56:33<31:56,  1.59it/s]{'loss': 0.3785, 'learning_rate': 5.868158794034401e-06, 'epoch': 301.61}\n",
            "{'loss': 0.3862, 'learning_rate': 5.8318702673308755e-06, 'epoch': 301.94}\n",
            " 76% 9362/12400 [2:56:40<28:39,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 12:10:42,442 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:10:42,442 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:10:42,442 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                          \n",
            " 76% 9362/12400 [2:56:45<28:39,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.3758845925331116, 'eval_runtime': 3.921, 'eval_samples_per_second': 55.853, 'eval_steps_per_second': 1.02, 'epoch': 302.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 12:10:46,365 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9362\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:10:46,366 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9362/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:10:48,286 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9362/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:10:48,287 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9362/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:10:52,614 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9300] due to args.save_total_limit\n",
            "{'loss': 0.3778, 'learning_rate': 5.795673617996046e-06, 'epoch': 302.26}\n",
            "{'loss': 0.3882, 'learning_rate': 5.759569103470649e-06, 'epoch': 302.58}\n",
            "                                          {'loss': 0.3905, 'learning_rate': 5.7235569805401384e-06, 'epoch': 302.9}\n",
            " 76% 9393/12400 [2:57:13<28:11,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 12:11:15,512 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:11:15,512 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:11:15,512 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3704105317592621, 'eval_runtime': 3.8971, 'eval_samples_per_second': 56.196, 'eval_steps_per_second': 1.026, 'epoch': 303.0}\n",
            " 76% 9393/12400 [2:57:18<28:11,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:11:19,411 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9393\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:11:19,413 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9393/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:11:21,326 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9393/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:11:21,327 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9393/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:11:25,698 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9331] due to args.save_total_limit\n",
            "{'loss': 0.3728, 'learning_rate': 5.6876375053328555e-06, 'epoch': 303.23}\n",
            " 76% 9410/12400 [2:57:39<32:08,  1.55it/s]{'loss': 0.378, 'learning_rate': 5.651810933318196e-06, 'epoch': 303.55}\n",
            "{'loss': 0.3935, 'learning_rate': 5.616077519304806e-06, 'epoch': 303.87}\n",
            " 76% 9424/12400 [2:57:46<27:32,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:11:48,618 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:11:48,618 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:11:48,618 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                          \n",
            " 76% 9424/12400 [2:57:51<27:32,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A{'eval_loss': 0.3835941255092621, 'eval_runtime': 3.9258, 'eval_samples_per_second': 55.785, 'eval_steps_per_second': 1.019, 'epoch': 304.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:11:52,545 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9424\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:11:52,547 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9424/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:11:54,496 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9424/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:11:54,497 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9424/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:11:58,917 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9362] due to args.save_total_limit\n",
            " 76% 9430/12400 [2:58:04<1:05:56,  1.33s/it]{'loss': 0.3853, 'learning_rate': 5.580437517438781e-06, 'epoch': 304.19}\n",
            "                                          {'loss': 0.3838, 'learning_rate': 5.544891181201816e-06, 'epoch': 304.52}\n",
            "                                          {'loss': 0.3862, 'learning_rate': 5.509438763409441e-06, 'epoch': 304.84}\n",
            " 76% 9455/12400 [2:58:20<27:13,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:12:22,025 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:12:22,025 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:12:22,025 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "                                          \n",
            " 76% 9455/12400 [2:58:24<27:13,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:12:25,944 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9455\n",
            "{'eval_loss': 0.383124440908432, 'eval_runtime': 3.9175, 'eval_samples_per_second': 55.903, 'eval_steps_per_second': 1.021, 'epoch': 305.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:12:25,946 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9455/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:12:27,903 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9455/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:12:27,904 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9455/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:12:32,484 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9393] due to args.save_total_limit\n",
            "{'loss': 0.3843, 'learning_rate': 5.474080516209229e-06, 'epoch': 305.16}\n",
            "                                          {'loss': 0.3809, 'learning_rate': 5.4388166910789486e-06, 'epoch': 305.48}\n",
            "                                          {'loss': 0.397, 'learning_rate': 5.403647538824851e-06, 'epoch': 305.81}\n",
            " 76% 9486/12400 [2:58:53<26:43,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:12:55,538 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:12:55,539 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:12:55,539 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                          \n",
            " 76% 9486/12400 [2:58:58<26:43,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]{'eval_loss': 0.37276825308799744, 'eval_runtime': 3.9409, 'eval_samples_per_second': 55.572, 'eval_steps_per_second': 1.015, 'epoch': 306.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:12:59,481 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9486\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:12:59,483 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9486/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:13:01,441 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9486/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:13:01,442 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9486/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:13:05,718 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9424] due to args.save_total_limit\n",
            " 77% 9490/12400 [2:59:11<1:39:40,  2.06s/it]{'loss': 0.3715, 'learning_rate': 5.368573309579827e-06, 'epoch': 306.13}\n",
            " 77% 9500/12400 [2:59:17<31:27,  1.54it/s]{'loss': 0.3955, 'learning_rate': 5.333594252801638e-06, 'epoch': 306.45}\n",
            " 77% 9510/12400 [2:59:23<29:44,  1.62it/s]{'loss': 0.3818, 'learning_rate': 5.2987106172711815e-06, 'epoch': 306.77}\n",
            " 77% 9517/12400 [2:59:27<26:21,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:13:28,694 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:13:28,694 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:13:28,694 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                          \n",
            " 77% 9517/12400 [2:59:31<26:21,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A{'eval_loss': 0.37553516030311584, 'eval_runtime': 3.9121, 'eval_samples_per_second': 55.98, 'eval_steps_per_second': 1.022, 'epoch': 307.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:13:32,608 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9517\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:13:32,609 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9517/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:13:34,569 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9517/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:13:34,570 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9517/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:13:41,744 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9455] due to args.save_total_limit\n",
            " 77% 9520/12400 [2:59:45<2:27:52,  3.08s/it]{'loss': 0.3887, 'learning_rate': 5.263922651090666e-06, 'epoch': 307.1}\n",
            "{'loss': 0.3843, 'learning_rate': 5.2292306016818854e-06, 'epoch': 307.42}\n",
            " 77% 9540/12400 [2:59:59<29:24,  1.62it/s]{'loss': 0.3829, 'learning_rate': 5.194634715784443e-06, 'epoch': 307.74}\n",
            " 77% 9548/12400 [3:00:03<26:21,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:14:04,918 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:14:04,918 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:14:04,918 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.36473390460014343, 'eval_runtime': 3.9142, 'eval_samples_per_second': 55.95, 'eval_steps_per_second': 1.022, 'epoch': 308.0}\n",
            " 77% 9548/12400 [3:00:07<26:21,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:14:08,834 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9548\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:14:08,836 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9548/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:14:10,800 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9548/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:14:10,801 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9548/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:14:15,192 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9269] due to args.save_total_limit\n",
            "{'loss': 0.3839, 'learning_rate': 5.160135239454004e-06, 'epoch': 308.06}\n",
            " 77% 9560/12400 [3:00:25<32:38,  1.45it/s]{'loss': 0.3739, 'learning_rate': 5.125732418060533e-06, 'epoch': 308.39}\n",
            " 77% 9570/12400 [3:00:31<29:17,  1.61it/s]{'loss': 0.3922, 'learning_rate': 5.091426496286566e-06, 'epoch': 308.71}\n",
            " 77% 9579/12400 [3:00:36<25:54,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:14:38,278 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:14:38,278 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:14:38,278 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.38138294219970703, 'eval_runtime': 3.9706, 'eval_samples_per_second': 55.155, 'eval_steps_per_second': 1.007, 'epoch': 309.0}\n",
            " 77% 9579/12400 [3:00:40<25:54,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:14:42,251 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9579\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:14:42,252 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9579/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:14:44,231 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9579/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:14:44,232 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9579/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:14:48,818 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9486] due to args.save_total_limit\n",
            "                                            {'loss': 0.3834, 'learning_rate': 5.0572177181254606e-06, 'epoch': 309.03}\n",
            "                                          {'loss': 0.3816, 'learning_rate': 5.02310632687965e-06, 'epoch': 309.35}\n",
            "{'loss': 0.3798, 'learning_rate': 4.989092565158954e-06, 'epoch': 309.68}\n",
            " 78% 9610/12400 [3:01:10<25:18,  1.84it/s]{'loss': 0.3869, 'learning_rate': 4.955176674878783e-06, 'epoch': 310.0}\n",
            "[INFO|trainer.py:2550] 2022-05-10 12:15:12,239 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:15:12,239 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:15:12,239 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.48it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.38303142786026, 'eval_runtime': 3.9522, 'eval_samples_per_second': 55.413, 'eval_steps_per_second': 1.012, 'epoch': 310.0}\n",
            " 78% 9610/12400 [3:01:14<25:18,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:15:16,193 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9610\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:15:16,195 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9610/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:15:18,134 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9610/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:15:18,135 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9610/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:15:22,684 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9517] due to args.save_total_limit\n",
            "{'loss': 0.3913, 'learning_rate': 4.921358897258491e-06, 'epoch': 310.32}\n",
            " 78% 9630/12400 [3:01:37<28:16,  1.63it/s]{'loss': 0.3782, 'learning_rate': 4.887639472819611e-06, 'epoch': 310.65}\n",
            "                                          {'loss': 0.3829, 'learning_rate': 4.854018641384169e-06, 'epoch': 310.97}\n",
            " 78% 9641/12400 [3:01:44<26:21,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 12:15:45,740 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:15:45,740 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:15:45,740 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "                                          \n",
            " 78% 9641/12400 [3:01:48<26:21,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]{'eval_loss': 0.3800510764122009, 'eval_runtime': 3.9053, 'eval_samples_per_second': 56.077, 'eval_steps_per_second': 1.024, 'epoch': 311.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:15:49,647 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9641\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:15:49,649 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9641/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:15:51,607 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9641/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:15:51,608 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9641/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:15:55,880 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9579] due to args.save_total_limit\n",
            "{'loss': 0.3878, 'learning_rate': 4.820496642072962e-06, 'epoch': 311.29}\n",
            "                                          {'loss': 0.3762, 'learning_rate': 4.787073713303869e-06, 'epoch': 311.61}\n",
            " 78% 9670/12400 [3:02:15<25:33,  1.78it/s]{'loss': 0.3826, 'learning_rate': 4.753750092790153e-06, 'epoch': 311.94}\n",
            " 78% 9672/12400 [3:02:17<25:38,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 12:16:18,767 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:16:18,767 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:16:18,767 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A{'eval_loss': 0.38476622104644775, 'eval_runtime': 3.9469, 'eval_samples_per_second': 55.486, 'eval_steps_per_second': 1.013, 'epoch': 312.0}\n",
            "                                          \n",
            " 78% 9672/12400 [3:02:21<25:38,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:16:22,716 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9672\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:16:22,717 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9672/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:16:24,660 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9672/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:16:24,661 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9672/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:16:29,099 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9610] due to args.save_total_limit\n",
            "                                          {'loss': 0.3818, 'learning_rate': 4.720526017538762e-06, 'epoch': 312.26}\n",
            " 78% 9690/12400 [3:02:43<27:54,  1.62it/s]{'loss': 0.3823, 'learning_rate': 4.687401723848668e-06, 'epoch': 312.58}\n",
            "{'loss': 0.3872, 'learning_rate': 4.654377447309144e-06, 'epoch': 312.9}\n",
            " 78% 9703/12400 [3:02:50<25:14,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 12:16:52,040 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:16:52,040 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:16:52,040 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                          \n",
            " 78% 9703/12400 [3:02:54<25:14,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]{'eval_loss': 0.37532323598861694, 'eval_runtime': 3.9575, 'eval_samples_per_second': 55.338, 'eval_steps_per_second': 1.011, 'epoch': 313.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:16:55,999 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9703\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:16:56,002 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9703/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:16:57,912 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9703/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:16:57,913 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9703/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:17:02,106 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9641] due to args.save_total_limit\n",
            " 78% 9710/12400 [3:03:09<49:18,  1.10s/it]{'loss': 0.3852, 'learning_rate': 4.621453422798121e-06, 'epoch': 313.23}\n",
            " 78% 9720/12400 [3:03:15<28:48,  1.55it/s]{'loss': 0.3848, 'learning_rate': 4.588629884480532e-06, 'epoch': 313.55}\n",
            " 78% 9730/12400 [3:03:21<26:29,  1.68it/s]{'loss': 0.3842, 'learning_rate': 4.555907065806582e-06, 'epoch': 313.87}\n",
            " 78% 9734/12400 [3:03:23<24:42,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:17:25,166 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:17:25,166 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:17:25,166 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.55it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3741552531719208, 'eval_runtime': 3.862, 'eval_samples_per_second': 56.706, 'eval_steps_per_second': 1.036, 'epoch': 314.0}\n",
            " 78% 9734/12400 [3:03:27<24:42,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:17:29,030 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9734\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:17:29,031 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9734/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:17:30,910 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9734/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:17:30,911 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9734/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:17:35,202 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9672] due to args.save_total_limit\n",
            "{'loss': 0.389, 'learning_rate': 4.523285199510171e-06, 'epoch': 314.19}\n",
            "{'loss': 0.37, 'learning_rate': 4.4907645176071695e-06, 'epoch': 314.52}\n",
            "{'loss': 0.383, 'learning_rate': 4.4583452513938086e-06, 'epoch': 314.84}\n",
            " 79% 9765/12400 [3:03:56<24:22,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:17:58,232 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:17:58,232 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:17:58,233 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.51it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.37569910287857056, 'eval_runtime': 3.8982, 'eval_samples_per_second': 56.179, 'eval_steps_per_second': 1.026, 'epoch': 315.0}\n",
            " 79% 9765/12400 [3:04:00<24:22,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:18:02,133 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9765\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:18:02,134 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9765/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:18:04,078 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9765/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:18:04,079 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9765/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:18:08,583 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9703] due to args.save_total_limit\n",
            "{'loss': 0.3784, 'learning_rate': 4.426027631445017e-06, 'epoch': 315.16}\n",
            " 79% 9780/12400 [3:04:20<28:08,  1.55it/s]{'loss': 0.3835, 'learning_rate': 4.393811887612793e-06, 'epoch': 315.48}\n",
            "                                          {'loss': 0.3867, 'learning_rate': 4.361698249024557e-06, 'epoch': 315.81}\n",
            " 79% 9796/12400 [3:04:29<24:06,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:18:31,456 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:18:31,456 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:18:31,457 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3814084827899933, 'eval_runtime': 3.9164, 'eval_samples_per_second': 55.919, 'eval_steps_per_second': 1.021, 'epoch': 316.0}\n",
            " 79% 9796/12400 [3:04:34<24:06,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:18:35,374 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9796\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:18:35,376 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9796/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:18:37,317 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9796/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:18:37,318 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9796/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:18:41,757 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9734] due to args.save_total_limit\n",
            "                                            {'loss': 0.3827, 'learning_rate': 4.32968694408153e-06, 'epoch': 316.13}\n",
            " 79% 9810/12400 [3:04:53<28:17,  1.53it/s]{'loss': 0.3787, 'learning_rate': 4.297778200457109e-06, 'epoch': 316.45}\n",
            "{'loss': 0.3777, 'learning_rate': 4.265972245095241e-06, 'epoch': 316.77}\n",
            " 79% 9827/12400 [3:05:02<23:30,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:19:04,661 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:19:04,662 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:19:04,662 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.37902605533599854, 'eval_runtime': 3.9162, 'eval_samples_per_second': 55.921, 'eval_steps_per_second': 1.021, 'epoch': 317.0}\n",
            " 79% 9827/12400 [3:05:07<23:30,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:19:08,579 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9827\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:19:08,581 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9827/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:19:10,572 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9827/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:19:10,573 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9827/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:19:14,787 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9765] due to args.save_total_limit\n",
            "{'loss': 0.3791, 'learning_rate': 4.2342693042088325e-06, 'epoch': 317.1}\n",
            " 79% 9840/12400 [3:05:25<29:43,  1.43it/s]{'loss': 0.3944, 'learning_rate': 4.2026696032780915e-06, 'epoch': 317.42}\n",
            " 79% 9850/12400 [3:05:31<26:01,  1.63it/s]{'loss': 0.3844, 'learning_rate': 4.17117336704897e-06, 'epoch': 317.74}\n",
            " 80% 9858/12400 [3:05:36<23:23,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:19:37,942 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:19:37,942 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:19:37,942 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 0.38263803720474243, 'eval_runtime': 3.9523, 'eval_samples_per_second': 55.41, 'eval_steps_per_second': 1.012, 'epoch': 318.0}\n",
            " 80% 9858/12400 [3:05:40<23:23,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:19:41,896 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9858\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:19:41,898 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9858/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:19:43,877 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9858/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:19:43,878 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9858/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:19:48,304 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9796] due to args.save_total_limit\n",
            " 80% 9860/12400 [3:05:51<2:32:40,  3.61s/it]{'loss': 0.3817, 'learning_rate': 4.1397808195315705e-06, 'epoch': 318.06}\n",
            "{'loss': 0.3873, 'learning_rate': 4.1084921839984865e-06, 'epoch': 318.39}\n",
            "                                          {'loss': 0.3876, 'learning_rate': 4.077307682983305e-06, 'epoch': 318.71}\n",
            " 80% 9889/12400 [3:06:09<22:46,  1.84it/s][INFO|trainer.py:2550] 2022-05-10 12:20:11,186 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:20:11,186 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:20:11,186 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                          \n",
            " 80% 9889/12400 [3:06:13<22:46,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.3717198371887207, 'eval_runtime': 3.906, 'eval_samples_per_second': 56.068, 'eval_steps_per_second': 1.024, 'epoch': 319.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 12:20:15,094 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9889\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:20:15,095 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9889/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:20:17,023 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9889/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:20:17,024 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9889/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:20:21,466 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9827] due to args.save_total_limit\n",
            "{'loss': 0.368, 'learning_rate': 4.046227538278952e-06, 'epoch': 319.03}\n",
            " 80% 9900/12400 [3:06:31<30:23,  1.37it/s]{'loss': 0.3808, 'learning_rate': 4.015251970936154e-06, 'epoch': 319.35}\n",
            "{'loss': 0.3776, 'learning_rate': 3.984381201261848e-06, 'epoch': 319.68}\n",
            "{'loss': 0.3842, 'learning_rate': 3.953615448817622e-06, 'epoch': 320.0}\n",
            " 80% 9920/12400 [3:06:43<22:46,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:20:44,906 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:20:44,906 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:20:44,906 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.54it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 0.3797171711921692, 'eval_runtime': 3.9188, 'eval_samples_per_second': 55.884, 'eval_steps_per_second': 1.021, 'epoch': 320.0}\n",
            " 80% 9920/12400 [3:06:47<22:46,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:20:48,827 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9920\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:20:48,829 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9920/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:20:50,805 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9920/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:20:50,806 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9920/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:20:55,034 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9858] due to args.save_total_limit\n",
            "{'loss': 0.38, 'learning_rate': 3.922954932418154e-06, 'epoch': 320.32}\n",
            "{'loss': 0.3777, 'learning_rate': 3.892399870129646e-06, 'epoch': 320.65}\n",
            " 80% 9950/12400 [3:07:15<22:41,  1.80it/s]{'loss': 0.3965, 'learning_rate': 3.861950479268305e-06, 'epoch': 320.97}\n",
            " 80% 9951/12400 [3:07:16<23:23,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 12:21:18,212 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:21:18,213 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:21:18,213 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 0.3729232847690582, 'eval_runtime': 3.9337, 'eval_samples_per_second': 55.673, 'eval_steps_per_second': 1.017, 'epoch': 321.0}\n",
            " 80% 9951/12400 [3:07:20<23:23,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:21:22,148 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9951\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:21:22,150 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9951/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:21:24,058 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9951/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:21:24,060 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9951/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:21:28,347 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9889] due to args.save_total_limit\n",
            "                                          {'loss': 0.3769, 'learning_rate': 3.831606976398731e-06, 'epoch': 321.29}\n",
            " 80% 9970/12400 [3:07:43<24:40,  1.64it/s]{'loss': 0.3891, 'learning_rate': 3.8013695773324623e-06, 'epoch': 321.61}\n",
            "                                          {'loss': 0.3891, 'learning_rate': 3.771238497126374e-06, 'epoch': 321.94}\n",
            " 80% 9982/12400 [3:07:49<22:49,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 12:21:51,418 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:21:51,418 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:21:51,418 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                          \n",
            " 80% 9982/12400 [3:07:54<22:49,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A{'eval_loss': 0.38099992275238037, 'eval_runtime': 3.9464, 'eval_samples_per_second': 55.493, 'eval_steps_per_second': 1.014, 'epoch': 322.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:21:55,366 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-9982\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:21:55,368 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-9982/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:21:57,318 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-9982/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:21:57,320 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-9982/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:22:01,583 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9920] due to args.save_total_limit\n",
            "{'loss': 0.3795, 'learning_rate': 3.741213950081162e-06, 'epoch': 322.26}\n",
            " 81% 10000/12400 [3:08:15<25:02,  1.60it/s]{'loss': 0.3831, 'learning_rate': 3.7112961497398577e-06, 'epoch': 322.58}\n",
            "{'loss': 0.3893, 'learning_rate': 3.6814853088862598e-06, 'epoch': 322.9}\n",
            " 81% 10013/12400 [3:08:23<22:19,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 12:22:24,757 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:22:24,757 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:22:24,757 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                           \n",
            " 81% 10013/12400 [3:08:27<22:19,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A{'eval_loss': 0.38279029726982117, 'eval_runtime': 3.9335, 'eval_samples_per_second': 55.676, 'eval_steps_per_second': 1.017, 'epoch': 323.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:22:28,692 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10013\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:22:28,693 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10013/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:22:30,654 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10013/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:22:30,655 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10013/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:22:35,052 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9951] due to args.save_total_limit\n",
            " 81% 10020/12400 [3:08:42<44:00,  1.11s/it]{'loss': 0.3855, 'learning_rate': 3.651781639543442e-06, 'epoch': 323.23}\n",
            " 81% 10030/12400 [3:08:48<26:03,  1.52it/s]{'loss': 0.3806, 'learning_rate': 3.622185352972251e-06, 'epoch': 323.55}\n",
            "                                           {'loss': 0.3831, 'learning_rate': 3.5926966596697902e-06, 'epoch': 323.87}\n",
            " 81% 10044/12400 [3:08:56<21:55,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:22:58,446 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:22:58,446 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:22:58,446 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.38138681650161743, 'eval_runtime': 3.9343, 'eval_samples_per_second': 55.665, 'eval_steps_per_second': 1.017, 'epoch': 324.0}\n",
            " 81% 10044/12400 [3:09:01<21:55,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:23:02,382 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10044\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:23:02,384 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10044/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:23:04,373 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10044/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:23:04,374 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10044/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:23:08,987 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-9982] due to args.save_total_limit\n",
            " 81% 10050/12400 [3:09:15<52:36,  1.34s/it]{'loss': 0.3755, 'learning_rate': 3.5633157693679365e-06, 'epoch': 324.19}\n",
            " 81% 10060/12400 [3:09:21<25:01,  1.56it/s]{'loss': 0.3828, 'learning_rate': 3.534042891031836e-06, 'epoch': 324.52}\n",
            "                                           {'loss': 0.3779, 'learning_rate': 3.5048782328584246e-06, 'epoch': 324.84}\n",
            " 81% 10075/12400 [3:09:30<21:24,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:23:32,011 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:23:32,011 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:23:32,011 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            " 81% 10075/12400 [3:09:34<21:24,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "{'eval_loss': 0.37390387058258057, 'eval_runtime': 3.9205, 'eval_samples_per_second': 55.861, 'eval_steps_per_second': 1.02, 'epoch': 325.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:23:35,933 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10075\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:23:35,934 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10075/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:23:37,902 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10075/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:23:37,903 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10075/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:23:42,302 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10013] due to args.save_total_limit\n",
            "                                             {'loss': 0.383, 'learning_rate': 3.475822002274941e-06, 'epoch': 325.16}\n",
            "                                           {'loss': 0.3933, 'learning_rate': 3.446874405937481e-06, 'epoch': 325.48}\n",
            "                                           {'loss': 0.3757, 'learning_rate': 3.4180356497294594e-06, 'epoch': 325.81}\n",
            " 82% 10106/12400 [3:10:03<21:17,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:24:05,385 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:24:05,385 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:24:05,385 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37604308128356934, 'eval_runtime': 3.9277, 'eval_samples_per_second': 55.758, 'eval_steps_per_second': 1.018, 'epoch': 326.0}\n",
            " 82% 10106/12400 [3:10:08<21:17,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:24:09,315 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10106\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:24:09,316 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10106/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:24:11,216 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10106/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:24:11,216 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10106/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:24:15,548 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10044] due to args.save_total_limit\n",
            " 82% 10110/12400 [3:10:20<1:18:03,  2.05s/it]{'loss': 0.3739, 'learning_rate': 3.3893059387602303e-06, 'epoch': 326.13}\n",
            " 82% 10120/12400 [3:10:27<25:03,  1.52it/s]{'loss': 0.3885, 'learning_rate': 3.3606854773635732e-06, 'epoch': 326.45}\n",
            "{'loss': 0.3825, 'learning_rate': 3.332174469096237e-06, 'epoch': 326.77}\n",
            " 82% 10137/12400 [3:10:36<20:41,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:24:38,459 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:24:38,459 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:24:38,459 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                           \n",
            "                                 {'eval_loss': 0.3750517666339874, 'eval_runtime': 3.883, 'eval_samples_per_second': 56.4, 'eval_steps_per_second': 1.03, 'epoch': 327.0}\n",
            " 82% 10137/12400 [3:10:41<20:41,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:24:42,344 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10137\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:24:42,345 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10137/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:24:44,279 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10137/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:24:44,280 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10137/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:24:48,672 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10075] due to args.save_total_limit\n",
            "{'loss': 0.3826, 'learning_rate': 3.3037731167365357e-06, 'epoch': 327.1}\n",
            "{'loss': 0.3861, 'learning_rate': 3.275481622282867e-06, 'epoch': 327.42}\n",
            " 82% 10160/12400 [3:11:06<23:04,  1.62it/s]{'loss': 0.3758, 'learning_rate': 3.2473001869522864e-06, 'epoch': 327.74}\n",
            " 82% 10168/12400 [3:11:09<20:14,  1.84it/s][INFO|trainer.py:2550] 2022-05-10 12:25:11,625 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:25:11,625 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:25:11,625 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37637850642204285, 'eval_runtime': 3.9537, 'eval_samples_per_second': 55.391, 'eval_steps_per_second': 1.012, 'epoch': 328.0}\n",
            " 82% 10168/12400 [3:11:14<20:14,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:25:15,580 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10168\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:25:15,582 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10168/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:25:17,495 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10168/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:25:17,496 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10168/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:25:21,882 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10106] due to args.save_total_limit\n",
            "{'loss': 0.3781, 'learning_rate': 3.2192290111790796e-06, 'epoch': 328.06}\n",
            " 82% 10180/12400 [3:11:31<25:55,  1.43it/s]{'loss': 0.3928, 'learning_rate': 3.1912682946133474e-06, 'epoch': 328.39}\n",
            " 82% 10190/12400 [3:11:38<22:37,  1.63it/s]{'loss': 0.3772, 'learning_rate': 3.163418236119552e-06, 'epoch': 328.71}\n",
            " 82% 10199/12400 [3:11:43<20:05,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 12:25:44,949 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:25:44,949 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:25:44,949 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.81it/s]\u001b[A\n",
            "                                           \n",
            " 82% 10199/12400 [3:11:47<20:05,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.38303011655807495, 'eval_runtime': 3.8753, 'eval_samples_per_second': 56.512, 'eval_steps_per_second': 1.032, 'epoch': 329.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 12:25:48,826 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10199\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:25:48,827 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10199/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:25:50,722 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10199/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:25:50,723 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10199/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:25:54,974 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10137] due to args.save_total_limit\n",
            " 82% 10200/12400 [3:11:58<2:55:00,  4.77s/it]{'loss': 0.3743, 'learning_rate': 3.1356790337751334e-06, 'epoch': 329.03}\n",
            " 82% 10210/12400 [3:12:04<26:06,  1.40it/s]{'loss': 0.3857, 'learning_rate': 3.108050884869108e-06, 'epoch': 329.35}\n",
            " 82% 10220/12400 [3:12:10<22:39,  1.60it/s]{'loss': 0.3837, 'learning_rate': 3.0805339859006157e-06, 'epoch': 329.68}\n",
            " 82% 10230/12400 [3:12:16<19:38,  1.84it/s]{'loss': 0.3876, 'learning_rate': 3.0531285325775894e-06, 'epoch': 330.0}\n",
            " 82% 10230/12400 [3:12:16<19:38,  1.84it/s][INFO|trainer.py:2550] 2022-05-10 12:26:18,071 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:26:18,071 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:26:18,071 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                           \n",
            " 82% 10230/12400 [3:12:20<19:38,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]{'eval_loss': 0.3851594924926758, 'eval_runtime': 3.9331, 'eval_samples_per_second': 55.681, 'eval_steps_per_second': 1.017, 'epoch': 330.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:26:22,006 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10230\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:26:22,008 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10230/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:26:23,988 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10230/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:26:23,989 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10230/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:26:28,382 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10168] due to args.save_total_limit\n",
            " 83% 10240/12400 [3:12:37<28:29,  1.26it/s]{'loss': 0.381, 'learning_rate': 3.025834719815307e-06, 'epoch': 330.32}\n",
            " 83% 10250/12400 [3:12:43<22:08,  1.62it/s]{'loss': 0.3857, 'learning_rate': 2.998652741735038e-06, 'epoch': 330.65}\n",
            "{'loss': 0.3855, 'learning_rate': 2.9715827916626453e-06, 'epoch': 330.97}\n",
            " 83% 10261/12400 [3:12:49<20:31,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 12:26:51,428 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:26:51,429 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:26:51,429 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            " 83% 10261/12400 [3:12:54<20:31,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.37637078762054443, 'eval_runtime': 3.8751, 'eval_samples_per_second': 56.514, 'eval_steps_per_second': 1.032, 'epoch': 331.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 12:26:55,305 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10261\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:26:55,307 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10261/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:26:57,271 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10261/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:26:57,272 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10261/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:27:01,772 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10199] due to args.save_total_limit\n",
            " 83% 10270/12400 [3:13:10<31:21,  1.13it/s]{'loss': 0.3826, 'learning_rate': 2.944625062127222e-06, 'epoch': 331.29}\n",
            "                                           {'loss': 0.3847, 'learning_rate': 2.9177797448597156e-06, 'epoch': 331.61}\n",
            " 83% 10290/12400 [3:13:22<19:54,  1.77it/s]{'loss': 0.3763, 'learning_rate': 2.8910470307915676e-06, 'epoch': 331.94}\n",
            " 83% 10292/12400 [3:13:23<19:51,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 12:27:24,818 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:27:24,818 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:27:24,818 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "                                           \n",
            " 83% 10292/12400 [3:13:27<19:51,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]{'eval_loss': 0.38150790333747864, 'eval_runtime': 3.8975, 'eval_samples_per_second': 56.19, 'eval_steps_per_second': 1.026, 'epoch': 332.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:27:28,717 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10292\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:27:28,719 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10292/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:27:30,706 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10292/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:27:30,707 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10292/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:27:35,124 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10230] due to args.save_total_limit\n",
            "{'loss': 0.3823, 'learning_rate': 2.8644271100533503e-06, 'epoch': 332.26}\n",
            " 83% 10310/12400 [3:13:48<21:43,  1.60it/s]{'loss': 0.384, 'learning_rate': 2.837920171973418e-06, 'epoch': 332.58}\n",
            "                                           {'loss': 0.3828, 'learning_rate': 2.811526405076573e-06, 'epoch': 332.9}\n",
            " 83% 10323/12400 [3:13:56<19:12,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:27:58,078 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:27:58,078 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:27:58,078 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "                                           \n",
            " 83% 10323/12400 [3:14:00<19:12,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]{'eval_loss': 0.38040682673454285, 'eval_runtime': 3.892, 'eval_samples_per_second': 56.269, 'eval_steps_per_second': 1.028, 'epoch': 333.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:28:01,972 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10323\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:28:01,974 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10323/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:28:03,854 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10323/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:28:03,855 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10323/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:28:08,371 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10261] due to args.save_total_limit\n",
            "{'loss': 0.3975, 'learning_rate': 2.7852459970826928e-06, 'epoch': 333.23}\n",
            " 83% 10340/12400 [3:14:21<22:06,  1.55it/s]{'loss': 0.3789, 'learning_rate': 2.7590791349054197e-06, 'epoch': 333.55}\n",
            "                                           {'loss': 0.3819, 'learning_rate': 2.7330260046508472e-06, 'epoch': 333.87}\n",
            " 84% 10354/12400 [3:14:29<18:55,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:28:31,316 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:28:31,316 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:28:31,316 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            " 84% 10354/12400 [3:14:34<18:55,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]{'eval_loss': 0.3858817517757416, 'eval_runtime': 3.9564, 'eval_samples_per_second': 55.353, 'eval_steps_per_second': 1.011, 'epoch': 334.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:28:35,274 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10354\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:28:35,275 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10354/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:28:37,191 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10354/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:28:37,192 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10354/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:28:41,935 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10292] due to args.save_total_limit\n",
            "                                           {'loss': 0.3685, 'learning_rate': 2.70708679161614e-06, 'epoch': 334.19}\n",
            "{'loss': 0.3942, 'learning_rate': 2.6812616802882826e-06, 'epoch': 334.52}\n",
            " 84% 10380/12400 [3:15:00<20:36,  1.63it/s]{'loss': 0.3792, 'learning_rate': 2.6555508543427183e-06, 'epoch': 334.84}\n",
            " 84% 10385/12400 [3:15:03<18:36,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:29:04,917 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:29:04,917 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:29:04,917 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A{'eval_loss': 0.3707471787929535, 'eval_runtime': 3.9349, 'eval_samples_per_second': 55.656, 'eval_steps_per_second': 1.017, 'epoch': 335.0}\n",
            "                                           \n",
            " 84% 10385/12400 [3:15:07<18:36,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:29:08,853 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10385\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:29:08,855 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10385/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:29:10,927 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10385/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:29:10,927 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10385/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:29:15,355 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10323] due to args.save_total_limit\n",
            "{'loss': 0.383, 'learning_rate': 2.62995449664207e-06, 'epoch': 335.16}\n",
            "{'loss': 0.3773, 'learning_rate': 2.6044727892348226e-06, 'epoch': 335.48}\n",
            "{'loss': 0.3883, 'learning_rate': 2.5791059133540417e-06, 'epoch': 335.81}\n",
            " 84% 10416/12400 [3:15:36<18:27,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:29:38,606 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:29:38,607 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:29:38,607 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3775758445262909, 'eval_runtime': 3.9306, 'eval_samples_per_second': 55.716, 'eval_steps_per_second': 1.018, 'epoch': 336.0}\n",
            " 84% 10416/12400 [3:15:41<18:27,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:29:42,539 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10416\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:29:42,541 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10416/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:29:44,457 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10416/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:29:44,458 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10416/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:29:48,722 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10354] due to args.save_total_limit\n",
            "{'loss': 0.3846, 'learning_rate': 2.5538540494160764e-06, 'epoch': 336.13}\n",
            "{'loss': 0.3751, 'learning_rate': 2.528717377019273e-06, 'epoch': 336.45}\n",
            " 84% 10440/12400 [3:16:06<20:02,  1.63it/s]{'loss': 0.3926, 'learning_rate': 2.503696074942723e-06, 'epoch': 336.77}\n",
            " 84% 10447/12400 [3:16:09<17:53,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:30:11,594 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:30:11,594 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:30:11,594 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.36996740102767944, 'eval_runtime': 3.8772, 'eval_samples_per_second': 56.484, 'eval_steps_per_second': 1.032, 'epoch': 337.0}\n",
            " 84% 10447/12400 [3:16:14<17:53,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:30:15,473 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10447\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:30:15,474 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10447/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:30:17,383 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10447/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:30:17,384 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10447/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:30:21,733 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10385] due to args.save_total_limit\n",
            " 84% 10450/12400 [3:16:26<1:26:46,  2.67s/it]{'loss': 0.3675, 'learning_rate': 2.478790321144941e-06, 'epoch': 337.1}\n",
            " 84% 10460/12400 [3:16:32<22:17,  1.45it/s]{'loss': 0.3789, 'learning_rate': 2.4540002927626595e-06, 'epoch': 337.42}\n",
            "                                           {'loss': 0.392, 'learning_rate': 2.4293261661095277e-06, 'epoch': 337.74}\n",
            " 84% 10478/12400 [3:16:43<17:30,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 12:30:44,829 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:30:44,829 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:30:44,830 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.85it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3688117563724518, 'eval_runtime': 3.9557, 'eval_samples_per_second': 55.363, 'eval_steps_per_second': 1.011, 'epoch': 338.0}\n",
            " 84% 10478/12400 [3:16:47<17:30,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:30:48,787 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10478\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:30:48,788 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10478/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:30:50,737 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10478/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:30:50,738 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10478/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:30:55,299 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10416] due to args.save_total_limit\n",
            "                                             {'loss': 0.3823, 'learning_rate': 2.4047681166748517e-06, 'epoch': 338.06}\n",
            "                                           {'loss': 0.3763, 'learning_rate': 2.3803263191223943e-06, 'epoch': 338.39}\n",
            "{'loss': 0.3829, 'learning_rate': 2.3560009472890857e-06, 'epoch': 338.71}\n",
            " 85% 10509/12400 [3:17:16<17:24,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:31:18,546 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:31:18,546 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:31:18,546 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.50it/s]\u001b[A\n",
            "                                           \n",
            " 85% 10509/12400 [3:17:21<17:24,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.72it/s]{'eval_loss': 0.377847820520401, 'eval_runtime': 4.0168, 'eval_samples_per_second': 54.521, 'eval_steps_per_second': 0.996, 'epoch': 339.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:31:22,565 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10509\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:31:22,566 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10509/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:31:24,558 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10509/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:31:24,559 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10509/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:31:29,050 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10447] due to args.save_total_limit\n",
            "{'loss': 0.3895, 'learning_rate': 2.331792174183802e-06, 'epoch': 339.03}\n",
            "                                           {'loss': 0.3851, 'learning_rate': 2.307700171986145e-06, 'epoch': 339.35}\n",
            "                                           {'loss': 0.3792, 'learning_rate': 2.283725112045199e-06, 'epoch': 339.68}\n",
            " 85% 10540/12400 [3:17:51<17:07,  1.81it/s]{'loss': 0.3834, 'learning_rate': 2.2598671648783317e-06, 'epoch': 340.0}\n",
            "[INFO|trainer.py:2550] 2022-05-10 12:31:52,632 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:31:52,632 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:31:52,632 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.82it/s]\u001b[A\n",
            "                                           \n",
            " 85% 10540/12400 [3:17:55<17:07,  1.81it/s]\n",
            "{'eval_loss': 0.38100847601890564, 'eval_runtime': 4.0131, 'eval_samples_per_second': 54.571, 'eval_steps_per_second': 0.997, 'epoch': 340.0}\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:31:56,647 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10540\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:31:56,648 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10540/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:31:58,580 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10540/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:31:58,581 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10540/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:32:02,962 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10478] due to args.save_total_limit\n",
            "{'loss': 0.3923, 'learning_rate': 2.2361265001699657e-06, 'epoch': 340.32}\n",
            " 85% 10560/12400 [3:18:18<18:46,  1.63it/s]{'loss': 0.3809, 'learning_rate': 2.2125032867703784e-06, 'epoch': 340.65}\n",
            "                                           {'loss': 0.3809, 'learning_rate': 2.188997692694499e-06, 'epoch': 340.97}\n",
            " 85% 10571/12400 [3:18:24<17:26,  1.75it/s][INFO|trainer.py:2550] 2022-05-10 12:32:26,140 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:32:26,140 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:32:26,140 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                           \n",
            " 85% 10571/12400 [3:18:28<17:26,  1.75it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]{'eval_loss': 0.3820000886917114, 'eval_runtime': 3.9377, 'eval_samples_per_second': 55.616, 'eval_steps_per_second': 1.016, 'epoch': 341.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:32:30,080 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10571\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:32:30,081 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10571/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:32:32,008 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10571/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:32:32,009 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10571/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:32:36,572 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10509] due to args.save_total_limit\n",
            " 85% 10580/12400 [3:18:44<28:20,  1.07it/s]{'loss': 0.3886, 'learning_rate': 2.1656098851207317e-06, 'epoch': 341.29}\n",
            " 85% 10590/12400 [3:18:51<19:25,  1.55it/s]{'loss': 0.3836, 'learning_rate': 2.1423400303897156e-06, 'epoch': 341.61}\n",
            " 85% 10600/12400 [3:18:58<17:27,  1.72it/s]{'loss': 0.368, 'learning_rate': 2.1191882940032137e-06, 'epoch': 341.94}\n",
            " 86% 10602/12400 [3:18:58<17:12,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 12:33:00,530 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:33:00,530 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:33:00,530 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                           \n",
            " 86% 10602/12400 [3:19:03<17:12,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.3822160065174103, 'eval_runtime': 3.9607, 'eval_samples_per_second': 55.293, 'eval_steps_per_second': 1.01, 'epoch': 342.0}\n",
            "\u001b[A[INFO|trainer.py:2270] 2022-05-10 12:33:04,493 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10602\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:33:04,495 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10602/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:33:06,441 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10602/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:33:06,442 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10602/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:33:11,027 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10540] due to args.save_total_limit\n",
            " 86% 10610/12400 [3:19:18<28:49,  1.04it/s]{'loss': 0.3948, 'learning_rate': 2.096154840622876e-06, 'epoch': 342.26}\n",
            "{'loss': 0.3906, 'learning_rate': 2.0732398340690855e-06, 'epoch': 342.58}\n",
            "                                           {'loss': 0.3743, 'learning_rate': 2.050443437319821e-06, 'epoch': 342.9}\n",
            " 86% 10633/12400 [3:19:32<16:29,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:33:34,217 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:33:34,217 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:33:34,217 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3791685700416565, 'eval_runtime': 3.9125, 'eval_samples_per_second': 55.974, 'eval_steps_per_second': 1.022, 'epoch': 343.0}\n",
            " 86% 10633/12400 [3:19:36<16:29,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:33:38,131 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10633\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:33:38,133 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10633/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:33:40,060 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10633/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:33:40,061 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10633/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:33:44,727 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10571] due to args.save_total_limit\n",
            "{'loss': 0.3909, 'learning_rate': 2.027765812509457e-06, 'epoch': 343.23}\n",
            " 86% 10650/12400 [3:19:58<19:16,  1.51it/s]{'loss': 0.3738, 'learning_rate': 2.0052071209276314e-06, 'epoch': 343.55}\n",
            "                                           {'loss': 0.3852, 'learning_rate': 1.9827675230181027e-06, 'epoch': 343.87}\n",
            " 86% 10664/12400 [3:20:06<16:08,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:34:07,799 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:34:07,799 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:34:07,799 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            " 86% 10664/12400 [3:20:10<16:08,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]{'eval_loss': 0.37890782952308655, 'eval_runtime': 4.0481, 'eval_samples_per_second': 54.099, 'eval_steps_per_second': 0.988, 'epoch': 344.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:34:11,849 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10664\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:34:11,850 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10664/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:34:13,775 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10664/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:34:13,776 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10664/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:34:18,430 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10602] due to args.save_total_limit\n",
            " 86% 10670/12400 [3:20:24<38:42,  1.34s/it]{'loss': 0.3854, 'learning_rate': 1.9604471783775943e-06, 'epoch': 344.19}\n",
            " 86% 10680/12400 [3:20:30<18:14,  1.57it/s]{'loss': 0.3795, 'learning_rate': 1.938246245754668e-06, 'epoch': 344.52}\n",
            " 86% 10690/12400 [3:20:37<17:38,  1.62it/s]{'loss': 0.3896, 'learning_rate': 1.9161648830485953e-06, 'epoch': 344.84}\n",
            " 86% 10695/12400 [3:20:39<15:49,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:34:41,501 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:34:41,501 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:34:41,501 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37930426001548767, 'eval_runtime': 3.9189, 'eval_samples_per_second': 55.883, 'eval_steps_per_second': 1.021, 'epoch': 345.0}\n",
            " 86% 10695/12400 [3:20:44<15:49,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:34:45,422 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10695\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:34:45,423 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10695/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:34:47,368 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10695/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:34:47,368 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10695/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:34:51,993 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10633] due to args.save_total_limit\n",
            " 86% 10700/12400 [3:20:57<47:07,  1.66s/it]{'loss': 0.3736, 'learning_rate': 1.8942032473082347e-06, 'epoch': 345.16}\n",
            " 86% 10710/12400 [3:21:04<18:08,  1.55it/s]{'loss': 0.383, 'learning_rate': 1.8723614947309083e-06, 'epoch': 345.48}\n",
            "                                           {'loss': 0.3869, 'learning_rate': 1.8506397806613053e-06, 'epoch': 345.81}\n",
            " 86% 10726/12400 [3:21:13<15:39,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 12:35:15,378 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:35:15,378 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:35:15,378 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3738962709903717, 'eval_runtime': 3.9383, 'eval_samples_per_second': 55.608, 'eval_steps_per_second': 1.016, 'epoch': 346.0}\n",
            " 86% 10726/12400 [3:21:18<15:39,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:35:19,317 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10726\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:35:19,319 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10726/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:35:21,235 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10726/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:35:21,236 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10726/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:35:25,832 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10664] due to args.save_total_limit\n",
            " 87% 10730/12400 [3:21:31<57:49,  2.08s/it]{'loss': 0.385, 'learning_rate': 1.8290382595903457e-06, 'epoch': 346.13}\n",
            " 87% 10740/12400 [3:21:37<18:17,  1.51it/s]{'loss': 0.3838, 'learning_rate': 1.8075570851541313e-06, 'epoch': 346.45}\n",
            "                                           {'loss': 0.3864, 'learning_rate': 1.786196410132807e-06, 'epoch': 346.77}\n",
            " 87% 10757/12400 [3:21:47<15:03,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:35:48,762 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:35:48,763 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:35:48,763 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3685973584651947, 'eval_runtime': 3.9427, 'eval_samples_per_second': 55.545, 'eval_steps_per_second': 1.015, 'epoch': 347.0}\n",
            " 87% 10757/12400 [3:21:51<15:03,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:35:52,707 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10757\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:35:52,709 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10757/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:35:54,658 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10757/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:35:54,659 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10757/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:35:59,057 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10695] due to args.save_total_limit\n",
            " 87% 10760/12400 [3:22:03<1:13:14,  2.68s/it]{'loss': 0.3785, 'learning_rate': 1.7649563864494924e-06, 'epoch': 347.1}\n",
            " 87% 10770/12400 [3:22:10<18:49,  1.44it/s]{'loss': 0.3805, 'learning_rate': 1.743837165169209e-06, 'epoch': 347.42}\n",
            "                                           {'loss': 0.3857, 'learning_rate': 1.722838896497789e-06, 'epoch': 347.74}\n",
            " 87% 10788/12400 [3:22:20<14:46,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:36:22,137 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:36:22,137 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:36:22,137 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.56it/s]\u001b[A\n",
            "                                           \n",
            " 87% 10788/12400 [3:22:24<14:46,  1.82it/s]{'eval_loss': 0.37511274218559265, 'eval_runtime': 3.9522, 'eval_samples_per_second': 55.413, 'eval_steps_per_second': 1.012, 'epoch': 348.0}\n",
            "\n",
            "100% 4/4 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:36:26,091 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10788\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:36:26,093 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10788/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:36:28,021 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10788/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:36:28,022 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10788/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:36:32,508 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10726] due to args.save_total_limit\n",
            " 87% 10790/12400 [3:22:36<1:36:41,  3.60s/it]{'loss': 0.3755, 'learning_rate': 1.7019617297808235e-06, 'epoch': 348.06}\n",
            " 87% 10800/12400 [3:22:42<18:41,  1.43it/s]{'loss': 0.3904, 'learning_rate': 1.6812058135025827e-06, 'epoch': 348.39}\n",
            "{'loss': 0.3886, 'learning_rate': 1.6605712952849876e-06, 'epoch': 348.71}\n",
            " 87% 10819/12400 [3:22:53<14:23,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 12:36:55,454 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:36:55,455 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:36:55,455 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            " 87% 10819/12400 [3:22:58<14:23,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]{'eval_loss': 0.3817712664604187, 'eval_runtime': 3.9462, 'eval_samples_per_second': 55.497, 'eval_steps_per_second': 1.014, 'epoch': 349.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:36:59,402 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10819\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:36:59,404 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10819/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:37:01,393 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10819/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:37:01,394 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10819/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:37:06,032 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10757] due to args.save_total_limit\n",
            "{'loss': 0.3754, 'learning_rate': 1.6400583218865185e-06, 'epoch': 349.03}\n",
            "{'loss': 0.3726, 'learning_rate': 1.6196670392012056e-06, 'epoch': 349.35}\n",
            "{'loss': 0.3884, 'learning_rate': 1.5993975922575946e-06, 'epoch': 349.68}\n",
            " 88% 10850/12400 [3:23:27<14:13,  1.82it/s]{'loss': 0.3876, 'learning_rate': 1.5792501252176717e-06, 'epoch': 350.0}\n",
            " 88% 10850/12400 [3:23:28<14:13,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:37:29,346 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:37:29,346 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:37:29,346 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                           \n",
            " 88% 10850/12400 [3:23:32<14:13,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:37:33,289 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10850\n",
            "{'eval_loss': 0.38127952814102173, 'eval_runtime': 3.9408, 'eval_samples_per_second': 55.573, 'eval_steps_per_second': 1.015, 'epoch': 350.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:37:33,290 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10850/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:37:35,243 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10850/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:37:35,244 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10850/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:37:39,770 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10788] due to args.save_total_limit\n",
            "{'loss': 0.3802, 'learning_rate': 1.5592247813758965e-06, 'epoch': 350.32}\n",
            "{'loss': 0.386, 'learning_rate': 1.5393217031581363e-06, 'epoch': 350.65}\n",
            " 88% 10880/12400 [3:24:01<14:14,  1.78it/s]{'loss': 0.3866, 'learning_rate': 1.5195410321206761e-06, 'epoch': 350.97}\n",
            " 88% 10881/12400 [3:24:01<14:36,  1.73it/s][INFO|trainer.py:2550] 2022-05-10 12:38:03,010 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:38:03,010 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:38:03,010 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.3769509792327881, 'eval_runtime': 3.9572, 'eval_samples_per_second': 55.343, 'eval_steps_per_second': 1.011, 'epoch': 351.0}\n",
            " 88% 10881/12400 [3:24:05<14:36,  1.73it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:38:06,969 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10881\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:38:06,971 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10881/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:38:08,911 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10881/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:38:08,912 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10881/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:38:13,252 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10819] due to args.save_total_limit\n",
            "{'loss': 0.3777, 'learning_rate': 1.4998829089492085e-06, 'epoch': 351.29}\n",
            " 88% 10900/12400 [3:24:28<15:24,  1.62it/s]{'loss': 0.384, 'learning_rate': 1.4803474734578268e-06, 'epoch': 351.61}\n",
            "{'loss': 0.3758, 'learning_rate': 1.4609348645880398e-06, 'epoch': 351.94}\n",
            " 88% 10912/12400 [3:24:34<14:04,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 12:38:36,469 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:38:36,470 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:38:36,470 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            " 88% 10912/12400 [3:24:39<14:04,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "{'eval_loss': 0.37667545676231384, 'eval_runtime': 3.9892, 'eval_samples_per_second': 54.898, 'eval_steps_per_second': 1.003, 'epoch': 352.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:38:40,461 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10912\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:38:40,462 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10912/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:38:42,389 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10912/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:38:42,390 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10912/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:38:46,983 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10850] due to args.save_total_limit\n",
            "{'loss': 0.3857, 'learning_rate': 1.4416452204077771e-06, 'epoch': 352.26}\n",
            "                                           {'loss': 0.383, 'learning_rate': 1.4224786781104079e-06, 'epoch': 352.58}\n",
            " 88% 10940/12400 [3:25:07<14:10,  1.72it/s]{'loss': 0.3856, 'learning_rate': 1.4034353740137624e-06, 'epoch': 352.9}\n",
            " 88% 10943/12400 [3:25:08<13:37,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 12:39:10,171 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:39:10,171 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:39:10,171 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                           \n",
            " 88% 10943/12400 [3:25:12<13:37,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]{'eval_loss': 0.37413841485977173, 'eval_runtime': 3.8719, 'eval_samples_per_second': 56.561, 'eval_steps_per_second': 1.033, 'epoch': 353.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:39:14,045 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10943\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:39:14,046 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10943/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:39:16,036 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10943/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:39:16,037 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10943/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:39:20,703 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10881] due to args.save_total_limit\n",
            "                                           {'loss': 0.3889, 'learning_rate': 1.3845154435591818e-06, 'epoch': 353.23}\n",
            " 88% 10960/12400 [3:25:34<15:45,  1.52it/s]{'loss': 0.3754, 'learning_rate': 1.3657190213105178e-06, 'epoch': 353.55}\n",
            "                                           {'loss': 0.384, 'learning_rate': 1.3470462409532083e-06, 'epoch': 353.87}\n",
            " 88% 10974/12400 [3:25:42<13:18,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:39:43,846 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:39:43,846 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:39:43,846 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37442517280578613, 'eval_runtime': 3.9327, 'eval_samples_per_second': 55.686, 'eval_steps_per_second': 1.017, 'epoch': 354.0}\n",
            " 88% 10974/12400 [3:25:46<13:18,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:39:47,780 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-10974\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:39:47,782 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-10974/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:39:49,714 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-10974/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:39:49,715 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-10974/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:39:54,227 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10912] due to args.save_total_limit\n",
            "{'loss': 0.3813, 'learning_rate': 1.3284972352933215e-06, 'epoch': 354.19}\n",
            " 89% 10990/12400 [3:26:07<14:55,  1.58it/s]{'loss': 0.3843, 'learning_rate': 1.3100721362565882e-06, 'epoch': 354.52}\n",
            " 89% 11000/12400 [3:26:12<14:08,  1.65it/s]{'loss': 0.3816, 'learning_rate': 1.291771074887498e-06, 'epoch': 354.84}\n",
            " 89% 11005/12400 [3:26:15<12:54,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:40:17,175 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:40:17,175 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:40:17,175 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                           \n",
            " 89% 11005/12400 [3:26:19<12:54,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]\u001b[A{'eval_loss': 0.3729604184627533, 'eval_runtime': 3.9507, 'eval_samples_per_second': 55.433, 'eval_steps_per_second': 1.012, 'epoch': 355.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:40:21,127 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11005\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:40:21,129 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11005/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:40:23,094 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11005/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:40:23,095 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11005/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:40:27,341 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10943] due to args.save_total_limit\n",
            "                                           {'loss': 0.3847, 'learning_rate': 1.2735941813483384e-06, 'epoch': 355.16}\n",
            "                                           {'loss': 0.3728, 'learning_rate': 1.2555415849182862e-06, 'epoch': 355.48}\n",
            "{'loss': 0.3888, 'learning_rate': 1.2376134139924777e-06, 'epoch': 355.81}\n",
            " 89% 11036/12400 [3:26:48<12:47,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 12:40:50,631 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:40:50,631 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:40:50,631 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            " 89% 11036/12400 [3:26:53<12:47,  1.78it/s]\n",
            "{'eval_loss': 0.3682635724544525, 'eval_runtime': 4.0411, 'eval_samples_per_second': 54.193, 'eval_steps_per_second': 0.99, 'epoch': 356.0}\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:40:54,674 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11036\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:40:54,676 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11036/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:40:56,640 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11036/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:40:56,641 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11036/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:41:01,243 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-10974] due to args.save_total_limit\n",
            "{'loss': 0.3835, 'learning_rate': 1.2198097960811025e-06, 'epoch': 356.13}\n",
            "{'loss': 0.3834, 'learning_rate': 1.2021308578084969e-06, 'epoch': 356.45}\n",
            "{'loss': 0.3771, 'learning_rate': 1.1845767249122313e-06, 'epoch': 356.77}\n",
            " 89% 11067/12400 [3:27:22<12:15,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:41:24,449 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:41:24,449 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:41:24,449 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                           \n",
            " 89% 11067/12400 [3:27:27<12:15,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "{'eval_loss': 0.37299978733062744, 'eval_runtime': 3.9059, 'eval_samples_per_second': 56.069, 'eval_steps_per_second': 1.024, 'epoch': 357.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:41:28,357 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11067\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:41:28,358 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11067/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:41:30,419 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11067/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:41:30,420 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11067/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:41:34,891 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11005] due to args.save_total_limit\n",
            "{'loss': 0.3871, 'learning_rate': 1.167147522242244e-06, 'epoch': 357.1}\n",
            "{'loss': 0.3755, 'learning_rate': 1.149843373759906e-06, 'epoch': 357.42}\n",
            "{'loss': 0.3907, 'learning_rate': 1.1326644025371964e-06, 'epoch': 357.74}\n",
            " 90% 11098/12400 [3:27:56<11:51,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 12:41:58,007 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:41:58,007 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:41:58,007 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.54it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3734748065471649, 'eval_runtime': 3.9207, 'eval_samples_per_second': 55.857, 'eval_steps_per_second': 1.02, 'epoch': 358.0}\n",
            " 90% 11098/12400 [3:28:00<11:51,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:42:01,930 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11098\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:42:01,932 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11098/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:42:03,940 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11098/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:42:03,941 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11098/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:42:08,648 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11036] due to args.save_total_limit\n",
            " 90% 11100/12400 [3:28:12<1:19:12,  3.66s/it]{'loss': 0.3853, 'learning_rate': 1.1156107307557824e-06, 'epoch': 358.06}\n",
            "{'loss': 0.3795, 'learning_rate': 1.0986824797061644e-06, 'epoch': 358.39}\n",
            " 90% 11120/12400 [3:28:24<13:11,  1.62it/s]{'loss': 0.3857, 'learning_rate': 1.0818797697868275e-06, 'epoch': 358.71}\n",
            " 90% 11129/12400 [3:28:30<11:43,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:42:31,850 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:42:31,850 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:42:31,850 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37905701994895935, 'eval_runtime': 3.9592, 'eval_samples_per_second': 55.314, 'eval_steps_per_second': 1.01, 'epoch': 359.0}\n",
            " 90% 11129/12400 [3:28:34<11:43,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:42:35,811 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11129\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:42:35,813 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11129/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:42:37,775 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11129/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:42:37,776 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11129/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:42:42,237 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11067] due to args.save_total_limit\n",
            "                                             {'loss': 0.3798, 'learning_rate': 1.0652027205033643e-06, 'epoch': 359.03}\n",
            "{'loss': 0.378, 'learning_rate': 1.0486514504676372e-06, 'epoch': 359.35}\n",
            "                                           {'loss': 0.3786, 'learning_rate': 1.0322260773969257e-06, 'epoch': 359.68}\n",
            "                                           {'loss': 0.3882, 'learning_rate': 1.0159267181131005e-06, 'epoch': 360.0}\n",
            " 90% 11160/12400 [3:29:04<11:24,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:43:05,683 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:43:05,683 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:43:05,683 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.60it/s]\u001b[A\n",
            "                                           \n",
            " 90% 11160/12400 [3:29:08<11:24,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]{'eval_loss': 0.379559725522995, 'eval_runtime': 4.0362, 'eval_samples_per_second': 54.259, 'eval_steps_per_second': 0.991, 'epoch': 360.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:43:09,721 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11160\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:43:09,722 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11160/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:43:11,703 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11160/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:43:11,704 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11160/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:43:16,207 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11098] due to args.save_total_limit\n",
            "{'loss': 0.3792, 'learning_rate': 9.997534885417851e-07, 'epoch': 360.32}\n",
            " 90% 11180/12400 [3:29:31<12:35,  1.61it/s]{'loss': 0.3852, 'learning_rate': 9.83706503711535e-07, 'epoch': 360.65}\n",
            "                                           {'loss': 0.3754, 'learning_rate': 9.677858777530114e-07, 'epoch': 360.97}\n",
            " 90% 11191/12400 [3:29:37<11:37,  1.73it/s][INFO|trainer.py:2550] 2022-05-10 12:43:39,498 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:43:39,498 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:43:39,498 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3810870051383972, 'eval_runtime': 3.9674, 'eval_samples_per_second': 55.2, 'eval_steps_per_second': 1.008, 'epoch': 361.0}\n",
            " 90% 11191/12400 [3:29:42<11:37,  1.73it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:43:43,467 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11191\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:43:43,469 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11191/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:43:45,434 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11191/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:43:45,436 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11191/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:43:50,093 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11129] due to args.save_total_limit\n",
            " 90% 11200/12400 [3:29:58<17:47,  1.12it/s]{'loss': 0.3824, 'learning_rate': 9.519917238981794e-07, 'epoch': 361.29}\n",
            "                                           {'loss': 0.3815, 'learning_rate': 9.363241544795099e-07, 'epoch': 361.61}\n",
            "                                           {'loss': 0.3863, 'learning_rate': 9.207832809291431e-07, 'epoch': 361.94}\n",
            " 90% 11222/12400 [3:30:11<11:19,  1.73it/s][INFO|trainer.py:2550] 2022-05-10 12:44:13,579 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:44:13,579 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:44:13,579 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.37740999460220337, 'eval_runtime': 3.9415, 'eval_samples_per_second': 55.563, 'eval_steps_per_second': 1.015, 'epoch': 362.0}\n",
            " 90% 11222/12400 [3:30:16<11:19,  1.73it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:44:17,522 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11222\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:44:17,524 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11222/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:44:19,496 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11222/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:44:19,497 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11222/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:44:24,272 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11160] due to args.save_total_limit\n",
            "                                           {'loss': 0.3783, 'learning_rate': 9.053692137781509e-07, 'epoch': 362.26}\n",
            "{'loss': 0.381, 'learning_rate': 8.900820626557101e-07, 'epoch': 362.58}\n",
            "                                           {'loss': 0.3902, 'learning_rate': 8.749219362883253e-07, 'epoch': 362.9}\n",
            " 91% 11253/12400 [3:30:46<10:47,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 12:44:47,754 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:44:47,754 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:44:47,754 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            " 91% 11253/12400 [3:30:50<10:47,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]{'eval_loss': 0.3770187795162201, 'eval_runtime': 4.0426, 'eval_samples_per_second': 54.173, 'eval_steps_per_second': 0.989, 'epoch': 363.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:44:51,798 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11253\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:44:51,800 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11253/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:44:53,796 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11253/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:44:53,797 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11253/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:44:58,263 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11191] due to args.save_total_limit\n",
            " 91% 11260/12400 [3:31:05<21:09,  1.11s/it]{'loss': 0.3801, 'learning_rate': 8.598889424990846e-07, 'epoch': 363.23}\n",
            "{'loss': 0.3834, 'learning_rate': 8.449831882068596e-07, 'epoch': 363.55}\n",
            "                                           {'loss': 0.3834, 'learning_rate': 8.302047794255667e-07, 'epoch': 363.87}\n",
            " 91% 11284/12400 [3:31:19<10:23,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:45:21,546 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:45:21,546 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:45:21,546 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                           \n",
            " 91% 11284/12400 [3:31:24<10:23,  1.79it/s]\n",
            "{'eval_loss': 0.3666788637638092, 'eval_runtime': 3.9508, 'eval_samples_per_second': 55.432, 'eval_steps_per_second': 1.012, 'epoch': 364.0}\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:45:25,499 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11284\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:45:25,501 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11284/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:45:27,440 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11284/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:45:27,442 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11284/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:45:32,011 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11222] due to args.save_total_limit\n",
            "{'loss': 0.3731, 'learning_rate': 8.155538212633969e-07, 'epoch': 364.19}\n",
            " 91% 11300/12400 [3:31:45<11:40,  1.57it/s]{'loss': 0.3854, 'learning_rate': 8.01030417922089e-07, 'epoch': 364.52}\n",
            " 91% 11310/12400 [3:31:51<11:21,  1.60it/s]{'loss': 0.3912, 'learning_rate': 7.866346726961639e-07, 'epoch': 364.84}\n",
            " 91% 11315/12400 [3:31:53<10:06,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:45:55,311 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:45:55,311 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:45:55,311 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                           \n",
            " 91% 11315/12400 [3:31:58<10:06,  1.79it/s]\n",
            "{'eval_loss': 0.381949782371521, 'eval_runtime': 3.9603, 'eval_samples_per_second': 55.299, 'eval_steps_per_second': 1.01, 'epoch': 365.0}\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:45:59,273 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11315\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:45:59,275 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11315/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:46:01,249 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11315/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:46:01,250 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11315/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:46:05,735 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11253] due to args.save_total_limit\n",
            " 91% 11320/12400 [3:32:11<29:59,  1.67s/it]{'loss': 0.3803, 'learning_rate': 7.723666879722042e-07, 'epoch': 365.16}\n",
            " 91% 11330/12400 [3:32:17<11:19,  1.58it/s]{'loss': 0.3844, 'learning_rate': 7.582265652281337e-07, 'epoch': 365.48}\n",
            "                                           {'loss': 0.3771, 'learning_rate': 7.442144050324706e-07, 'epoch': 365.81}\n",
            " 92% 11346/12400 [3:32:27<09:49,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:46:29,044 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:46:29,045 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:46:29,045 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.54it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37687334418296814, 'eval_runtime': 3.9758, 'eval_samples_per_second': 55.083, 'eval_steps_per_second': 1.006, 'epoch': 366.0}\n",
            " 92% 11346/12400 [3:32:31<09:49,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:46:33,022 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11346\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:46:33,024 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11346/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:46:34,997 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11346/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:46:34,998 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11346/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:46:39,300 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11284] due to args.save_total_limit\n",
            " 92% 11350/12400 [3:32:44<36:17,  2.07s/it]{'loss': 0.3791, 'learning_rate': 7.303303070436424e-07, 'epoch': 366.13}\n",
            " 92% 11360/12400 [3:32:51<11:35,  1.50it/s]{'loss': 0.3814, 'learning_rate': 7.165743700092527e-07, 'epoch': 366.45}\n",
            "                                           {'loss': 0.3844, 'learning_rate': 7.02946691765393e-07, 'epoch': 366.77}\n",
            " 92% 11377/12400 [3:33:00<09:24,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:47:02,525 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:47:02,525 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:47:02,525 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            " 92% 11377/12400 [3:33:05<09:24,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:47:06,513 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11377\n",
            "{'eval_loss': 0.37872347235679626, 'eval_runtime': 3.9861, 'eval_samples_per_second': 54.941, 'eval_steps_per_second': 1.003, 'epoch': 367.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:47:06,514 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11377/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:47:08,491 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11377/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:47:08,492 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11377/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:47:13,165 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11315] due to args.save_total_limit\n",
            " 92% 11380/12400 [3:33:18<46:45,  2.75s/it]{'loss': 0.3938, 'learning_rate': 6.894473692359383e-07, 'epoch': 367.1}\n",
            "                                           {'loss': 0.3805, 'learning_rate': 6.760764984318629e-07, 'epoch': 367.42}\n",
            "{'loss': 0.3901, 'learning_rate': 6.628341744505612e-07, 'epoch': 367.74}\n",
            " 92% 11408/12400 [3:33:34<09:09,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:47:36,535 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:47:36,535 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:47:36,535 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37969493865966797, 'eval_runtime': 4.0201, 'eval_samples_per_second': 54.476, 'eval_steps_per_second': 0.995, 'epoch': 368.0}\n",
            " 92% 11408/12400 [3:33:39<09:09,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:47:40,557 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11408\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:47:40,559 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11408/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:47:42,516 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11408/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:47:42,517 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11408/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:47:47,088 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11346] due to args.save_total_limit\n",
            "                                             {'loss': 0.3856, 'learning_rate': 6.497204914751592e-07, 'epoch': 368.06}\n",
            "                                           {'loss': 0.3729, 'learning_rate': 6.367355427738543e-07, 'epoch': 368.39}\n",
            "{'loss': 0.3907, 'learning_rate': 6.238794206992533e-07, 'epoch': 368.71}\n",
            " 92% 11439/12400 [3:34:08<08:54,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:48:10,584 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:48:10,584 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:48:10,584 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.38it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3717797100543976, 'eval_runtime': 3.9419, 'eval_samples_per_second': 55.557, 'eval_steps_per_second': 1.015, 'epoch': 369.0}\n",
            " 92% 11439/12400 [3:34:13<08:54,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:48:14,528 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11439\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:48:14,530 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11439/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:48:16,479 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11439/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:48:16,480 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11439/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:48:20,900 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11377] due to args.save_total_limit\n",
            "{'loss': 0.3735, 'learning_rate': 6.111522166877082e-07, 'epoch': 369.03}\n",
            " 92% 11450/12400 [3:34:30<11:29,  1.38it/s]{'loss': 0.3798, 'learning_rate': 5.985540212586716e-07, 'epoch': 369.35}\n",
            "                                           {'loss': 0.3905, 'learning_rate': 5.860849240140423e-07, 'epoch': 369.68}\n",
            "                                           {'loss': 0.3848, 'learning_rate': 5.737450136375555e-07, 'epoch': 370.0}\n",
            " 92% 11470/12400 [3:34:42<08:32,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:48:44,077 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:48:44,077 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:48:44,077 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.38544389605522156, 'eval_runtime': 3.9568, 'eval_samples_per_second': 55.348, 'eval_steps_per_second': 1.011, 'epoch': 370.0}\n",
            " 92% 11470/12400 [3:34:46<08:32,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:48:48,036 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11470\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:48:48,038 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11470/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:48:50,031 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11470/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:48:50,032 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11470/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:48:54,332 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11408] due to args.save_total_limit\n",
            " 93% 11480/12400 [3:35:02<12:04,  1.27it/s]{'loss': 0.3857, 'learning_rate': 5.615343778941129e-07, 'epoch': 370.32}\n",
            "                                           {'loss': 0.3875, 'learning_rate': 5.494531036291889e-07, 'epoch': 370.65}\n",
            "                                           {'loss': 0.3778, 'learning_rate': 5.375012767682048e-07, 'epoch': 370.97}\n",
            " 93% 11501/12400 [3:35:15<08:35,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 12:49:17,559 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:49:17,559 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:49:17,559 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.39it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3847624957561493, 'eval_runtime': 4.0012, 'eval_samples_per_second': 54.734, 'eval_steps_per_second': 1.0, 'epoch': 371.0}\n",
            " 93% 11501/12400 [3:35:20<08:35,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.65it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:49:21,562 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11501\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:49:21,564 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11501/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:49:23,580 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11501/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:49:23,581 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11501/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:49:28,263 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11439] due to args.save_total_limit\n",
            " 93% 11510/12400 [3:35:36<13:12,  1.12it/s]{'loss': 0.384, 'learning_rate': 5.256789823159032e-07, 'epoch': 371.29}\n",
            "                                           {'loss': 0.3869, 'learning_rate': 5.139863043557721e-07, 'epoch': 371.61}\n",
            "                                           {'loss': 0.3815, 'learning_rate': 5.024233260494205e-07, 'epoch': 371.94}\n",
            " 93% 11532/12400 [3:35:49<08:11,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 12:49:51,477 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:49:51,477 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:49:51,477 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            " 93% 11532/12400 [3:35:54<08:11,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]{'eval_loss': 0.372734397649765, 'eval_runtime': 4.018, 'eval_samples_per_second': 54.504, 'eval_steps_per_second': 0.996, 'epoch': 372.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:49:55,497 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11532\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:49:55,498 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11532/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:49:57,452 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11532/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:49:57,453 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11532/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:50:01,926 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11470] due to args.save_total_limit\n",
            " 93% 11540/12400 [3:36:09<13:51,  1.03it/s]{'loss': 0.377, 'learning_rate': 4.909901296359993e-07, 'epoch': 372.26}\n",
            "{'loss': 0.3749, 'learning_rate': 4.796867964316123e-07, 'epoch': 372.58}\n",
            "                                           {'loss': 0.3829, 'learning_rate': 4.68513406828748e-07, 'epoch': 372.9}\n",
            " 93% 11563/12400 [3:36:23<07:51,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 12:50:25,344 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:50:25,345 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:50:25,345 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3782980144023895, 'eval_runtime': 3.9497, 'eval_samples_per_second': 55.447, 'eval_steps_per_second': 1.013, 'epoch': 373.0}\n",
            " 93% 11563/12400 [3:36:28<07:51,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:50:29,296 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11563\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:50:29,298 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11563/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:50:31,252 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11563/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:50:31,253 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11563/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:50:35,605 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11501] due to args.save_total_limit\n",
            " 93% 11570/12400 [3:36:42<15:23,  1.11s/it]{'loss': 0.3801, 'learning_rate': 4.574700402956842e-07, 'epoch': 373.23}\n",
            " 93% 11580/12400 [3:36:48<08:59,  1.52it/s]{'loss': 0.3842, 'learning_rate': 4.4655677537595097e-07, 'epoch': 373.55}\n",
            "                                           {'loss': 0.3773, 'learning_rate': 4.3577368968775796e-07, 'epoch': 373.87}\n",
            " 94% 11594/12400 [3:36:57<07:31,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:50:58,970 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:50:58,970 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:50:58,970 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                           \n",
            "                                 {'eval_loss': 0.3861607313156128, 'eval_runtime': 4.0125, 'eval_samples_per_second': 54.58, 'eval_steps_per_second': 0.997, 'epoch': 374.0}\n",
            " 94% 11594/12400 [3:37:01<07:31,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:51:02,984 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11594\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:51:02,986 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11594/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:51:04,976 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11594/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:51:04,977 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11594/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:51:09,573 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11532] due to args.save_total_limit\n",
            "{'loss': 0.3849, 'learning_rate': 4.251208599234328e-07, 'epoch': 374.19}\n",
            " 94% 11610/12400 [3:37:22<08:18,  1.58it/s]{'loss': 0.3798, 'learning_rate': 4.145983618489043e-07, 'epoch': 374.52}\n",
            "                                           {'loss': 0.3907, 'learning_rate': 4.042062703031285e-07, 'epoch': 374.84}\n",
            " 94% 11625/12400 [3:37:31<07:10,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:51:32,743 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:51:32,743 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:51:32,743 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.38275182247161865, 'eval_runtime': 4.079, 'eval_samples_per_second': 53.69, 'eval_steps_per_second': 0.981, 'epoch': 375.0}\n",
            " 94% 11625/12400 [3:37:35<07:10,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:51:36,824 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11625\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:51:36,825 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11625/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:51:38,755 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11625/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:51:38,756 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11625/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:51:43,176 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11563] due to args.save_total_limit\n",
            " 94% 11630/12400 [3:37:48<21:28,  1.67s/it]{'loss': 0.3715, 'learning_rate': 3.9394465919758214e-07, 'epoch': 375.16}\n",
            "{'loss': 0.3864, 'learning_rate': 3.838136015157284e-07, 'epoch': 375.48}\n",
            "                                           {'loss': 0.3777, 'learning_rate': 3.7381316931249586e-07, 'epoch': 375.81}\n",
            " 94% 11656/12400 [3:38:04<06:55,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:52:06,496 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:52:06,496 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:52:06,496 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            " 94% 11656/12400 [3:38:09<06:55,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A{'eval_loss': 0.3764781653881073, 'eval_runtime': 4.0209, 'eval_samples_per_second': 54.465, 'eval_steps_per_second': 0.995, 'epoch': 376.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:52:10,519 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11656\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:52:10,520 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11656/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:52:12,454 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11656/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:52:12,455 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11656/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:52:16,964 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11594] due to args.save_total_limit\n",
            "{'loss': 0.3846, 'learning_rate': 3.6394343371376685e-07, 'epoch': 376.13}\n",
            " 94% 11670/12400 [3:38:28<08:03,  1.51it/s]{'loss': 0.3779, 'learning_rate': 3.5420446491587764e-07, 'epoch': 376.45}\n",
            "{'loss': 0.3918, 'learning_rate': 3.4459633218510834e-07, 'epoch': 376.77}\n",
            " 94% 11687/12400 [3:38:38<06:31,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:52:39,990 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:52:39,990 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:52:39,990 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            " 94% 11687/12400 [3:38:42<06:31,  1.82it/s]{'eval_loss': 0.3824365735054016, 'eval_runtime': 3.9442, 'eval_samples_per_second': 55.525, 'eval_steps_per_second': 1.014, 'epoch': 377.0}\n",
            "\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:52:43,936 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11687\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:52:43,938 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11687/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:52:45,873 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11687/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:52:45,874 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11687/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:52:50,328 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11625] due to args.save_total_limit\n",
            "                                           {'loss': 0.3815, 'learning_rate': 3.351191038571961e-07, 'epoch': 377.1}\n",
            "                                           {'loss': 0.3877, 'learning_rate': 3.2577284733686023e-07, 'epoch': 377.42}\n",
            " 94% 11710/12400 [3:39:07<07:01,  1.64it/s]{'loss': 0.3811, 'learning_rate': 3.1655762909729846e-07, 'epoch': 377.74}\n",
            " 94% 11718/12400 [3:39:11<06:17,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:53:13,633 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:53:13,633 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:53:13,633 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            " 94% 11718/12400 [3:39:16<06:17,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A{'eval_loss': 0.3763153553009033, 'eval_runtime': 3.975, 'eval_samples_per_second': 55.094, 'eval_steps_per_second': 1.006, 'epoch': 378.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:53:17,610 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11718\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:53:17,611 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11718/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:53:19,586 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11718/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:53:19,587 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11718/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:53:24,374 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11656] due to args.save_total_limit\n",
            "{'loss': 0.3788, 'learning_rate': 3.0747351467972915e-07, 'epoch': 378.06}\n",
            "                                           {'loss': 0.3865, 'learning_rate': 2.9852056869293304e-07, 'epoch': 378.39}\n",
            "{'loss': 0.3799, 'learning_rate': 2.8969885481277267e-07, 'epoch': 378.71}\n",
            " 95% 11749/12400 [3:39:45<06:00,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:53:47,652 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:53:47,652 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:53:47,652 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37300756573677063, 'eval_runtime': 3.9903, 'eval_samples_per_second': 54.884, 'eval_steps_per_second': 1.002, 'epoch': 379.0}\n",
            " 95% 11749/12400 [3:39:50<06:00,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:53:51,645 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11749\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:53:51,647 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11749/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:53:53,608 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11749/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:53:53,608 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11749/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:53:58,200 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11687] due to args.save_total_limit\n",
            " 95% 11750/12400 [3:40:01<53:54,  4.98s/it]{'loss': 0.3785, 'learning_rate': 2.810084357817592e-07, 'epoch': 379.03}\n",
            "{'loss': 0.3819, 'learning_rate': 2.7244937340859253e-07, 'epoch': 379.35}\n",
            "{'loss': 0.3885, 'learning_rate': 2.640217285677322e-07, 'epoch': 379.68}\n",
            "{'loss': 0.3826, 'learning_rate': 2.557255611989564e-07, 'epoch': 380.0}\n",
            " 95% 11780/12400 [3:40:20<05:44,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:54:21,845 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:54:21,845 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:54:21,845 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.42it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37362873554229736, 'eval_runtime': 3.9596, 'eval_samples_per_second': 55.309, 'eval_steps_per_second': 1.01, 'epoch': 380.0}\n",
            " 95% 11780/12400 [3:40:24<05:44,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:54:25,806 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11780\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:54:25,808 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11780/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:54:27,822 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11780/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:54:27,823 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11780/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:54:32,315 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11718] due to args.save_total_limit\n",
            " 95% 11790/12400 [3:40:40<08:06,  1.25it/s]{'loss': 0.3864, 'learning_rate': 2.4756093030693505e-07, 'epoch': 380.32}\n",
            "{'loss': 0.3852, 'learning_rate': 2.395278939608218e-07, 'epoch': 380.65}\n",
            " 95% 11810/12400 [3:40:53<05:28,  1.80it/s]{'loss': 0.3762, 'learning_rate': 2.316265092938231e-07, 'epoch': 380.97}\n",
            " 95% 11811/12400 [3:40:53<05:38,  1.74it/s][INFO|trainer.py:2550] 2022-05-10 12:54:55,504 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:54:55,505 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:54:55,505 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.378900408744812, 'eval_runtime': 3.9058, 'eval_samples_per_second': 56.071, 'eval_steps_per_second': 1.024, 'epoch': 381.0}\n",
            " 95% 11811/12400 [3:40:58<05:38,  1.74it/s]\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:54:59,412 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11811\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:54:59,414 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11811/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:55:01,364 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11811/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:55:01,365 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11811/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:55:05,895 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11749] due to args.save_total_limit\n",
            " 95% 11820/12400 [3:41:13<08:32,  1.13it/s]{'loss': 0.388, 'learning_rate': 2.2385683250281325e-07, 'epoch': 381.29}\n",
            "                                           {'loss': 0.3833, 'learning_rate': 2.162189188479116e-07, 'epoch': 381.61}\n",
            " 95% 11840/12400 [3:41:26<05:18,  1.76it/s]{'loss': 0.3805, 'learning_rate': 2.0871282265211009e-07, 'epoch': 381.94}\n",
            " 96% 11842/12400 [3:41:27<05:16,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 12:55:28,973 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:55:28,973 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:55:28,973 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37825360894203186, 'eval_runtime': 3.9161, 'eval_samples_per_second': 55.923, 'eval_steps_per_second': 1.021, 'epoch': 382.0}\n",
            " 96% 11842/12400 [3:41:31<05:16,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:55:32,891 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11842\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:55:32,893 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11842/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:55:34,877 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11842/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:55:34,878 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11842/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:55:39,620 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11780] due to args.save_total_limit\n",
            " 96% 11850/12400 [3:41:47<08:49,  1.04it/s]{'loss': 0.3824, 'learning_rate': 2.0133859730087144e-07, 'epoch': 382.26}\n",
            "{'loss': 0.3831, 'learning_rate': 1.9409629524175867e-07, 'epoch': 382.58}\n",
            "                                           {'loss': 0.3768, 'learning_rate': 1.8698596798406228e-07, 'epoch': 382.9}\n",
            " 96% 11873/12400 [3:42:00<04:57,  1.77it/s][INFO|trainer.py:2550] 2022-05-10 12:56:02,558 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:56:02,558 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:56:02,558 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                           \n",
            " 96% 11873/12400 [3:42:05<04:57,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]{'eval_loss': 0.38268762826919556, 'eval_runtime': 3.9486, 'eval_samples_per_second': 55.463, 'eval_steps_per_second': 1.013, 'epoch': 383.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:56:06,508 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11873\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:56:06,510 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11873/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:56:08,483 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11873/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:56:08,484 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11873/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:56:13,083 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11811] due to args.save_total_limit\n",
            " 96% 11880/12400 [3:42:20<09:37,  1.11s/it]{'loss': 0.39, 'learning_rate': 1.800076660984258e-07, 'epoch': 383.23}\n",
            "                                           {'loss': 0.3819, 'learning_rate': 1.7316143921649178e-07, 'epoch': 383.55}\n",
            " 96% 11900/12400 [3:42:32<04:56,  1.68it/s]{'loss': 0.3746, 'learning_rate': 1.664473360305521e-07, 'epoch': 383.87}\n",
            " 96% 11904/12400 [3:42:34<04:34,  1.80it/s][INFO|trainer.py:2550] 2022-05-10 12:56:36,173 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:56:36,174 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:56:36,174 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.374007910490036, 'eval_runtime': 3.9132, 'eval_samples_per_second': 55.965, 'eval_steps_per_second': 1.022, 'epoch': 384.0}\n",
            " 96% 11904/12400 [3:42:38<04:34,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.65it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:56:40,089 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11904\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:56:40,091 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11904/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:56:42,015 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11904/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:56:42,016 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11904/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:56:46,636 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11842] due to args.save_total_limit\n",
            "{'loss': 0.3906, 'learning_rate': 1.5986540429319198e-07, 'epoch': 384.19}\n",
            "{'loss': 0.391, 'learning_rate': 1.5341569081696313e-07, 'epoch': 384.52}\n",
            "{'loss': 0.3687, 'learning_rate': 1.4709824147403835e-07, 'epoch': 384.84}\n",
            " 96% 11935/12400 [3:43:08<04:19,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:57:09,757 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:57:09,757 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:57:09,757 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            " 96% 11935/12400 [3:43:12<04:19,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:57:13,652 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11935\n",
            "{'eval_loss': 0.38180744647979736, 'eval_runtime': 3.8933, 'eval_samples_per_second': 56.251, 'eval_steps_per_second': 1.027, 'epoch': 385.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:57:13,654 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11935/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:57:15,622 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11935/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:57:15,623 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11935/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:57:20,039 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11873] due to args.save_total_limit\n",
            "{'loss': 0.3804, 'learning_rate': 1.4091310119589276e-07, 'epoch': 385.16}\n",
            " 96% 11950/12400 [3:43:32<04:44,  1.58it/s]{'loss': 0.3822, 'learning_rate': 1.3486031397298148e-07, 'epoch': 385.48}\n",
            "                                           {'loss': 0.3841, 'learning_rate': 1.289399228544291e-07, 'epoch': 385.81}\n",
            " 96% 11966/12400 [3:43:41<04:01,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 12:57:43,187 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:57:43,187 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:57:43,187 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                           \n",
            " 96% 11966/12400 [3:43:45<04:01,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]{'eval_loss': 0.37251731753349304, 'eval_runtime': 3.8827, 'eval_samples_per_second': 56.404, 'eval_steps_per_second': 1.03, 'epoch': 386.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:57:47,071 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11966\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:57:47,073 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11966/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:57:49,071 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11966/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:57:49,072 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11966/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:57:53,649 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11904] due to args.save_total_limit\n",
            " 97% 11970/12400 [3:43:58<14:54,  2.08s/it]{'loss': 0.3853, 'learning_rate': 1.2315196994771987e-07, 'epoch': 386.13}\n",
            "{'loss': 0.3857, 'learning_rate': 1.1749649641840178e-07, 'epoch': 386.45}\n",
            " 97% 11990/12400 [3:44:10<04:04,  1.68it/s]{'loss': 0.3737, 'learning_rate': 1.1197354248979537e-07, 'epoch': 386.77}\n",
            " 97% 11997/12400 [3:44:14<03:40,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:58:16,413 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:58:16,413 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:58:16,413 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            " 97% 11997/12400 [3:44:19<03:40,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.98it/s]{'eval_loss': 0.37929943203926086, 'eval_runtime': 3.8842, 'eval_samples_per_second': 56.382, 'eval_steps_per_second': 1.03, 'epoch': 387.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:58:20,299 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-11997\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:58:20,301 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-11997/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:58:22,217 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-11997/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:58:22,218 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-11997/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:58:26,882 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11935] due to args.save_total_limit\n",
            "                                           {'loss': 0.3737, 'learning_rate': 1.0658314744269383e-07, 'epoch': 387.1}\n",
            " 97% 12010/12400 [3:44:38<04:33,  1.42it/s]{'loss': 0.3823, 'learning_rate': 1.0132534961510493e-07, 'epoch': 387.42}\n",
            " 97% 12020/12400 [3:44:44<03:55,  1.61it/s]{'loss': 0.3931, 'learning_rate': 9.620018640196584e-08, 'epoch': 387.74}\n",
            " 97% 12028/12400 [3:44:48<03:24,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 12:58:50,125 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:58:50,125 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:58:50,125 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                           \n",
            " 97% 12028/12400 [3:44:53<03:24,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]{'eval_loss': 0.3776080906391144, 'eval_runtime': 4.1671, 'eval_samples_per_second': 52.555, 'eval_steps_per_second': 0.96, 'epoch': 388.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:58:54,294 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12028\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:58:54,295 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12028/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:58:56,229 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12028/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:58:56,230 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12028/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:59:00,823 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11966] due to args.save_total_limit\n",
            "                                           {'loss': 0.3824, 'learning_rate': 9.120769425487659e-08, 'epoch': 388.06}\n",
            " 97% 12040/12400 [3:45:11<04:10,  1.44it/s]{'loss': 0.3776, 'learning_rate': 8.634790868184416e-08, 'epoch': 388.39}\n",
            "                                           {'loss': 0.3804, 'learning_rate': 8.162086424702633e-08, 'epoch': 388.71}\n",
            " 97% 12059/12400 [3:45:22<03:08,  1.81it/s][INFO|trainer.py:2550] 2022-05-10 12:59:23,971 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:59:23,971 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:59:23,971 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            " 97% 12059/12400 [3:45:26<03:08,  1.81it/s]\n",
            "{'eval_loss': 0.3817208409309387, 'eval_runtime': 3.9129, 'eval_samples_per_second': 55.969, 'eval_steps_per_second': 1.022, 'epoch': 389.0}\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 12:59:27,885 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12059\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 12:59:27,887 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12059/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 12:59:29,815 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12059/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 12:59:29,816 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12059/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 12:59:34,431 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-11997] due to args.save_total_limit\n",
            "{'loss': 0.3837, 'learning_rate': 7.702659457049648e-08, 'epoch': 389.03}\n",
            " 97% 12070/12400 [3:45:44<04:05,  1.35it/s]{'loss': 0.3852, 'learning_rate': 7.256513232798759e-08, 'epoch': 389.35}\n",
            "{'loss': 0.3678, 'learning_rate': 6.823650925067564e-08, 'epoch': 389.68}\n",
            "                                           {'loss': 0.3915, 'learning_rate': 6.404075612494036e-08, 'epoch': 390.0}\n",
            " 98% 12090/12400 [3:45:56<02:48,  1.84it/s][INFO|trainer.py:2550] 2022-05-10 12:59:57,542 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 12:59:57,542 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 12:59:57,543 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3778541684150696, 'eval_runtime': 3.9407, 'eval_samples_per_second': 55.574, 'eval_steps_per_second': 1.015, 'epoch': 390.0}\n",
            " 98% 12090/12400 [3:46:00<02:48,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:00:01,485 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12090\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:00:01,486 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12090/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:00:03,453 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12090/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:00:03,454 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12090/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:00:07,844 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12028] due to args.save_total_limit\n",
            " 98% 12100/12400 [3:46:16<03:53,  1.28it/s]{'loss': 0.3738, 'learning_rate': 5.99779027921611e-08, 'epoch': 390.32}\n",
            " 98% 12110/12400 [3:46:22<02:58,  1.62it/s]{'loss': 0.3812, 'learning_rate': 5.604797814849002e-08, 'epoch': 390.65}\n",
            " 98% 12120/12400 [3:46:29<02:35,  1.80it/s]{'loss': 0.3872, 'learning_rate': 5.2251010144658403e-08, 'epoch': 390.97}\n",
            " 98% 12121/12400 [3:46:29<02:39,  1.75it/s][INFO|trainer.py:2550] 2022-05-10 13:00:30,812 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:00:30,812 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:00:30,812 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.55it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3698241412639618, 'eval_runtime': 3.8942, 'eval_samples_per_second': 56.237, 'eval_steps_per_second': 1.027, 'epoch': 391.0}\n",
            " 98% 12121/12400 [3:46:33<02:39,  1.75it/s]\n",
            "100% 4/4 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:00:34,708 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12121\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:00:34,710 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12121/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:00:36,642 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12121/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:00:36,643 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12121/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:00:40,973 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12059] due to args.save_total_limit\n",
            "{'loss': 0.3797, 'learning_rate': 4.8587025785766507e-08, 'epoch': 391.29}\n",
            " 98% 12140/12400 [3:46:55<02:37,  1.65it/s]{'loss': 0.3772, 'learning_rate': 4.505605113110031e-08, 'epoch': 391.61}\n",
            " 98% 12150/12400 [3:47:01<02:21,  1.76it/s]{'loss': 0.384, 'learning_rate': 4.165811129394625e-08, 'epoch': 391.94}\n",
            " 98% 12152/12400 [3:47:02<02:20,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 13:01:03,940 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:01:03,940 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:01:03,940 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            " 98% 12152/12400 [3:47:06<02:20,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A{'eval_loss': 0.3818459212779999, 'eval_runtime': 4.0175, 'eval_samples_per_second': 54.512, 'eval_steps_per_second': 0.996, 'epoch': 392.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:01:07,960 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12152\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:01:07,961 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12152/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:01:09,925 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12152/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:01:09,926 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12152/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:01:14,503 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12090] due to args.save_total_limit\n",
            "{'loss': 0.3806, 'learning_rate': 3.8393230441405994e-08, 'epoch': 392.26}\n",
            " 98% 12170/12400 [3:47:27<02:23,  1.60it/s]{'loss': 0.3797, 'learning_rate': 3.526143179423194e-08, 'epoch': 392.58}\n",
            "                                           {'loss': 0.3863, 'learning_rate': 3.2262737626650295e-08, 'epoch': 392.9}\n",
            " 98% 12183/12400 [3:47:35<02:02,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 13:01:37,512 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:01:37,512 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:01:37,512 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.43it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A{'eval_loss': 0.3780148923397064, 'eval_runtime': 3.908, 'eval_samples_per_second': 56.039, 'eval_steps_per_second': 1.024, 'epoch': 393.0}\n",
            "                                           \n",
            " 98% 12183/12400 [3:47:40<02:02,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:01:41,422 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12183\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:01:41,423 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12183/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:01:43,333 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12183/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:01:43,334 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12183/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:01:47,809 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12121] due to args.save_total_limit\n",
            "                                           {'loss': 0.3822, 'learning_rate': 2.939716926622371e-08, 'epoch': 393.23}\n",
            "{'loss': 0.3825, 'learning_rate': 2.6664747093676358e-08, 'epoch': 393.55}\n",
            "                                           {'loss': 0.3837, 'learning_rate': 2.4065490542760774e-08, 'epoch': 393.87}\n",
            " 98% 12214/12400 [3:48:09<01:45,  1.76it/s][INFO|trainer.py:2550] 2022-05-10 13:02:11,249 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:02:11,249 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:02:11,250 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.38474196195602417, 'eval_runtime': 3.9806, 'eval_samples_per_second': 55.017, 'eval_steps_per_second': 1.005, 'epoch': 394.0}\n",
            " 98% 12214/12400 [3:48:13<01:45,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.68it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:02:15,232 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12214\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:02:15,233 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12214/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:02:17,201 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12214/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:02:17,202 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12214/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:02:21,787 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12152] due to args.save_total_limit\n",
            " 99% 12220/12400 [3:48:28<04:02,  1.35s/it]{'loss': 0.3894, 'learning_rate': 2.159941810012042e-08, 'epoch': 394.19}\n",
            "{'loss': 0.3851, 'learning_rate': 1.926654730515856e-08, 'epoch': 394.52}\n",
            "                                           {'loss': 0.3722, 'learning_rate': 1.7066894749905025e-08, 'epoch': 394.84}\n",
            " 99% 12245/12400 [3:48:43<01:27,  1.78it/s][INFO|trainer.py:2550] 2022-05-10 13:02:45,196 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:02:45,197 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:02:45,197 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                           \n",
            " 99% 12245/12400 [3:48:47<01:27,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]{'eval_loss': 0.3797823488712311, 'eval_runtime': 3.9278, 'eval_samples_per_second': 55.756, 'eval_steps_per_second': 1.018, 'epoch': 395.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:02:49,126 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12245\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:02:49,128 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12245/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:02:51,170 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12245/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:02:51,171 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12245/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:02:55,847 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12183] due to args.save_total_limit\n",
            "                                           {'loss': 0.3852, 'learning_rate': 1.5000476078910057e-08, 'epoch': 395.16}\n",
            "{'loss': 0.3787, 'learning_rate': 1.3067305989119397e-08, 'epoch': 395.48}\n",
            " 99% 12270/12400 [3:49:14<01:21,  1.59it/s]{'loss': 0.383, 'learning_rate': 1.1267398229784786e-08, 'epoch': 395.81}\n",
            " 99% 12276/12400 [3:49:17<01:09,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 13:03:19,228 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:03:19,228 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:03:19,228 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37939363718032837, 'eval_runtime': 4.0139, 'eval_samples_per_second': 54.561, 'eval_steps_per_second': 0.997, 'epoch': 396.0}\n",
            " 99% 12276/12400 [3:49:21<01:09,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:03:23,243 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12276\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:03:23,245 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12276/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:03:25,287 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12276/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:03:25,288 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12276/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:03:30,175 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12214] due to args.save_total_limit\n",
            "{'loss': 0.3943, 'learning_rate': 9.600765602355704e-09, 'epoch': 396.13}\n",
            "                                           {'loss': 0.3782, 'learning_rate': 8.067419960391952e-09, 'epoch': 396.45}\n",
            " 99% 12300/12400 [3:49:47<01:01,  1.63it/s]{'loss': 0.3814, 'learning_rate': 6.667372209476218e-09, 'epoch': 396.77}\n",
            " 99% 12307/12400 [3:49:51<00:51,  1.79it/s][INFO|trainer.py:2550] 2022-05-10 13:03:53,432 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:03:53,433 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:03:53,433 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37840893864631653, 'eval_runtime': 4.056, 'eval_samples_per_second': 53.995, 'eval_steps_per_second': 0.986, 'epoch': 397.0}\n",
            " 99% 12307/12400 [3:49:56<00:51,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:03:57,490 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12307\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:03:57,492 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12307/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:03:59,450 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12307/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:03:59,451 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12307/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:04:03,999 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12245] due to args.save_total_limit\n",
            " 99% 12310/12400 [3:50:08<04:06,  2.74s/it]{'loss': 0.3688, 'learning_rate': 5.400632307145381e-09, 'epoch': 397.1}\n",
            " 99% 12320/12400 [3:50:15<00:56,  1.42it/s]{'loss': 0.3911, 'learning_rate': 4.2672092628072415e-09, 'epoch': 397.42}\n",
            " 99% 12330/12400 [3:50:21<00:43,  1.61it/s]{'loss': 0.3881, 'learning_rate': 3.2671111376843253e-09, 'epoch': 397.74}\n",
            "100% 12338/12400 [3:50:25<00:34,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 13:04:27,264 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:04:27,264 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:04:27,265 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                           \n",
            "100% 12338/12400 [3:50:29<00:34,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.372389018535614, 'eval_runtime': 3.9224, 'eval_samples_per_second': 55.834, 'eval_steps_per_second': 1.02, 'epoch': 398.0}\n",
            "[INFO|trainer.py:2270] 2022-05-10 13:04:31,189 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12338\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:04:31,190 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12338/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:04:33,173 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12338/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:04:33,174 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12338/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:04:37,491 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12276] due to args.save_total_limit\n",
            "{'loss': 0.3754, 'learning_rate': 2.4003450447576676e-09, 'epoch': 398.06}\n",
            "{'loss': 0.3735, 'learning_rate': 1.666917148710617e-09, 'epoch': 398.39}\n",
            "100% 12360/12400 [3:50:54<00:24,  1.63it/s]{'loss': 0.3791, 'learning_rate': 1.0668326658871962e-09, 'epoch': 398.71}\n",
            "100% 12369/12400 [3:50:58<00:17,  1.82it/s][INFO|trainer.py:2550] 2022-05-10 13:05:00,473 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:05:00,473 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:05:00,473 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.55it/s]\u001b[A\n",
            "                                           \n",
            "100% 12369/12400 [3:51:03<00:17,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A{'eval_loss': 0.36800068616867065, 'eval_runtime': 3.9234, 'eval_samples_per_second': 55.819, 'eval_steps_per_second': 1.02, 'epoch': 399.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:05:04,398 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12369\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:05:04,400 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12369/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:05:06,382 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12369/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:05:06,383 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12369/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:05:10,884 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12307] due to args.save_total_limit\n",
            "100% 12370/12400 [3:51:14<02:26,  4.90s/it]{'loss': 0.3887, 'learning_rate': 6.000958642567166e-10, 'epoch': 399.03}\n",
            "{'loss': 0.3869, 'learning_rate': 2.6671006338463417e-10, 'epoch': 399.35}\n",
            "{'loss': 0.3788, 'learning_rate': 6.66776344034059e-11, 'epoch': 399.68}\n",
            "100% 12400/12400 [3:51:32<00:00,  1.83it/s][INFO|trainer.py:2550] 2022-05-10 13:05:34,018 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:05:34,019 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:05:34,019 >>   Batch size = 64\n",
            "{'loss': 0.3855, 'learning_rate': 0.0, 'epoch': 400.0}\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.37364014983177185, 'eval_runtime': 3.9428, 'eval_samples_per_second': 55.544, 'eval_steps_per_second': 1.014, 'epoch': 400.0}\n",
            "100% 12400/12400 [3:51:36<00:00,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.67it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2270] 2022-05-10 13:05:37,963 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/checkpoint-12400\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:05:37,965 >> Configuration saved in orchid219_pretrain_vit-mae-base/checkpoint-12400/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:05:39,939 >> Model weights saved in orchid219_pretrain_vit-mae-base/checkpoint-12400/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:05:39,941 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/checkpoint-12400/preprocessor_config.json\n",
            "[INFO|trainer.py:2378] 2022-05-10 13:05:45,102 >> Deleting older checkpoint [orchid219_pretrain_vit-mae-base/checkpoint-12338] due to args.save_total_limit\n",
            "[INFO|trainer.py:1622] 2022-05-10 13:05:45,283 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1687] 2022-05-10 13:05:45,283 >> Loading best model from orchid219_pretrain_vit-mae-base/checkpoint-9548 (score: 0.36473390460014343).\n",
            "                                           {'train_runtime': 13916.0511, 'train_samples_per_second': 56.654, 'train_steps_per_second': 0.891, 'train_loss': 0.3955882899799655, 'epoch': 400.0}\n",
            "100% 12400/12400 [3:51:52<00:00,  1.12s/it]\n",
            "[INFO|trainer.py:2270] 2022-05-10 13:05:53,467 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:05:53,469 >> Configuration saved in orchid219_pretrain_vit-mae-base/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:05:56,134 >> Model weights saved in orchid219_pretrain_vit-mae-base/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:05:56,135 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/preprocessor_config.json\n",
            "[INFO|trainer.py:2270] 2022-05-10 13:05:56,135 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:05:56,137 >> Configuration saved in orchid219_pretrain_vit-mae-base/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:05:58,299 >> Model weights saved in orchid219_pretrain_vit-mae-base/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:05:58,300 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/preprocessor_config.json\n",
            "Several commits (2) will be pushed upstream.\n",
            "WARNING:huggingface_hub.repository:Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n",
            "WARNING:huggingface_hub.repository:The progress bars may be unreliable.\n",
            "Upload file pytorch_model.bin:   0% 3.34k/427M [00:00<?, ?B/s]\n",
            "Upload file pytorch_model.bin:   0% 359k/427M [00:01<20:30, 364kB/s]\n",
            "Upload file pytorch_model.bin:   2% 9.71M/427M [00:10<06:44, 1.08MB/s]\n",
            "Upload file pytorch_model.bin: 100% 427M/427M [06:49<00:00, 1.11MB/s]remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_vit-mae-base\n",
            "   a95ad9d..a36ca6a  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_vit-mae-base\n",
            "   a95ad9d..a36ca6a  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 427M/427M [06:50<00:00, 1.09MB/s]\n",
            "\n",
            "Upload file runs/May10_09-13-26_f17f2ccc4c2b/events.out.tfevents.1652174037.f17f2ccc4c2b.1007.0: 100% 300k/300k [06:50<00:00, 542B/s] \u001b[A\n",
            "Upload file runs/May10_09-13-26_f17f2ccc4c2b/events.out.tfevents.1652174037.f17f2ccc4c2b.1007.0: 100% 300k/300k [06:50<00:00, 739B/s]\n",
            "[INFO|modelcard.py:460] 2022-05-10 13:13:00,599 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'dataset': {'name': 'orchid219', 'type': 'orchid219', 'args': 'orchid219'}}\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_vit-mae-base\n",
            "   a36ca6a..0cb4e9e  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_vit-mae-base\n",
            "   a36ca6a..0cb4e9e  main -> main\n",
            "\n",
            "***** train metrics *****\n",
            "  epoch                    =      400.0\n",
            "  train_loss               =     0.3956\n",
            "  train_runtime            = 3:51:56.05\n",
            "  train_samples_per_second =     56.654\n",
            "  train_steps_per_second   =      0.891\n",
            "[INFO|trainer.py:2550] 2022-05-10 13:13:05,831 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2552] 2022-05-10 13:13:05,831 >>   Num examples = 219\n",
            "[INFO|trainer.py:2555] 2022-05-10 13:13:05,831 >>   Batch size = 64\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]***** eval metrics *****\n",
            "100% 4/4 [00:00<00:00,  8.57it/s]\n",
            "  epoch                   =      400.0\n",
            "  eval_loss               =     0.3769\n",
            "  eval_runtime            = 0:00:04.04\n",
            "  eval_samples_per_second =       54.1\n",
            "  eval_steps_per_second   =      0.988\n",
            "[INFO|trainer.py:2270] 2022-05-10 13:13:09,913 >> Saving model checkpoint to orchid219_pretrain_vit-mae-base/\n",
            "[INFO|configuration_utils.py:446] 2022-05-10 13:13:09,915 >> Configuration saved in orchid219_pretrain_vit-mae-base/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-10 13:13:12,385 >> Model weights saved in orchid219_pretrain_vit-mae-base/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-10 13:13:12,386 >> Feature extractor saved in orchid219_pretrain_vit-mae-base/preprocessor_config.json\n",
            "Upload file pytorch_model.bin:   0% 3.34k/427M [00:00<?, ?B/s]\n",
            "Upload file pytorch_model.bin: 100% 427M/427M [06:39<00:00, 1.17MB/s]remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_vit-mae-base\n",
            "   0cb4e9e..9156d58  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_vit-mae-base\n",
            "   0cb4e9e..9156d58  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 427M/427M [06:40<00:00, 1.12MB/s]\n",
            "\n",
            "Upload file runs/May10_09-13-26_f17f2ccc4c2b/events.out.tfevents.1652188389.f17f2ccc4c2b.1007.2: 100% 311/311 [06:40<?, ?B/s]\u001b[A\n",
            "Upload file runs/May10_09-13-26_f17f2ccc4c2b/events.out.tfevents.1652188389.f17f2ccc4c2b.1007.2: 100% 311/311 [06:40<?, ?B/s]\n",
            "[INFO|modelcard.py:460] 2022-05-10 13:20:02,939 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'dataset': {'name': 'gary109/orchid219', 'type': 'orchid219', 'args': 'orchid219'}}\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_vit-mae-base\n",
            "   9156d58..7dc7234  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_vit-mae-base\n",
            "   9156d58..7dc7234  main -> main\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▂▂▂▂▁▂▁▁▁▁▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁▂▂█▅▅▆▇▅▆▂▂▂▂▂▂▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second █▇▇▁▃▃▃▂▄▃▇▇▇▇▇▆▆▆▇▆▆▆▆▆▇▇▇▇▇▆▇▇▆▇▆▆▆▆▄▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second █▇▇▁▃▃▃▂▄▃▇▇▇▇▇▆▆▆▇▆▆▆▆▆▇▇▇▇▇▆▇▇▆▇▆▆▆▆▄▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▃▅███████▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▄▃▃▃▃▂▂▃▂▃▃▂▂▃▂▁▂▂▂▂▂▂▁▂▁▁▂▁▂▂▁▁▁▂▂▁▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.37688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 4.0481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 54.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.988\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 400.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 12400\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.3855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 7.96848337012654e+19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.39559\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 13916.0511\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 56.654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.891\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33morchid219_pretrain_vit-mae-base/\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/gary109/huggingface/runs/uu53x3br\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220510_091357-uu53x3br/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch run_mae.py \\\n",
        "    --dataset_name=\"gary109/orchid219\" \\\n",
        "    --model_name_or_path=\"facebook/vit-mae-base\" \\\n",
        "    --output_dir \"orchid219_pretrain_vit-mae-base/\" \\\n",
        "    --remove_unused_columns=\"False\" \\\n",
        "    --label_names=\"pixel_values\" \\\n",
        "    --mask_ratio=\"0.75\" \\\n",
        "    --norm_pix_loss --do_train --do_eval \\\n",
        "    --base_learning_rate=\"1.5e-4\" \\\n",
        "    --lr_scheduler_type=\"cosine\" \\\n",
        "    --weight_decay=\"0.05\" \\\n",
        "    --num_train_epochs=\"400\" \\\n",
        "    --warmup_ratio=\"0.05\" \\\n",
        "    --per_device_train_batch_size=\"64\" \\\n",
        "    --per_device_eval_batch_size=\"64\" \\\n",
        "    --logging_strategy=\"steps\" \\\n",
        "    --logging_steps=\"10\" \\\n",
        "    --evaluation_strategy=\"epoch\" \\\n",
        "    --save_strategy=\"epoch\" \\\n",
        "    --load_best_model_at_end=\"True\" \\\n",
        "    --save_total_limit=\"3\" \\\n",
        "    --overwrite_output_dir \\\n",
        "    --push_to_hub \\\n",
        "    --hub_model_id=\"orchid219_pretrain_vit-mae-base\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --seed=\"1337\" \\\n",
        "    --save_steps=\"1000\" \\\n",
        "    --use_auth_token=\"True\"\n",
        "\n",
        "#  --cache_dir=\"/content/drive/MyDrive/datasets/cache_orchid219\"\n",
        "# !OMP_NUM_THREADS=1     \n",
        "# --dataset_config_name=\"crop14-balance\" \\\n",
        "# --gradient_accumulation_steps 8 \\\n",
        "# --gradient_checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PntiQvYf92jN"
      },
      "source": [
        "### [facebook/vit-mae-large] ===> orchid219_pretrain_vit-mae-large\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlJ2BKsa90N3",
        "outputId": "72be2d96-e3f9-460d-d1f5-cd600bb1bd1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: accelerate: command not found\n"
          ]
        }
      ],
      "source": [
        "!OMP_NUM_THREADS=1 accelerate launch run_mae.py \\\n",
        "    --dataset_name=\"gary109/orchid219\" \\\n",
        "    --model_name_or_path=\"facebook/vit-mae-large\" \\\n",
        "    --output_dir=\"orchid219_pretrain_vit-mae-large\" \\\n",
        "    --remove_unused_columns=\"False\" \\\n",
        "    --label_names=\"pixel_values\" \\\n",
        "    --mask_ratio=\"0.75\" \\\n",
        "    --norm_pix_loss --do_train --do_eval \\\n",
        "    --base_learning_rate=\"1.5e-4\" \\\n",
        "    --lr_scheduler_type=\"cosine\" \\\n",
        "    --weight_decay=\"0.05\" \\\n",
        "    --num_train_epochs=\"800\" \\\n",
        "    --warmup_ratio=\"0.05\" \\\n",
        "    --per_device_train_batch_size=\"8\" \\\n",
        "    --per_device_eval_batch_size=\"8\" \\\n",
        "    --logging_strategy=\"steps\" \\\n",
        "    --logging_steps=\"10\" \\\n",
        "    --evaluation_strategy=\"epoch\" \\\n",
        "    --save_strategy=\"epoch\" \\\n",
        "    --load_best_model_at_end=\"True\" \\\n",
        "    --save_total_limit=\"3\" \\\n",
        "    --overwrite_output_dir \\\n",
        "    --push_to_hub \\\n",
        "    --hub_model_id=\"orchid219_pretrain_vit-mae-large\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --seed=\"1337\" \\\n",
        "    --save_steps=\"1000\" \\\n",
        "    --use_auth_token=\"True\" \n",
        "\n",
        "    # --cache_dir=\"/content/drive/MyDrive/datasets/cache_orchid219\"\n",
        "    # --gradient_accumulation_steps=\"8\" \\\n",
        "    # --gradient_checkpointing \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYwSZcbDD-ir"
      },
      "source": [
        "### [可行] facebook/data2vec-vision-base\n",
        "- crop14_balance_pretrain_data2vec-vision-base-mae\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_yh5I-EEME6",
        "outputId": "69570763-e237-4b78-feab-d1a4b01fc9d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "                                             {'loss': 0.4345, 'learning_rate': 6.912716764616719e-06, 'epoch': 731.61}\n",
            "                                             {'loss': 0.455, 'learning_rate': 6.897211802392931e-06, 'epoch': 731.94}\n",
            " 73% 22692/31000 [6:42:07<1:16:36,  1.81it/s][INFO|trainer.py:2625] 2022-05-18 22:55:46,998 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 22:55:46,998 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 22:55:46,998 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "{'eval_loss': 0.4328511655330658, 'eval_runtime': 3.8019, 'eval_samples_per_second': 57.603, 'eval_steps_per_second': 1.052, 'epoch': 732.0}\n",
            "\n",
            " 73% 22692/31000 [6:42:11<1:16:36,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 22:55:50,801 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22692\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 22:55:50,803 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22692/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 22:55:52,776 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22692/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 22:55:52,777 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22692/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 22:55:57,147 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22630] due to args.save_total_limit\n",
            " 73% 22700/31000 [6:42:26<2:08:16,  1.08it/s]{'loss': 0.4523, 'learning_rate': 6.881720328233498e-06, 'epoch': 732.26}\n",
            " 73% 22710/31000 [6:42:32<1:19:29,  1.74it/s]{'loss': 0.4393, 'learning_rate': 6.866242359767202e-06, 'epoch': 732.58}\n",
            "                                             {'loss': 0.4393, 'learning_rate': 6.850777914607442e-06, 'epoch': 732.9}\n",
            " 73% 22723/31000 [6:42:39<1:15:45,  1.82it/s][INFO|trainer.py:2625] 2022-05-18 22:56:18,923 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 22:56:18,923 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 22:56:18,923 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                             \n",
            "{'eval_loss': 0.4331464469432831, 'eval_runtime': 3.7582, 'eval_samples_per_second': 58.272, 'eval_steps_per_second': 1.064, 'epoch': 733.0}\n",
            " 73% 22723/31000 [6:42:43<1:15:45,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 22:56:22,683 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22723\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 22:56:22,685 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22723/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 22:56:24,613 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22723/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 22:56:24,614 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22723/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 22:56:28,900 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22661] due to args.save_total_limit\n",
            " 73% 22730/31000 [6:42:58<2:27:23,  1.07s/it]{'loss': 0.4439, 'learning_rate': 6.8353270103522056e-06, 'epoch': 733.23}\n",
            " 73% 22740/31000 [6:43:03<1:19:39,  1.73it/s]{'loss': 0.4338, 'learning_rate': 6.819889664584096e-06, 'epoch': 733.55}\n",
            "{'loss': 0.4509, 'learning_rate': 6.804465894870284e-06, 'epoch': 733.87}\n",
            " 73% 22754/31000 [6:43:11<1:15:07,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 22:56:50,801 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 22:56:50,801 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 22:56:50,801 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "{'eval_loss': 0.4263162910938263, 'eval_runtime': 3.792, 'eval_samples_per_second': 57.753, 'eval_steps_per_second': 1.055, 'epoch': 734.0}\n",
            "\n",
            " 73% 22754/31000 [6:43:15<1:15:07,  1.83it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 22:56:54,594 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22754\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 22:56:54,596 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22754/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 22:56:56,472 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22754/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 22:56:56,474 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22754/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 22:57:00,851 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22692] due to args.save_total_limit\n",
            " 73% 22760/31000 [6:43:29<2:57:13,  1.29s/it]{'loss': 0.4542, 'learning_rate': 6.789055718762492e-06, 'epoch': 734.19}\n",
            "                                             {'loss': 0.432, 'learning_rate': 6.773659153796955e-06, 'epoch': 734.52}\n",
            "{'loss': 0.4527, 'learning_rate': 6.758276217494435e-06, 'epoch': 734.84}\n",
            " 74% 22785/31000 [6:43:43<1:13:54,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 22:57:22,771 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 22:57:22,771 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 22:57:22,772 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.83it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.98it/s]\u001b[A\n",
            "                                             \n",
            " 74% 22785/31000 [6:43:47<1:13:54,  1.85it/s]\n",
            "                                 {'eval_loss': 0.42763829231262207, 'eval_runtime': 3.7553, 'eval_samples_per_second': 58.318, 'eval_steps_per_second': 1.065, 'epoch': 735.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-18 22:57:26,528 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22785\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 22:57:26,530 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22785/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 22:57:28,431 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22785/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 22:57:28,432 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22785/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 22:57:32,735 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22723] due to args.save_total_limit\n",
            " 74% 22790/31000 [6:44:00<3:38:22,  1.60s/it]{'loss': 0.4352, 'learning_rate': 6.742906927360204e-06, 'epoch': 735.16}\n",
            "{'loss': 0.4508, 'learning_rate': 6.727551300883962e-06, 'epoch': 735.48}\n",
            "                                             {'loss': 0.4475, 'learning_rate': 6.7122093555398954e-06, 'epoch': 735.81}\n",
            " 74% 22816/31000 [6:44:15<1:13:42,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 22:57:54,658 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 22:57:54,659 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 22:57:54,659 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                             \n",
            " 74% 22816/31000 [6:44:19<1:13:42,  1.85it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.42650747299194336, 'eval_runtime': 3.7224, 'eval_samples_per_second': 58.834, 'eval_steps_per_second': 1.075, 'epoch': 736.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 22:57:58,383 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22816\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 22:57:58,385 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22816/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 22:58:00,378 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22816/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 22:58:00,380 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22816/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 22:58:04,670 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22754] due to args.save_total_limit\n",
            " 74% 22820/31000 [6:44:32<4:32:05,  2.00s/it]{'loss': 0.4323, 'learning_rate': 6.696881108786592e-06, 'epoch': 736.13}\n",
            " 74% 22830/31000 [6:44:38<1:24:40,  1.61it/s]{'loss': 0.4324, 'learning_rate': 6.681566578067093e-06, 'epoch': 736.45}\n",
            "                                             {'loss': 0.4415, 'learning_rate': 6.666265780808787e-06, 'epoch': 736.77}\n",
            " 74% 22847/31000 [6:44:47<1:13:40,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 22:58:26,646 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 22:58:26,647 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 22:58:26,647 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.43779975175857544, 'eval_runtime': 3.7392, 'eval_samples_per_second': 58.569, 'eval_steps_per_second': 1.07, 'epoch': 737.0}\n",
            " 74% 22847/31000 [6:44:51<1:13:40,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 22:58:30,387 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22847\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 22:58:30,390 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22847/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 22:58:32,314 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22847/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 22:58:32,316 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22847/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 22:58:36,845 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22785] due to args.save_total_limit\n",
            " 74% 22850/31000 [6:45:03<5:58:02,  2.64s/it]{'loss': 0.464, 'learning_rate': 6.650978734423461e-06, 'epoch': 737.1}\n",
            "{'loss': 0.4429, 'learning_rate': 6.635705456307245e-06, 'epoch': 737.42}\n",
            "                                             {'loss': 0.4409, 'learning_rate': 6.620445963840605e-06, 'epoch': 737.74}\n",
            " 74% 22878/31000 [6:45:19<1:13:05,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 22:58:58,861 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 22:58:58,861 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 22:58:58,862 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.56it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.69it/s]\u001b[A\n",
            "                                             \n",
            "{'eval_loss': 0.42704546451568604, 'eval_runtime': 3.8761, 'eval_samples_per_second': 56.5, 'eval_steps_per_second': 1.032, 'epoch': 738.0}\n",
            " 74% 22878/31000 [6:45:23<1:13:05,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 22:59:02,739 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22878\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 22:59:02,741 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22878/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 22:59:04,632 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22878/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 22:59:04,633 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22878/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 22:59:09,035 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22816] due to args.save_total_limit\n",
            " 74% 22880/31000 [6:45:34<7:54:22,  3.51s/it] {'loss': 0.4291, 'learning_rate': 6.605200274388322e-06, 'epoch': 738.06}\n",
            " 74% 22890/31000 [6:45:40<1:31:16,  1.48it/s]{'loss': 0.447, 'learning_rate': 6.589968405299455e-06, 'epoch': 738.39}\n",
            "{'loss': 0.4511, 'learning_rate': 6.574750373907349e-06, 'epoch': 738.71}\n",
            " 74% 22909/31000 [6:45:51<1:13:01,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 22:59:31,038 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 22:59:31,038 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 22:59:31,038 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.88it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.43075236678123474, 'eval_runtime': 3.8112, 'eval_samples_per_second': 57.463, 'eval_steps_per_second': 1.05, 'epoch': 739.0}\n",
            " 74% 22909/31000 [6:45:55<1:13:01,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 22:59:34,851 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22909\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 22:59:34,853 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22909/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 22:59:36,848 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22909/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 22:59:36,849 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22909/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 22:59:41,413 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22847] due to args.save_total_limit\n",
            " 74% 22910/31000 [6:46:06<10:50:47,  4.83s/it]{'loss': 0.4313, 'learning_rate': 6.5595461975296e-06, 'epoch': 739.03}\n",
            "{'loss': 0.445, 'learning_rate': 6.544355893468039e-06, 'epoch': 739.35}\n",
            " 74% 22930/31000 [6:46:18<1:16:49,  1.75it/s]{'loss': 0.4448, 'learning_rate': 6.529179479008706e-06, 'epoch': 739.68}\n",
            "                                             {'loss': 0.4433, 'learning_rate': 6.5140169714218364e-06, 'epoch': 740.0}\n",
            " 74% 22940/31000 [6:46:24<1:12:37,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:00:03,370 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:00:03,370 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:00:03,371 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.60it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.4267643690109253, 'eval_runtime': 3.7881, 'eval_samples_per_second': 57.813, 'eval_steps_per_second': 1.056, 'epoch': 740.0}\n",
            " 74% 22940/31000 [6:46:28<1:12:37,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:00:07,160 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22940\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:00:07,162 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22940/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:00:09,092 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22940/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:00:09,093 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22940/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:00:13,710 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22878] due to args.save_total_limit\n",
            " 74% 22950/31000 [6:46:44<1:42:50,  1.30it/s]{'loss': 0.4302, 'learning_rate': 6.49886838796185e-06, 'epoch': 740.32}\n",
            "{'loss': 0.4601, 'learning_rate': 6.483733745867299e-06, 'epoch': 740.65}\n",
            "{'loss': 0.4408, 'learning_rate': 6.4686130623608915e-06, 'epoch': 740.97}\n",
            " 74% 22971/31000 [6:46:56<1:15:09,  1.78it/s][INFO|trainer.py:2625] 2022-05-18 23:00:35,630 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:00:35,630 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:00:35,630 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "{'eval_loss': 0.43560582399368286, 'eval_runtime': 3.7446, 'eval_samples_per_second': 58.485, 'eval_steps_per_second': 1.068, 'epoch': 741.0}\n",
            "\n",
            " 74% 22971/31000 [6:47:00<1:15:09,  1.78it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:00:39,376 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22971\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:00:39,377 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22971/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:00:41,273 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22971/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:00:41,274 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22971/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:00:45,924 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22909] due to args.save_total_limit\n",
            " 74% 22980/31000 [6:47:16<1:53:27,  1.18it/s]{'loss': 0.4467, 'learning_rate': 6.453506354649443e-06, 'epoch': 741.29}\n",
            "{'loss': 0.4388, 'learning_rate': 6.4384136399238665e-06, 'epoch': 741.61}\n",
            " 74% 23000/31000 [6:47:27<1:12:33,  1.84it/s]{'loss': 0.4404, 'learning_rate': 6.4233349353591565e-06, 'epoch': 741.94}\n",
            " 74% 23002/31000 [6:47:28<1:13:45,  1.81it/s][INFO|trainer.py:2625] 2022-05-18 23:01:07,855 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:01:07,856 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:01:07,856 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.82it/s]\u001b[A\n",
            "                                             \n",
            " 74% 23002/31000 [6:47:32<1:13:45,  1.81it/s]\n",
            "{'eval_loss': 0.42861834168434143, 'eval_runtime': 3.7916, 'eval_samples_per_second': 57.76, 'eval_steps_per_second': 1.055, 'epoch': 742.0}\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:01:11,649 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23002\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:01:11,651 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23002/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:01:13,564 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23002/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:01:13,565 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23002/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:01:17,967 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22940] due to args.save_total_limit\n",
            "{'loss': 0.4418, 'learning_rate': 6.4082702581143406e-06, 'epoch': 742.26}\n",
            "                                             {'loss': 0.4543, 'learning_rate': 6.393219625332529e-06, 'epoch': 742.58}\n",
            " 74% 23030/31000 [6:47:59<1:16:02,  1.75it/s]{'loss': 0.4353, 'learning_rate': 6.378183054140801e-06, 'epoch': 742.9}\n",
            " 74% 23033/31000 [6:48:01<1:14:00,  1.79it/s][INFO|trainer.py:2625] 2022-05-18 23:01:40,323 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:01:40,323 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:01:40,323 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                             \n",
            " 74% 23033/31000 [6:48:05<1:14:00,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A{'eval_loss': 0.42706498503685, 'eval_runtime': 3.8739, 'eval_samples_per_second': 56.532, 'eval_steps_per_second': 1.033, 'epoch': 743.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:01:44,199 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23033\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:01:44,201 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23033/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:01:46,121 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23033/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:01:46,122 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23033/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:01:50,353 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-22971] due to args.save_total_limit\n",
            " 74% 23040/31000 [6:48:19<2:23:11,  1.08s/it]{'loss': 0.4406, 'learning_rate': 6.363160561650273e-06, 'epoch': 743.23}\n",
            " 74% 23050/31000 [6:48:25<1:20:37,  1.64it/s]{'loss': 0.4452, 'learning_rate': 6.348152164955999e-06, 'epoch': 743.55}\n",
            "{'loss': 0.4365, 'learning_rate': 6.333157881137041e-06, 'epoch': 743.87}\n",
            " 74% 23064/31000 [6:48:33<1:12:41,  1.82it/s][INFO|trainer.py:2625] 2022-05-18 23:02:12,574 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:02:12,574 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:02:12,575 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.4343319833278656, 'eval_runtime': 3.7949, 'eval_samples_per_second': 57.709, 'eval_steps_per_second': 1.054, 'epoch': 744.0}\n",
            " 74% 23064/31000 [6:48:37<1:12:41,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:02:16,371 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23064\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:02:16,373 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23064/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:02:18,287 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23064/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:02:18,288 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23064/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:02:22,697 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23002] due to args.save_total_limit\n",
            " 74% 23070/31000 [6:48:51<2:50:49,  1.29s/it]{'loss': 0.4485, 'learning_rate': 6.318177727256374e-06, 'epoch': 744.19}\n",
            "                                             {'loss': 0.4351, 'learning_rate': 6.303211720360886e-06, 'epoch': 744.52}\n",
            " 74% 23090/31000 [6:49:02<1:14:56,  1.76it/s]{'loss': 0.4479, 'learning_rate': 6.288259877481384e-06, 'epoch': 744.84}\n",
            " 74% 23095/31000 [6:49:05<1:11:30,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:02:44,623 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:02:44,623 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:02:44,623 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                             \n",
            " 74% 23095/31000 [6:49:09<1:11:30,  1.84it/s]{'eval_loss': 0.4281131327152252, 'eval_runtime': 3.8139, 'eval_samples_per_second': 57.421, 'eval_steps_per_second': 1.049, 'epoch': 745.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:02:48,438 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23095\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:02:48,440 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23095/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:02:50,342 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23095/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:02:50,342 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23095/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:02:54,646 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23033] due to args.save_total_limit\n",
            "{'loss': 0.448, 'learning_rate': 6.273322215632548e-06, 'epoch': 745.16}\n",
            " 75% 23110/31000 [6:49:28<1:19:47,  1.65it/s]{'loss': 0.4373, 'learning_rate': 6.2583987518129275e-06, 'epoch': 745.48}\n",
            "                                             {'loss': 0.4471, 'learning_rate': 6.2434895030048986e-06, 'epoch': 745.81}\n",
            " 75% 23126/31000 [6:49:37<1:11:08,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:03:16,684 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:03:16,684 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:03:16,684 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.43638351559638977, 'eval_runtime': 3.7825, 'eval_samples_per_second': 57.898, 'eval_steps_per_second': 1.057, 'epoch': 746.0}\n",
            " 75% 23126/31000 [6:49:41<1:11:08,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:03:20,469 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23126\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:03:20,470 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23126/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:03:22,373 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23126/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:03:22,373 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23126/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:03:26,872 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23064] due to args.save_total_limit\n",
            " 75% 23130/31000 [6:49:54<4:24:37,  2.02s/it]{'loss': 0.4389, 'learning_rate': 6.228594486174675e-06, 'epoch': 746.13}\n",
            " 75% 23140/31000 [6:49:59<1:22:13,  1.59it/s]{'loss': 0.4384, 'learning_rate': 6.213713718272286e-06, 'epoch': 746.45}\n",
            "{'loss': 0.4442, 'learning_rate': 6.198847216231519e-06, 'epoch': 746.77}\n",
            " 75% 23157/31000 [6:50:09<1:11:26,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:03:49,008 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:03:49,008 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:03:49,008 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "\u001b[A{'eval_loss': 0.4235852062702179, 'eval_runtime': 3.7979, 'eval_samples_per_second': 57.663, 'eval_steps_per_second': 1.053, 'epoch': 747.0}\n",
            "                                             \n",
            " 75% 23157/31000 [6:50:13<1:11:26,  1.83it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:03:52,808 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23157\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:03:52,810 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23157/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:03:54,749 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23157/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:03:54,750 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23157/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:03:59,084 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23095] due to args.save_total_limit\n",
            "{'loss': 0.4352, 'learning_rate': 6.183994996969942e-06, 'epoch': 747.1}\n",
            "                                             {'loss': 0.4568, 'learning_rate': 6.169157077388871e-06, 'epoch': 747.42}\n",
            " 75% 23180/31000 [6:50:37<1:16:52,  1.70it/s]{'loss': 0.4364, 'learning_rate': 6.154333474373355e-06, 'epoch': 747.74}\n",
            " 75% 23188/31000 [6:50:41<1:10:31,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:04:21,187 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:04:21,188 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:04:21,188 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.54it/s]\u001b[A\n",
            "                                             \n",
            " 75% 23188/31000 [6:50:46<1:10:31,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A{'eval_loss': 0.42735233902931213, 'eval_runtime': 3.8316, 'eval_samples_per_second': 57.156, 'eval_steps_per_second': 1.044, 'epoch': 748.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:04:25,021 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23188\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:04:25,023 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23188/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:04:26,997 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23188/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:04:26,998 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23188/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:04:31,495 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23126] due to args.save_total_limit\n",
            " 75% 23190/31000 [6:50:57<7:43:35,  3.56s/it] {'loss': 0.4477, 'learning_rate': 6.139524204792128e-06, 'epoch': 748.06}\n",
            " 75% 23200/31000 [6:51:03<1:27:10,  1.49it/s]{'loss': 0.4545, 'learning_rate': 6.124729285497636e-06, 'epoch': 748.39}\n",
            " 75% 23210/31000 [6:51:09<1:14:00,  1.75it/s]{'loss': 0.4348, 'learning_rate': 6.109948733325987e-06, 'epoch': 748.71}\n",
            " 75% 23219/31000 [6:51:14<1:10:16,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:04:53,652 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:04:53,652 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:04:53,652 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "{'eval_loss': 0.430806964635849, 'eval_runtime': 3.763, 'eval_samples_per_second': 58.198, 'eval_steps_per_second': 1.063, 'epoch': 749.0}\n",
            "\n",
            " 75% 23219/31000 [6:51:18<1:10:16,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:04:57,417 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23219\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:04:57,418 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23219/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:04:59,323 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23219/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:04:59,332 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23219/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:05:03,702 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23157] due to args.save_total_limit\n",
            "{'loss': 0.4383, 'learning_rate': 6.0951825650969385e-06, 'epoch': 749.03}\n",
            "{'loss': 0.4463, 'learning_rate': 6.080430797613888e-06, 'epoch': 749.35}\n",
            "{'loss': 0.4421, 'learning_rate': 6.065693447663822e-06, 'epoch': 749.68}\n",
            " 75% 23250/31000 [6:51:46<1:09:41,  1.85it/s]{'loss': 0.4371, 'learning_rate': 6.050970532017358e-06, 'epoch': 750.0}\n",
            " 75% 23250/31000 [6:51:46<1:09:41,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:05:25,647 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:05:25,647 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:05:25,647 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.58it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.43139949440956116, 'eval_runtime': 3.7886, 'eval_samples_per_second': 57.805, 'eval_steps_per_second': 1.056, 'epoch': 750.0}\n",
            " 75% 23250/31000 [6:51:50<1:09:41,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:05:29,437 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23250\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:05:29,440 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23250/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:05:31,325 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23250/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:05:31,326 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23250/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:05:35,639 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23188] due to args.save_total_limit\n",
            "{'loss': 0.4491, 'learning_rate': 6.0362620674286485e-06, 'epoch': 750.32}\n",
            "{'loss': 0.4486, 'learning_rate': 6.021568070635426e-06, 'epoch': 750.65}\n",
            "{'loss': 0.439, 'learning_rate': 6.006888558358954e-06, 'epoch': 750.97}\n",
            " 75% 23281/31000 [6:52:18<1:12:22,  1.78it/s][INFO|trainer.py:2625] 2022-05-18 23:05:57,680 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:05:57,680 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:05:57,680 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.42993584275245667, 'eval_runtime': 3.7766, 'eval_samples_per_second': 57.989, 'eval_steps_per_second': 1.059, 'epoch': 751.0}\n",
            " 75% 23281/31000 [6:52:22<1:12:22,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:06:01,458 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23281\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:06:01,460 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23281/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:06:03,323 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23281/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:06:03,323 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23281/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:06:07,678 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23219] due to args.save_total_limit\n",
            " 75% 23290/31000 [6:52:37<1:47:29,  1.20it/s]{'loss': 0.4311, 'learning_rate': 5.992223547304008e-06, 'epoch': 751.29}\n",
            "                                             {'loss': 0.4467, 'learning_rate': 5.977573054158873e-06, 'epoch': 751.61}\n",
            "                                             {'loss': 0.4423, 'learning_rate': 5.962937095595294e-06, 'epoch': 751.94}\n",
            " 75% 23312/31000 [6:52:50<1:10:54,  1.81it/s][INFO|trainer.py:2625] 2022-05-18 23:06:29,606 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:06:29,606 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:06:29,606 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "                                             \n",
            "{'eval_loss': 0.4329652488231659, 'eval_runtime': 3.8429, 'eval_samples_per_second': 56.989, 'eval_steps_per_second': 1.041, 'epoch': 752.0}\n",
            " 75% 23312/31000 [6:52:54<1:10:54,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:06:33,451 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23312\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:06:33,453 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23312/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:06:35,332 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23312/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:06:35,333 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23312/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:06:39,774 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23250] due to args.save_total_limit\n",
            " 75% 23320/31000 [6:53:11<2:14:20,  1.05s/it]{'loss': 0.4402, 'learning_rate': 5.948315688268492e-06, 'epoch': 752.26}\n",
            "{'loss': 0.4462, 'learning_rate': 5.9337088488171254e-06, 'epoch': 752.58}\n",
            "{'loss': 0.4408, 'learning_rate': 5.919116593863273e-06, 'epoch': 752.9}\n",
            " 75% 23343/31000 [6:53:24<1:09:53,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:07:03,997 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:07:03,997 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:07:03,997 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                             \n",
            " 75% 23343/31000 [6:53:28<1:09:53,  1.83it/s]{'eval_loss': 0.4329974055290222, 'eval_runtime': 3.836, 'eval_samples_per_second': 57.091, 'eval_steps_per_second': 1.043, 'epoch': 753.0}\n",
            "\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:07:07,835 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23343\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:07:07,836 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23343/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:07:09,776 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23343/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:07:09,777 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23343/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:07:14,130 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23281] due to args.save_total_limit\n",
            " 75% 23350/31000 [6:53:42<2:17:58,  1.08s/it]{'loss': 0.4461, 'learning_rate': 5.9045389400124185e-06, 'epoch': 753.23}\n",
            " 75% 23360/31000 [6:53:48<1:15:08,  1.69it/s]{'loss': 0.4431, 'learning_rate': 5.889975903853429e-06, 'epoch': 753.55}\n",
            "                                             {'loss': 0.4345, 'learning_rate': 5.875427501958542e-06, 'epoch': 753.87}\n",
            " 75% 23374/31000 [6:53:56<1:09:24,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:07:36,239 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:07:36,240 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:07:36,240 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                             \n",
            " 75% 23374/31000 [6:54:01<1:09:24,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A{'eval_loss': 0.43180617690086365, 'eval_runtime': 3.8841, 'eval_samples_per_second': 56.384, 'eval_steps_per_second': 1.03, 'epoch': 754.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:07:40,125 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23374\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:07:40,127 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23374/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:07:42,060 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23374/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:07:42,061 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23374/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:07:46,489 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23312] due to args.save_total_limit\n",
            "                                             {'loss': 0.4398, 'learning_rate': 5.860893750883328e-06, 'epoch': 754.19}\n",
            "{'loss': 0.4428, 'learning_rate': 5.846374667166701e-06, 'epoch': 754.52}\n",
            " 75% 23400/31000 [6:54:26<1:12:21,  1.75it/s]{'loss': 0.4401, 'learning_rate': 5.8318702673308755e-06, 'epoch': 754.84}\n",
            " 76% 23405/31000 [6:54:29<1:08:52,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:08:08,599 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:08:08,599 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:08:08,600 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                             \n",
            "{'eval_loss': 0.43311867117881775, 'eval_runtime': 3.7713, 'eval_samples_per_second': 58.069, 'eval_steps_per_second': 1.061, 'epoch': 755.0}\n",
            " 76% 23405/31000 [6:54:33<1:08:52,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:08:12,373 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23405\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:08:12,374 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23405/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:08:14,299 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23405/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:08:14,300 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23405/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:08:18,710 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23343] due to args.save_total_limit\n",
            " 76% 23410/31000 [6:54:46<3:23:33,  1.61s/it]{'loss': 0.4467, 'learning_rate': 5.817380567881361e-06, 'epoch': 755.16}\n",
            "{'loss': 0.4368, 'learning_rate': 5.802905585306939e-06, 'epoch': 755.48}\n",
            "{'loss': 0.4526, 'learning_rate': 5.788445336079626e-06, 'epoch': 755.81}\n",
            " 76% 23436/31000 [6:55:01<1:08:09,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:08:40,662 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:08:40,662 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:08:40,662 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                             \n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A{'eval_loss': 0.43134748935699463, 'eval_runtime': 3.7213, 'eval_samples_per_second': 58.85, 'eval_steps_per_second': 1.075, 'epoch': 756.0}\n",
            " 76% 23436/31000 [6:55:05<1:08:09,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:08:44,385 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23436\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:08:44,387 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23436/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:08:46,257 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23436/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:08:46,258 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23436/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:08:50,495 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23374] due to args.save_total_limit\n",
            "{'loss': 0.4432, 'learning_rate': 5.7739998366547064e-06, 'epoch': 756.13}\n",
            " 76% 23450/31000 [6:55:23<1:19:27,  1.58it/s]{'loss': 0.4528, 'learning_rate': 5.759569103470649e-06, 'epoch': 756.45}\n",
            "                                             {'loss': 0.443, 'learning_rate': 5.745153152949137e-06, 'epoch': 756.77}\n",
            " 76% 23467/31000 [6:55:33<1:08:22,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:09:12,522 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:09:12,522 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:09:12,522 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.81it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.43457457423210144, 'eval_runtime': 3.7754, 'eval_samples_per_second': 58.008, 'eval_steps_per_second': 1.06, 'epoch': 757.0}\n",
            " 76% 23467/31000 [6:55:37<1:08:22,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:09:16,299 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23467\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:09:16,301 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23467/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:09:18,204 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23467/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:09:18,205 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23467/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:09:22,596 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23405] due to args.save_total_limit\n",
            "                                             {'loss': 0.432, 'learning_rate': 5.730752001495011e-06, 'epoch': 757.1}\n",
            " 76% 23480/31000 [6:55:54<1:18:23,  1.60it/s]{'loss': 0.4394, 'learning_rate': 5.716365665496301e-06, 'epoch': 757.42}\n",
            " 76% 23490/31000 [6:56:01<1:12:29,  1.73it/s]{'loss': 0.4449, 'learning_rate': 5.7019941613241605e-06, 'epoch': 757.74}\n",
            " 76% 23498/31000 [6:56:05<1:07:30,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:09:44,527 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:09:44,527 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:09:44,527 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                             \n",
            " 76% 23498/31000 [6:56:09<1:07:30,  1.85it/s]\n",
            "{'eval_loss': 0.43533793091773987, 'eval_runtime': 3.7486, 'eval_samples_per_second': 58.421, 'eval_steps_per_second': 1.067, 'epoch': 758.0}\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:09:48,278 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23498\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:09:48,279 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23498/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:09:50,193 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23498/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:09:50,194 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23498/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:09:54,621 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23436] due to args.save_total_limit\n",
            "                                             {'loss': 0.4343, 'learning_rate': 5.6876375053328555e-06, 'epoch': 758.06}\n",
            " 76% 23510/31000 [6:56:27<1:23:10,  1.50it/s]{'loss': 0.4417, 'learning_rate': 5.67329571385977e-06, 'epoch': 758.39}\n",
            "                                             {'loss': 0.4393, 'learning_rate': 5.65896880322537e-06, 'epoch': 758.71}\n",
            " 76% 23529/31000 [6:56:37<1:07:29,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:10:16,674 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:10:16,674 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:10:16,674 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                             \n",
            " 76% 23529/31000 [6:56:41<1:07:29,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "{'eval_loss': 0.4257277250289917, 'eval_runtime': 3.7616, 'eval_samples_per_second': 58.219, 'eval_steps_per_second': 1.063, 'epoch': 759.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:10:20,437 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23529\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:10:20,439 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23529/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:10:22,336 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23529/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:10:22,337 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23529/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:10:26,635 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23467] due to args.save_total_limit\n",
            "{'loss': 0.4588, 'learning_rate': 5.644656789733194e-06, 'epoch': 759.03}\n",
            " 76% 23540/31000 [6:56:58<1:28:00,  1.41it/s]{'loss': 0.4518, 'learning_rate': 5.6303596896698024e-06, 'epoch': 759.35}\n",
            " 76% 23550/31000 [6:57:04<1:10:58,  1.75it/s]{'loss': 0.4379, 'learning_rate': 5.616077519304806e-06, 'epoch': 759.68}\n",
            "{'loss': 0.4408, 'learning_rate': 5.601810294890839e-06, 'epoch': 760.0}\n",
            " 76% 23560/31000 [6:57:09<1:07:07,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:10:48,599 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:10:48,600 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:10:48,600 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                             \n",
            " 76% 23560/31000 [6:57:13<1:07:07,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A{'eval_loss': 0.4261641800403595, 'eval_runtime': 3.7696, 'eval_samples_per_second': 58.097, 'eval_steps_per_second': 1.061, 'epoch': 760.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:10:52,371 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23560\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:10:52,373 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23560/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:10:54,275 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23560/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:10:54,276 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23560/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:10:58,640 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23498] due to args.save_total_limit\n",
            "{'loss': 0.4334, 'learning_rate': 5.5875580326634935e-06, 'epoch': 760.32}\n",
            "                                             {'loss': 0.4523, 'learning_rate': 5.573320748841359e-06, 'epoch': 760.65}\n",
            "{'loss': 0.4417, 'learning_rate': 5.559098459625972e-06, 'epoch': 760.97}\n",
            " 76% 23591/31000 [6:57:41<1:09:11,  1.78it/s][INFO|trainer.py:2625] 2022-05-18 23:11:20,666 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:11:20,666 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:11:20,666 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                             \n",
            " 76% 23591/31000 [6:57:45<1:09:11,  1.78it/s]\n",
            "{'eval_loss': 0.4340049922466278, 'eval_runtime': 3.8299, 'eval_samples_per_second': 57.182, 'eval_steps_per_second': 1.044, 'epoch': 761.0}\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:11:24,498 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23591\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:11:24,500 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23591/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:11:26,398 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23591/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:11:26,400 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23591/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:11:30,767 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23529] due to args.save_total_limit\n",
            " 76% 23600/31000 [6:58:01<1:44:29,  1.18it/s]{'loss': 0.4479, 'learning_rate': 5.544891181201816e-06, 'epoch': 761.29}\n",
            " 76% 23610/31000 [6:58:06<1:12:09,  1.71it/s]{'loss': 0.4487, 'learning_rate': 5.530698929736271e-06, 'epoch': 761.61}\n",
            " 76% 23620/31000 [6:58:12<1:07:14,  1.83it/s]{'loss': 0.4338, 'learning_rate': 5.516521721379634e-06, 'epoch': 761.94}\n",
            " 76% 23622/31000 [6:58:13<1:08:26,  1.80it/s][INFO|trainer.py:2625] 2022-05-18 23:11:52,757 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:11:52,757 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:11:52,757 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                             \n",
            "{'eval_loss': 0.43022069334983826, 'eval_runtime': 3.7681, 'eval_samples_per_second': 58.119, 'eval_steps_per_second': 1.062, 'epoch': 762.0}\n",
            " 76% 23622/31000 [6:58:17<1:08:26,  1.80it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:11:56,527 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23622\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:11:56,528 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23622/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:11:58,395 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23622/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:11:58,396 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23622/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:12:02,885 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23560] due to args.save_total_limit\n",
            "                                             {'loss': 0.4384, 'learning_rate': 5.502359572265082e-06, 'epoch': 762.26}\n",
            " 76% 23640/31000 [6:58:38<1:11:17,  1.72it/s]{'loss': 0.4422, 'learning_rate': 5.48821249850865e-06, 'epoch': 762.58}\n",
            "                                             {'loss': 0.4483, 'learning_rate': 5.474080516209229e-06, 'epoch': 762.9}\n",
            " 76% 23653/31000 [6:58:45<1:07:32,  1.81it/s][INFO|trainer.py:2625] 2022-05-18 23:12:24,858 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:12:24,858 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:12:24,858 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.26it/s]\u001b[A\n",
            "                                             \n",
            "                                 {'eval_loss': 0.4263707995414734, 'eval_runtime': 3.7655, 'eval_samples_per_second': 58.16, 'eval_steps_per_second': 1.062, 'epoch': 763.0}\n",
            " 76% 23653/31000 [6:58:49<1:07:32,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.71it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:12:28,625 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23653\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:12:28,627 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23653/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:12:30,564 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23653/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:12:30,566 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23653/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:12:34,842 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23591] due to args.save_total_limit\n",
            "                                             {'loss': 0.4404, 'learning_rate': 5.459963641448507e-06, 'epoch': 763.23}\n",
            " 76% 23670/31000 [6:59:09<1:11:48,  1.70it/s]{'loss': 0.4423, 'learning_rate': 5.445861890291026e-06, 'epoch': 763.55}\n",
            "                                             {'loss': 0.4453, 'learning_rate': 5.431775278784076e-06, 'epoch': 763.87}\n",
            " 76% 23684/31000 [6:59:17<1:06:28,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:12:56,925 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:12:56,925 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:12:56,925 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.59it/s]\u001b[A\n",
            "                                             \n",
            " 76% 23684/31000 [6:59:21<1:06:28,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "{'eval_loss': 0.43219423294067383, 'eval_runtime': 3.7732, 'eval_samples_per_second': 58.041, 'eval_steps_per_second': 1.06, 'epoch': 764.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:13:00,700 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23684\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:13:00,702 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23684/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:13:02,607 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23684/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:13:02,608 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23684/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:13:06,934 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23622] due to args.save_total_limit\n",
            "                                             {'loss': 0.4394, 'learning_rate': 5.417703822957747e-06, 'epoch': 764.19}\n",
            "{'loss': 0.4495, 'learning_rate': 5.403647538824851e-06, 'epoch': 764.52}\n",
            "                                             {'loss': 0.4389, 'learning_rate': 5.389606442380972e-06, 'epoch': 764.84}\n",
            " 76% 23715/31000 [6:59:49<1:05:54,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:13:28,720 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:13:28,720 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:13:28,720 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.56it/s]\u001b[A\n",
            "                                             \n",
            " 76% 23715/31000 [6:59:53<1:05:54,  1.84it/s]\n",
            "{'eval_loss': 0.43480977416038513, 'eval_runtime': 3.8744, 'eval_samples_per_second': 56.524, 'eval_steps_per_second': 1.032, 'epoch': 765.0}\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:13:32,596 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23715\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:13:32,598 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23715/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:13:34,488 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23715/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:13:34,489 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23715/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:13:38,940 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23653] due to args.save_total_limit\n",
            "{'loss': 0.4372, 'learning_rate': 5.375580549604397e-06, 'epoch': 765.16}\n",
            "                                             {'loss': 0.4517, 'learning_rate': 5.361569876456094e-06, 'epoch': 765.48}\n",
            "                                             {'loss': 0.4409, 'learning_rate': 5.347574438879733e-06, 'epoch': 765.81}\n",
            " 77% 23746/31000 [7:00:21<1:05:47,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:14:01,064 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:14:01,064 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:14:01,064 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.55it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.42888903617858887, 'eval_runtime': 3.84, 'eval_samples_per_second': 57.031, 'eval_steps_per_second': 1.042, 'epoch': 766.0}\n",
            " 77% 23746/31000 [7:00:26<1:05:47,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:14:04,905 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23746\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:14:04,907 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23746/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:14:06,858 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23746/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:14:06,859 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23746/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:14:11,634 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23684] due to args.save_total_limit\n",
            "{'loss': 0.4499, 'learning_rate': 5.333594252801638e-06, 'epoch': 766.13}\n",
            " 77% 23760/31000 [7:00:45<1:16:18,  1.58it/s]{'loss': 0.4422, 'learning_rate': 5.3196293341307876e-06, 'epoch': 766.45}\n",
            " 77% 23770/31000 [7:00:51<1:10:56,  1.70it/s]{'loss': 0.4372, 'learning_rate': 5.305679698758764e-06, 'epoch': 766.77}\n",
            " 77% 23777/31000 [7:00:54<1:05:46,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:14:33,850 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:14:33,850 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:14:33,850 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.42439550161361694, 'eval_runtime': 3.7739, 'eval_samples_per_second': 58.03, 'eval_steps_per_second': 1.06, 'epoch': 767.0}\n",
            " 77% 23777/31000 [7:00:58<1:05:46,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:14:37,625 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23777\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:14:37,627 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23777/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:14:39,548 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23777/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:14:39,549 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23777/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:14:43,855 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23715] due to args.save_total_limit\n",
            "                                             {'loss': 0.438, 'learning_rate': 5.291745362559771e-06, 'epoch': 767.1}\n",
            "                                             {'loss': 0.4351, 'learning_rate': 5.277826341390622e-06, 'epoch': 767.42}\n",
            " 77% 23800/31000 [7:01:22<1:09:39,  1.72it/s]{'loss': 0.4427, 'learning_rate': 5.263922651090666e-06, 'epoch': 767.74}\n",
            " 77% 23808/31000 [7:01:26<1:04:48,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:15:05,946 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:15:05,946 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:15:05,946 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                             \n",
            " 77% 23808/31000 [7:01:30<1:04:48,  1.85it/s]{'eval_loss': 0.4343336224555969, 'eval_runtime': 3.8365, 'eval_samples_per_second': 57.083, 'eval_steps_per_second': 1.043, 'epoch': 768.0}\n",
            "\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:15:09,785 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23808\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:15:09,786 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23808/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:15:11,728 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23808/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:15:11,729 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23808/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:15:16,034 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23746] due to args.save_total_limit\n",
            " 77% 23810/31000 [7:01:41<6:44:33,  3.38s/it]{'loss': 0.4461, 'learning_rate': 5.250034307481826e-06, 'epoch': 768.06}\n",
            "{'loss': 0.44, 'learning_rate': 5.2361613263685614e-06, 'epoch': 768.39}\n",
            "                                             {'loss': 0.4445, 'learning_rate': 5.222303723537851e-06, 'epoch': 768.71}\n",
            " 77% 23839/31000 [7:01:58<1:04:28,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:15:38,235 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:15:38,235 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:15:38,235 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.48it/s]\u001b[A\n",
            "                                             \n",
            " 77% 23839/31000 [7:02:03<1:04:28,  1.85it/s]\n",
            "{'eval_loss': 0.4224601984024048, 'eval_runtime': 3.9315, 'eval_samples_per_second': 55.704, 'eval_steps_per_second': 1.017, 'epoch': 769.0}\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:15:42,169 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23839\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:15:42,170 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23839/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:15:44,116 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23839/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:15:44,117 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23839/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:15:48,668 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23777] due to args.save_total_limit\n",
            "{'loss': 0.4501, 'learning_rate': 5.2084615147591585e-06, 'epoch': 769.03}\n",
            "{'loss': 0.4437, 'learning_rate': 5.194634715784443e-06, 'epoch': 769.35}\n",
            "{'loss': 0.4323, 'learning_rate': 5.18082334234813e-06, 'epoch': 769.68}\n",
            "{'loss': 0.4471, 'learning_rate': 5.1670274101670814e-06, 'epoch': 770.0}\n",
            " 77% 23870/31000 [7:02:31<1:04:04,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:16:10,697 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:16:10,697 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:16:10,697 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.42it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.4372307062149048, 'eval_runtime': 3.7792, 'eval_samples_per_second': 57.95, 'eval_steps_per_second': 1.058, 'epoch': 770.0}\n",
            " 77% 23870/31000 [7:02:35<1:04:04,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:16:14,478 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23870\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:16:14,479 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23870/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:16:16,398 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23870/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:16:16,399 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23870/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:16:20,847 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23808] due to args.save_total_limit\n",
            "                                             {'loss': 0.4227, 'learning_rate': 5.153246934940603e-06, 'epoch': 770.32}\n",
            " 77% 23890/31000 [7:02:57<1:09:09,  1.71it/s]{'loss': 0.4462, 'learning_rate': 5.139481932350378e-06, 'epoch': 770.65}\n",
            "                                             {'loss': 0.4576, 'learning_rate': 5.125732418060533e-06, 'epoch': 770.97}\n",
            " 77% 23901/31000 [7:03:03<1:07:01,  1.77it/s][INFO|trainer.py:2625] 2022-05-18 23:16:42,937 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:16:42,937 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:16:42,937 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "                                             \n",
            " 77% 23901/31000 [7:03:07<1:07:01,  1.77it/s]\n",
            "                                 {'eval_loss': 0.43135887384414673, 'eval_runtime': 3.7829, 'eval_samples_per_second': 57.891, 'eval_steps_per_second': 1.057, 'epoch': 771.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-18 23:16:46,721 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23901\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:16:46,723 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23901/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:16:48,628 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23901/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:16:48,630 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23901/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:16:52,958 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23839] due to args.save_total_limit\n",
            " 77% 23910/31000 [7:03:23<1:41:03,  1.17it/s]{'loss': 0.4342, 'learning_rate': 5.111998407717521e-06, 'epoch': 771.29}\n",
            "                                             {'loss': 0.4392, 'learning_rate': 5.098279916950178e-06, 'epoch': 771.61}\n",
            "                                             {'loss': 0.448, 'learning_rate': 5.084576961369676e-06, 'epoch': 771.94}\n",
            " 77% 23932/31000 [7:03:35<1:05:19,  1.80it/s][INFO|trainer.py:2625] 2022-05-18 23:17:15,076 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:17:15,076 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:17:15,076 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.42294520139694214, 'eval_runtime': 3.8292, 'eval_samples_per_second': 57.192, 'eval_steps_per_second': 1.045, 'epoch': 772.0}\n",
            " 77% 23932/31000 [7:03:40<1:05:19,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:17:18,907 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23932\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:17:18,909 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23932/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:17:20,811 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23932/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:17:20,812 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23932/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:17:25,370 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23870] due to args.save_total_limit\n",
            "{'loss': 0.4457, 'learning_rate': 5.0708895565695024e-06, 'epoch': 772.26}\n",
            "{'loss': 0.4464, 'learning_rate': 5.0572177181254606e-06, 'epoch': 772.58}\n",
            "                                             {'loss': 0.4374, 'learning_rate': 5.043561461595618e-06, 'epoch': 772.9}\n",
            " 77% 23963/31000 [7:04:08<1:04:42,  1.81it/s][INFO|trainer.py:2625] 2022-05-18 23:17:47,440 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:17:47,440 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:17:47,440 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                             \n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A{'eval_loss': 0.42786920070648193, 'eval_runtime': 3.8485, 'eval_samples_per_second': 56.906, 'eval_steps_per_second': 1.039, 'epoch': 773.0}\n",
            " 77% 23963/31000 [7:04:12<1:04:42,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:17:51,290 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23963\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:17:51,292 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23963/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:17:53,228 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23963/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:17:53,229 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23963/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:17:58,087 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23901] due to args.save_total_limit\n",
            " 77% 23970/31000 [7:04:27<2:08:00,  1.09s/it]{'loss': 0.4295, 'learning_rate': 5.029920802520331e-06, 'epoch': 773.23}\n",
            " 77% 23980/31000 [7:04:33<1:11:59,  1.63it/s]{'loss': 0.4504, 'learning_rate': 5.0162957564222e-06, 'epoch': 773.55}\n",
            "                                             {'loss': 0.4468, 'learning_rate': 5.002686338806057e-06, 'epoch': 773.87}\n",
            " 77% 23994/31000 [7:04:41<1:03:57,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:18:20,327 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:18:20,327 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:18:20,327 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                             \n",
            " 77% 23994/31000 [7:04:45<1:03:57,  1.83it/s]\n",
            "{'eval_loss': 0.4332658648490906, 'eval_runtime': 3.7867, 'eval_samples_per_second': 57.834, 'eval_steps_per_second': 1.056, 'epoch': 774.0}\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:18:24,115 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23994\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:18:24,117 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23994/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:18:26,020 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23994/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:18:26,021 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23994/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:18:30,384 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23932] due to args.save_total_limit\n",
            "                                             {'loss': 0.4508, 'learning_rate': 4.989092565158954e-06, 'epoch': 774.19}\n",
            " 77% 24010/31000 [7:05:04<1:09:41,  1.67it/s]{'loss': 0.4411, 'learning_rate': 4.9755144509501225e-06, 'epoch': 774.52}\n",
            "                                             {'loss': 0.4491, 'learning_rate': 4.9619520116310105e-06, 'epoch': 774.84}\n",
            " 78% 24025/31000 [7:05:13<1:03:12,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:18:52,414 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:18:52,414 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:18:52,414 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                             \n",
            " 78% 24025/31000 [7:05:17<1:03:12,  1.84it/s]{'eval_loss': 0.42876410484313965, 'eval_runtime': 3.77, 'eval_samples_per_second': 58.09, 'eval_steps_per_second': 1.061, 'epoch': 775.0}\n",
            "\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:18:56,186 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24025\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:18:56,188 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24025/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:18:58,086 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24025/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:18:58,087 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24025/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:19:02,692 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23963] due to args.save_total_limit\n",
            " 78% 24030/31000 [7:05:30<3:07:16,  1.61s/it]{'loss': 0.4248, 'learning_rate': 4.948405262635187e-06, 'epoch': 775.16}\n",
            "{'loss': 0.4424, 'learning_rate': 4.934874219378394e-06, 'epoch': 775.48}\n",
            " 78% 24050/31000 [7:05:41<1:05:45,  1.76it/s]{'loss': 0.4422, 'learning_rate': 4.921358897258491e-06, 'epoch': 775.81}\n",
            " 78% 24056/31000 [7:05:45<1:02:39,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:19:24,492 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:19:24,492 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:19:24,493 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.83it/s]\u001b[A\n",
            "                                             \n",
            " 78% 24056/31000 [7:05:49<1:02:39,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.96it/s]\u001b[A{'eval_loss': 0.4309999644756317, 'eval_runtime': 3.812, 'eval_samples_per_second': 57.451, 'eval_steps_per_second': 1.049, 'epoch': 776.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:19:28,306 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24056\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:19:28,308 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24056/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:19:30,235 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24056/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:19:30,236 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24056/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:19:34,647 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-23994] due to args.save_total_limit\n",
            "{'loss': 0.4387, 'learning_rate': 4.907859311655446e-06, 'epoch': 776.13}\n",
            "{'loss': 0.442, 'learning_rate': 4.894375477931334e-06, 'epoch': 776.45}\n",
            "                                             {'loss': 0.4408, 'learning_rate': 4.880907411430276e-06, 'epoch': 776.77}\n",
            " 78% 24087/31000 [7:06:19<1:01:58,  1.86it/s][INFO|trainer.py:2625] 2022-05-18 23:19:58,956 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:19:58,956 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:19:58,956 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "{'eval_loss': 0.4260128140449524, 'eval_runtime': 3.7935, 'eval_samples_per_second': 57.731, 'eval_steps_per_second': 1.054, 'epoch': 777.0}\n",
            "\n",
            " 78% 24087/31000 [7:06:23<1:01:58,  1.86it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:20:02,751 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24087\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:20:02,752 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24087/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:20:04,673 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24087/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:20:04,674 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24087/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:20:09,432 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24025] due to args.save_total_limit\n",
            " 78% 24090/31000 [7:06:36<5:07:22,  2.67s/it]{'loss': 0.4427, 'learning_rate': 4.867455127478477e-06, 'epoch': 777.1}\n",
            " 78% 24100/31000 [7:06:42<1:12:06,  1.59it/s]{'loss': 0.438, 'learning_rate': 4.854018641384169e-06, 'epoch': 777.42}\n",
            "{'loss': 0.4565, 'learning_rate': 4.840597968437617e-06, 'epoch': 777.74}\n",
            " 78% 24118/31000 [7:06:51<1:02:08,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:20:31,294 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:20:31,295 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:20:31,295 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.4305524528026581, 'eval_runtime': 3.8572, 'eval_samples_per_second': 56.776, 'eval_steps_per_second': 1.037, 'epoch': 778.0}\n",
            " 78% 24118/31000 [7:06:56<1:02:08,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:20:35,154 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24118\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:20:35,156 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24118/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:20:37,092 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24118/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:20:37,093 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24118/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:20:42,185 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24056] due to args.save_total_limit\n",
            " 78% 24120/31000 [7:07:07<7:00:30,  3.67s/it]{'loss': 0.4341, 'learning_rate': 4.827193123911069e-06, 'epoch': 778.06}\n",
            " 78% 24130/31000 [7:07:13<1:16:11,  1.50it/s]{'loss': 0.4448, 'learning_rate': 4.813804123058789e-06, 'epoch': 778.39}\n",
            "{'loss': 0.4517, 'learning_rate': 4.800430981117003e-06, 'epoch': 778.71}\n",
            " 78% 24149/31000 [7:07:24<1:01:49,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:21:04,159 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:21:04,159 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:21:04,159 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.43543148040771484, 'eval_runtime': 3.7089, 'eval_samples_per_second': 59.048, 'eval_steps_per_second': 1.078, 'epoch': 779.0}\n",
            " 78% 24149/31000 [7:07:28<1:01:49,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:21:07,870 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24149\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:21:07,872 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24149/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:21:09,732 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24149/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:21:09,733 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24149/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:21:14,614 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24087] due to args.save_total_limit\n",
            "{'loss': 0.4291, 'learning_rate': 4.787073713303869e-06, 'epoch': 779.03}\n",
            " 78% 24160/31000 [7:07:46<1:21:34,  1.40it/s]{'loss': 0.4367, 'learning_rate': 4.773732334819507e-06, 'epoch': 779.35}\n",
            " 78% 24170/31000 [7:07:51<1:04:51,  1.76it/s]{'loss': 0.447, 'learning_rate': 4.7604068608459454e-06, 'epoch': 779.68}\n",
            "{'loss': 0.4427, 'learning_rate': 4.74709730654712e-06, 'epoch': 780.0}\n",
            " 78% 24180/31000 [7:07:57<1:01:32,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:21:36,703 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:21:36,703 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:21:36,703 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "                                             \n",
            "{'eval_loss': 0.4256061911582947, 'eval_runtime': 3.7934, 'eval_samples_per_second': 57.733, 'eval_steps_per_second': 1.054, 'epoch': 780.0}\n",
            " 78% 24180/31000 [7:08:01<1:01:32,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:21:40,498 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24180\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:21:40,500 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24180/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:21:42,410 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24180/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:21:42,411 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24180/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:21:47,375 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24118] due to args.save_total_limit\n",
            "                                             {'loss': 0.4507, 'learning_rate': 4.7338036870688306e-06, 'epoch': 780.32}\n",
            "                                             {'loss': 0.4364, 'learning_rate': 4.720526017538762e-06, 'epoch': 780.65}\n",
            "                                             {'loss': 0.4376, 'learning_rate': 4.707264313066457e-06, 'epoch': 780.97}\n",
            " 78% 24211/31000 [7:08:29<1:03:40,  1.78it/s][INFO|trainer.py:2625] 2022-05-18 23:22:09,292 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:22:09,292 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:22:09,292 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                             \n",
            "{'eval_loss': 0.4367230534553528, 'eval_runtime': 3.917, 'eval_samples_per_second': 55.91, 'eval_steps_per_second': 1.021, 'epoch': 781.0}\n",
            " 78% 24211/31000 [7:08:34<1:03:40,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:22:13,211 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24211\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:22:13,213 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24211/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:22:15,074 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24211/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:22:15,075 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24211/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:22:19,905 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24149] due to args.save_total_limit\n",
            " 78% 24220/31000 [7:08:50<1:36:19,  1.17it/s]{'loss': 0.4338, 'learning_rate': 4.694018588743264e-06, 'epoch': 781.29}\n",
            " 78% 24230/31000 [7:08:56<1:05:50,  1.71it/s]{'loss': 0.4498, 'learning_rate': 4.680788859642367e-06, 'epoch': 781.61}\n",
            "{'loss': 0.4357, 'learning_rate': 4.6675751408187395e-06, 'epoch': 781.94}\n",
            " 78% 24242/31000 [7:09:02<1:02:22,  1.81it/s][INFO|trainer.py:2625] 2022-05-18 23:22:41,750 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:22:41,751 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:22:41,751 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                             \n",
            "\u001b[A{'eval_loss': 0.4246254861354828, 'eval_runtime': 3.7331, 'eval_samples_per_second': 58.665, 'eval_steps_per_second': 1.072, 'epoch': 782.0}\n",
            " 78% 24242/31000 [7:09:06<1:02:22,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.97it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:22:45,486 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24242\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:22:45,487 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24242/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:22:47,336 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24242/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:22:47,337 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24242/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:22:52,464 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24180] due to args.save_total_limit\n",
            " 78% 24250/31000 [7:09:21<1:46:03,  1.06it/s]{'loss': 0.4413, 'learning_rate': 4.654377447309144e-06, 'epoch': 782.26}\n",
            " 78% 24260/31000 [7:09:28<1:07:01,  1.68it/s]{'loss': 0.4396, 'learning_rate': 4.641195794132089e-06, 'epoch': 782.58}\n",
            " 78% 24270/31000 [7:09:33<1:03:18,  1.77it/s]{'loss': 0.4536, 'learning_rate': 4.628030196287851e-06, 'epoch': 782.9}\n",
            " 78% 24273/31000 [7:09:35<1:02:10,  1.80it/s][INFO|trainer.py:2625] 2022-05-18 23:23:14,625 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:23:14,625 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:23:14,625 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "{'eval_loss': 0.4280368983745575, 'eval_runtime': 3.7745, 'eval_samples_per_second': 58.021, 'eval_steps_per_second': 1.06, 'epoch': 783.0}\n",
            "\n",
            " 78% 24273/31000 [7:09:39<1:02:10,  1.80it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:23:18,401 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24273\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:23:18,403 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24273/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:23:20,313 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24273/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:23:20,314 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24273/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:23:25,350 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24211] due to args.save_total_limit\n",
            " 78% 24280/31000 [7:09:54<2:03:01,  1.10s/it]{'loss': 0.4359, 'learning_rate': 4.614880668758422e-06, 'epoch': 783.23}\n",
            "{'loss': 0.4565, 'learning_rate': 4.601747226507512e-06, 'epoch': 783.55}\n",
            " 78% 24300/31000 [7:10:05<1:02:48,  1.78it/s]{'loss': 0.4364, 'learning_rate': 4.588629884480532e-06, 'epoch': 783.87}\n",
            " 78% 24304/31000 [7:10:07<1:00:49,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:23:47,266 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:23:47,266 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:23:47,267 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                             \n",
            " 78% 24304/31000 [7:10:12<1:00:49,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "{'eval_loss': 0.42439496517181396, 'eval_runtime': 3.8053, 'eval_samples_per_second': 57.552, 'eval_steps_per_second': 1.051, 'epoch': 784.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:23:51,073 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24304\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:23:51,075 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24304/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:23:52,984 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24304/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:23:52,985 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24304/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:23:57,858 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24242] due to args.save_total_limit\n",
            "                                             {'loss': 0.4273, 'learning_rate': 4.575528657604548e-06, 'epoch': 784.19}\n",
            "                                             {'loss': 0.4486, 'learning_rate': 4.562443560788328e-06, 'epoch': 784.52}\n",
            "{'loss': 0.454, 'learning_rate': 4.549374608922243e-06, 'epoch': 784.84}\n",
            " 78% 24335/31000 [7:10:40<1:00:22,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:24:19,836 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:24:19,836 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:24:19,836 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.41it/s]\u001b[A\n",
            "                                             \n",
            " 78% 24335/31000 [7:10:44<1:00:22,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.4356975257396698, 'eval_runtime': 3.7833, 'eval_samples_per_second': 57.886, 'eval_steps_per_second': 1.057, 'epoch': 785.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:24:23,621 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24335\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:24:23,623 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24335/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:24:25,508 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24335/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:24:25,510 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24335/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:24:30,576 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24273] due to args.save_total_limit\n",
            "                                             {'loss': 0.4452, 'learning_rate': 4.536321816878314e-06, 'epoch': 785.16}\n",
            "                                             {'loss': 0.4401, 'learning_rate': 4.523285199510171e-06, 'epoch': 785.48}\n",
            " 79% 24360/31000 [7:11:09<1:03:13,  1.75it/s]{'loss': 0.4327, 'learning_rate': 4.510264771653032e-06, 'epoch': 785.81}\n",
            " 79% 24366/31000 [7:11:13<59:54,  1.85it/s]  [INFO|trainer.py:2625] 2022-05-18 23:24:52,519 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:24:52,519 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:24:52,520 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.82it/s]\u001b[A\n",
            "                                           \n",
            " 79% 24366/31000 [7:11:17<59:54,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.4303279221057892, 'eval_runtime': 3.7578, 'eval_samples_per_second': 58.278, 'eval_steps_per_second': 1.064, 'epoch': 786.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:24:56,280 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24366\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:24:56,281 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24366/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:24:58,256 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24366/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:24:58,258 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24366/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:25:03,121 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24304] due to args.save_total_limit\n",
            " 79% 24370/31000 [7:11:32<4:13:47,  2.30s/it]{'loss': 0.4476, 'learning_rate': 4.497260548123704e-06, 'epoch': 786.13}\n",
            "                                             {'loss': 0.4421, 'learning_rate': 4.484272543720532e-06, 'epoch': 786.45}\n",
            "{'loss': 0.432, 'learning_rate': 4.4713007732234244e-06, 'epoch': 786.77}\n",
            " 79% 24397/31000 [7:11:48<59:49,  1.84it/s]  [INFO|trainer.py:2625] 2022-05-18 23:25:27,462 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:25:27,462 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:25:27,462 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                           \n",
            " 79% 24397/31000 [7:11:52<59:49,  1.84it/s]{'eval_loss': 0.4335451126098633, 'eval_runtime': 3.791, 'eval_samples_per_second': 57.768, 'eval_steps_per_second': 1.055, 'epoch': 787.0}\n",
            "\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:25:31,254 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24397\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:25:31,256 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24397/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:25:33,149 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24397/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:25:33,150 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24397/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:25:38,002 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24335] due to args.save_total_limit\n",
            " 79% 24400/31000 [7:12:04<4:54:45,  2.68s/it]{'loss': 0.4394, 'learning_rate': 4.4583452513938086e-06, 'epoch': 787.1}\n",
            "                                             {'loss': 0.4322, 'learning_rate': 4.445405992974619e-06, 'epoch': 787.42}\n",
            "{'loss': 0.4452, 'learning_rate': 4.432483012690294e-06, 'epoch': 787.74}\n",
            " 79% 24428/31000 [7:12:20<59:22,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:26:00,034 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:26:00,034 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:26:00,034 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "                                           \n",
            " 79% 24428/31000 [7:12:24<59:22,  1.85it/s]{'eval_loss': 0.42784014344215393, 'eval_runtime': 3.7839, 'eval_samples_per_second': 57.876, 'eval_steps_per_second': 1.057, 'epoch': 788.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:26:03,820 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24428\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:26:03,821 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24428/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:26:05,735 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24428/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:26:05,736 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24428/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:26:10,516 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24366] due to args.save_total_limit\n",
            " 79% 24430/31000 [7:12:36<6:34:51,  3.61s/it]{'loss': 0.4473, 'learning_rate': 4.419576325246722e-06, 'epoch': 788.06}\n",
            "{'loss': 0.4393, 'learning_rate': 4.406685945331292e-06, 'epoch': 788.39}\n",
            " 79% 24450/31000 [7:12:48<1:04:10,  1.70it/s]{'loss': 0.4406, 'learning_rate': 4.393811887612793e-06, 'epoch': 788.71}\n",
            " 79% 24459/31000 [7:12:53<59:05,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:26:32,788 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:26:32,789 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:26:32,789 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                           \n",
            " 79% 24459/31000 [7:12:57<59:05,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]{'eval_loss': 0.421009361743927, 'eval_runtime': 3.7221, 'eval_samples_per_second': 58.838, 'eval_steps_per_second': 1.075, 'epoch': 789.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:26:36,512 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24459\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:26:36,514 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24459/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:26:38,439 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24459/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:26:38,440 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24459/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:26:43,375 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24397] due to args.save_total_limit\n",
            "                                             {'loss': 0.4483, 'learning_rate': 4.380954166741466e-06, 'epoch': 789.03}\n",
            "{'loss': 0.4407, 'learning_rate': 4.368112797348953e-06, 'epoch': 789.35}\n",
            " 79% 24480/31000 [7:13:20<1:02:25,  1.74it/s]{'loss': 0.4407, 'learning_rate': 4.355287794048291e-06, 'epoch': 789.68}\n",
            " 79% 24490/31000 [7:13:26<58:36,  1.85it/s]{'loss': 0.4498, 'learning_rate': 4.342479171433897e-06, 'epoch': 790.0}\n",
            " 79% 24490/31000 [7:13:26<58:36,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:27:05,435 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:27:05,435 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:27:05,435 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                           \n",
            " 79% 24490/31000 [7:13:30<58:36,  1.85it/s]\n",
            "                                 {'eval_loss': 0.4333195686340332, 'eval_runtime': 3.7441, 'eval_samples_per_second': 58.493, 'eval_steps_per_second': 1.068, 'epoch': 790.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-18 23:27:09,181 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24490\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:27:09,183 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24490/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:27:11,092 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24490/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:27:11,094 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24490/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:27:15,965 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24428] due to args.save_total_limit\n",
            "                                             {'loss': 0.4386, 'learning_rate': 4.32968694408153e-06, 'epoch': 790.32}\n",
            " 79% 24510/31000 [7:13:52<1:03:19,  1.71it/s]{'loss': 0.4423, 'learning_rate': 4.31691112654831e-06, 'epoch': 790.65}\n",
            " 79% 24520/31000 [7:13:57<58:32,  1.84it/s]{'loss': 0.4434, 'learning_rate': 4.304151733372676e-06, 'epoch': 790.97}\n",
            " 79% 24521/31000 [7:13:58<1:00:38,  1.78it/s][INFO|trainer.py:2625] 2022-05-18 23:27:37,893 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:27:37,893 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:27:37,893 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                             \n",
            " 79% 24521/31000 [7:14:02<1:00:38,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.42831912636756897, 'eval_runtime': 3.8872, 'eval_samples_per_second': 56.339, 'eval_steps_per_second': 1.029, 'epoch': 791.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:27:41,782 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24521\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:27:41,783 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24521/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:27:43,692 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24521/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:27:43,694 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24521/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:27:48,634 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24459] due to args.save_total_limit\n",
            "{'loss': 0.4453, 'learning_rate': 4.291408779074384e-06, 'epoch': 791.29}\n",
            "{'loss': 0.4449, 'learning_rate': 4.278682278154461e-06, 'epoch': 791.61}\n",
            "                                           {'loss': 0.4454, 'learning_rate': 4.265972245095241e-06, 'epoch': 791.94}\n",
            " 79% 24552/31000 [7:14:31<1:00:27,  1.78it/s][INFO|trainer.py:2625] 2022-05-18 23:28:10,720 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:28:10,720 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:28:10,720 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                             \n",
            " 79% 24552/31000 [7:14:35<1:00:27,  1.78it/s]\n",
            "{'eval_loss': 0.43502822518348694, 'eval_runtime': 3.7761, 'eval_samples_per_second': 57.996, 'eval_steps_per_second': 1.059, 'epoch': 792.0}\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:28:14,498 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24552\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:28:14,500 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24552/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:28:16,414 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24552/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:28:16,415 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24552/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:28:21,216 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24490] due to args.save_total_limit\n",
            " 79% 24560/31000 [7:14:50<1:41:13,  1.06it/s]{'loss': 0.4325, 'learning_rate': 4.253278694360303e-06, 'epoch': 792.26}\n",
            "{'loss': 0.4352, 'learning_rate': 4.240601640394461e-06, 'epoch': 792.58}\n",
            "{'loss': 0.449, 'learning_rate': 4.227941097623773e-06, 'epoch': 792.9}\n",
            " 79% 24583/31000 [7:15:03<59:04,  1.81it/s]  [INFO|trainer.py:2625] 2022-05-18 23:28:43,246 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:28:43,246 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:28:43,246 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.54it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "{'eval_loss': 0.42788195610046387, 'eval_runtime': 3.7672, 'eval_samples_per_second': 58.133, 'eval_steps_per_second': 1.062, 'epoch': 793.0}\n",
            "\n",
            " 79% 24583/31000 [7:15:08<59:04,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:28:47,015 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24583\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:28:47,017 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24583/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:28:48,919 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24583/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:28:48,920 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24583/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:28:53,810 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24521] due to args.save_total_limit\n",
            "                                             {'loss': 0.4282, 'learning_rate': 4.215297080455494e-06, 'epoch': 793.23}\n",
            "{'loss': 0.4439, 'learning_rate': 4.2026696032780915e-06, 'epoch': 793.55}\n",
            "{'loss': 0.4423, 'learning_rate': 4.190058680461182e-06, 'epoch': 793.87}\n",
            " 79% 24614/31000 [7:15:36<57:54,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:29:15,807 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:29:15,807 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:29:15,807 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                           \n",
            " 79% 24614/31000 [7:15:40<57:54,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "{'eval_loss': 0.4276457130908966, 'eval_runtime': 3.7798, 'eval_samples_per_second': 57.94, 'eval_steps_per_second': 1.058, 'epoch': 794.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:29:19,589 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24614\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:29:19,591 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24614/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:29:21,482 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24614/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:29:21,483 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24614/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:29:26,289 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24552] due to args.save_total_limit\n",
            "{'loss': 0.4371, 'learning_rate': 4.177464326355567e-06, 'epoch': 794.19}\n",
            " 79% 24630/31000 [7:16:00<1:03:34,  1.67it/s]{'loss': 0.442, 'learning_rate': 4.1648865552932015e-06, 'epoch': 794.52}\n",
            "{'loss': 0.4433, 'learning_rate': 4.1523253815871375e-06, 'epoch': 794.84}\n",
            " 80% 24645/31000 [7:16:08<57:39,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:29:48,144 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:29:48,144 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:29:48,144 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                           \n",
            " 80% 24645/31000 [7:16:13<57:39,  1.84it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.42718374729156494, 'eval_runtime': 3.8289, 'eval_samples_per_second': 57.196, 'eval_steps_per_second': 1.045, 'epoch': 795.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:29:51,974 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24645\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:29:51,976 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24645/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:29:53,915 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24645/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:29:53,916 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24645/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:29:58,684 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24583] due to args.save_total_limit\n",
            " 80% 24650/31000 [7:16:26<2:53:25,  1.64s/it]{'loss': 0.4473, 'learning_rate': 4.1397808195315705e-06, 'epoch': 795.16}\n",
            " 80% 24660/31000 [7:16:32<1:03:46,  1.66it/s]{'loss': 0.4374, 'learning_rate': 4.12725288340176e-06, 'epoch': 795.48}\n",
            "{'loss': 0.4412, 'learning_rate': 4.114741587454093e-06, 'epoch': 795.81}\n",
            " 80% 24676/31000 [7:16:41<56:58,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:30:20,564 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:30:20,564 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:30:20,564 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.98it/s]\u001b[A\n",
            "{'eval_loss': 0.4294891953468323, 'eval_runtime': 3.7858, 'eval_samples_per_second': 57.847, 'eval_steps_per_second': 1.057, 'epoch': 796.0}\n",
            "\n",
            " 80% 24676/31000 [7:16:45<56:58,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:30:24,351 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24676\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:30:24,353 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24676/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:30:26,255 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24676/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:30:26,258 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24676/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:30:31,074 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24614] due to args.save_total_limit\n",
            "{'loss': 0.4452, 'learning_rate': 4.102246945925971e-06, 'epoch': 796.13}\n",
            " 80% 24690/31000 [7:17:04<1:04:22,  1.63it/s]{'loss': 0.4554, 'learning_rate': 4.089768973035875e-06, 'epoch': 796.45}\n",
            "{'loss': 0.4498, 'learning_rate': 4.077307682983305e-06, 'epoch': 796.77}\n",
            " 80% 24707/31000 [7:17:13<57:08,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:30:53,071 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:30:53,071 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:30:53,071 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.56it/s]\u001b[A\n",
            "                                           \n",
            " 80% 24707/31000 [7:17:18<57:08,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]{'eval_loss': 0.43377721309661865, 'eval_runtime': 3.8381, 'eval_samples_per_second': 57.06, 'eval_steps_per_second': 1.042, 'epoch': 797.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:30:56,911 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24707\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:30:56,912 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24707/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:30:58,771 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24707/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:30:58,772 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24707/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:31:03,560 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24645] due to args.save_total_limit\n",
            " 80% 24710/31000 [7:17:29<4:40:30,  2.68s/it]{'loss': 0.4327, 'learning_rate': 4.06486308994878e-06, 'epoch': 797.1}\n",
            " 80% 24720/31000 [7:17:35<1:09:08,  1.51it/s]{'loss': 0.439, 'learning_rate': 4.052435208093825e-06, 'epoch': 797.42}\n",
            " 80% 24730/31000 [7:17:41<1:01:19,  1.70it/s]{'loss': 0.4462, 'learning_rate': 4.0400240515609285e-06, 'epoch': 797.74}\n",
            " 80% 24738/31000 [7:17:46<56:23,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:31:25,689 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:31:25,689 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:31:25,689 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                           \n",
            " 80% 24738/31000 [7:17:50<56:23,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]{'eval_loss': 0.4200621545314789, 'eval_runtime': 3.77, 'eval_samples_per_second': 58.09, 'eval_steps_per_second': 1.061, 'epoch': 798.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:31:29,461 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24738\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:31:29,463 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24738/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:31:31,365 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24738/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:31:31,366 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24738/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:31:36,245 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-20398] due to args.save_total_limit\n",
            "{'loss': 0.439, 'learning_rate': 4.027629634473566e-06, 'epoch': 798.06}\n",
            " 80% 24750/31000 [7:18:08<1:09:00,  1.51it/s]{'loss': 0.4527, 'learning_rate': 4.015251970936154e-06, 'epoch': 798.39}\n",
            "{'loss': 0.4368, 'learning_rate': 4.00289107503405e-06, 'epoch': 798.71}\n",
            " 80% 24769/31000 [7:18:18<56:21,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:31:58,073 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:31:58,073 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:31:58,073 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "{'eval_loss': 0.43066245317459106, 'eval_runtime': 3.7919, 'eval_samples_per_second': 57.755, 'eval_steps_per_second': 1.055, 'epoch': 799.0}\n",
            "\n",
            " 80% 24769/31000 [7:18:22<56:21,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:32:01,867 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24769\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:32:01,868 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24769/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:32:03,825 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24769/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:32:03,826 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24769/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:32:08,817 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24676] due to args.save_total_limit\n",
            " 80% 24770/31000 [7:18:33<8:29:43,  4.91s/it]{'loss': 0.444, 'learning_rate': 3.990546960833525e-06, 'epoch': 799.03}\n",
            " 80% 24780/31000 [7:18:39<1:14:02,  1.40it/s]{'loss': 0.4386, 'learning_rate': 3.978219642381757e-06, 'epoch': 799.35}\n",
            " 80% 24790/31000 [7:18:45<58:48,  1.76it/s]{'loss': 0.4339, 'learning_rate': 3.965909133706815e-06, 'epoch': 799.68}\n",
            "                                           {'loss': 0.4454, 'learning_rate': 3.953615448817622e-06, 'epoch': 800.0}\n",
            " 80% 24800/31000 [7:18:51<55:47,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:32:30,602 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:32:30,602 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:32:30,602 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                           \n",
            " 80% 24800/31000 [7:18:55<55:47,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:32:34,386 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24800\n",
            "{'eval_loss': 0.4331837594509125, 'eval_runtime': 3.7822, 'eval_samples_per_second': 57.902, 'eval_steps_per_second': 1.058, 'epoch': 800.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:32:34,388 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24800/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:32:36,315 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24800/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:32:36,316 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24800/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:32:41,189 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24707] due to args.save_total_limit\n",
            "                                             {'loss': 0.4472, 'learning_rate': 3.941338601703972e-06, 'epoch': 800.32}\n",
            "                                             {'loss': 0.4357, 'learning_rate': 3.929078606336496e-06, 'epoch': 800.65}\n",
            " 80% 24830/31000 [7:19:23<55:47,  1.84it/s]{'loss': 0.4397, 'learning_rate': 3.916835476666645e-06, 'epoch': 800.97}\n",
            " 80% 24831/31000 [7:19:23<57:51,  1.78it/s][INFO|trainer.py:2625] 2022-05-18 23:33:03,191 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:33:03,191 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:33:03,191 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.42784586548805237, 'eval_runtime': 3.8547, 'eval_samples_per_second': 56.814, 'eval_steps_per_second': 1.038, 'epoch': 801.0}\n",
            " 80% 24831/31000 [7:19:28<57:51,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:33:07,048 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24831\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:33:07,049 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24831/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:33:08,950 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24831/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:33:08,951 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24831/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:33:13,857 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24769] due to args.save_total_limit\n",
            " 80% 24840/31000 [7:19:43<1:27:58,  1.17it/s]{'loss': 0.4437, 'learning_rate': 3.9046092266266825e-06, 'epoch': 801.29}\n",
            "                                           {'loss': 0.4513, 'learning_rate': 3.892399870129646e-06, 'epoch': 801.61}\n",
            "{'loss': 0.4383, 'learning_rate': 3.880207421069382e-06, 'epoch': 801.94}\n",
            " 80% 24862/31000 [7:19:56<56:45,  1.80it/s][INFO|trainer.py:2625] 2022-05-18 23:33:35,849 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:33:35,849 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:33:35,849 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                           \n",
            " 80% 24862/31000 [7:20:00<56:45,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]{'eval_loss': 0.4294675290584564, 'eval_runtime': 3.7653, 'eval_samples_per_second': 58.162, 'eval_steps_per_second': 1.062, 'epoch': 802.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:33:39,616 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24862\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:33:39,618 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24862/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:33:41,524 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24862/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:33:41,525 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24862/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:33:46,249 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24800] due to args.save_total_limit\n",
            "{'loss': 0.4429, 'learning_rate': 3.8680318933204635e-06, 'epoch': 802.26}\n",
            "{'loss': 0.4407, 'learning_rate': 3.855873300738222e-06, 'epoch': 802.58}\n",
            "{'loss': 0.4424, 'learning_rate': 3.843731657158721e-06, 'epoch': 802.9}\n",
            " 80% 24893/31000 [7:20:29<56:04,  1.82it/s][INFO|trainer.py:2625] 2022-05-18 23:34:08,343 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:34:08,343 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:34:08,343 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4295477569103241, 'eval_runtime': 3.786, 'eval_samples_per_second': 57.845, 'eval_steps_per_second': 1.057, 'epoch': 803.0}\n",
            " 80% 24893/31000 [7:20:33<56:04,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:34:12,131 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24893\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:34:12,133 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24893/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:34:14,045 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24893/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:34:14,046 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24893/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:34:18,907 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24831] due to args.save_total_limit\n",
            " 80% 24900/31000 [7:20:47<1:50:47,  1.09s/it]{'loss': 0.4444, 'learning_rate': 3.831606976398731e-06, 'epoch': 803.23}\n",
            "{'loss': 0.4451, 'learning_rate': 3.819499272255725e-06, 'epoch': 803.55}\n",
            "                                           {'loss': 0.4432, 'learning_rate': 3.8074085585078455e-06, 'epoch': 803.87}\n",
            " 80% 24924/31000 [7:21:01<55:20,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:34:41,021 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:34:41,021 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:34:41,021 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.81it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                           \n",
            " 80% 24924/31000 [7:21:05<55:20,  1.83it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.42867031693458557, 'eval_runtime': 3.7351, 'eval_samples_per_second': 58.633, 'eval_steps_per_second': 1.071, 'epoch': 804.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:34:44,758 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24924\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:34:44,761 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24924/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:34:46,667 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24924/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:34:46,668 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24924/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:34:51,796 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24862] due to args.save_total_limit\n",
            "{'loss': 0.4369, 'learning_rate': 3.7953348489139093e-06, 'epoch': 804.19}\n",
            " 80% 24940/31000 [7:21:26<1:00:40,  1.66it/s]{'loss': 0.4386, 'learning_rate': 3.783278157213384e-06, 'epoch': 804.52}\n",
            " 80% 24950/31000 [7:21:32<57:28,  1.75it/s]{'loss': 0.4444, 'learning_rate': 3.771238497126374e-06, 'epoch': 804.84}\n",
            " 80% 24955/31000 [7:21:34<54:41,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:35:13,582 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:35:13,582 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:35:13,583 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4363679885864258, 'eval_runtime': 3.9051, 'eval_samples_per_second': 56.081, 'eval_steps_per_second': 1.024, 'epoch': 805.0}\n",
            " 80% 24955/31000 [7:21:38<54:41,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:35:17,489 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24955\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:35:17,491 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24955/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:35:19,426 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24955/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:35:19,427 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24955/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:35:24,708 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24893] due to args.save_total_limit\n",
            " 81% 24960/31000 [7:21:52<2:49:21,  1.68s/it]{'loss': 0.448, 'learning_rate': 3.759215882353587e-06, 'epoch': 805.16}\n",
            " 81% 24970/31000 [7:21:58<1:01:54,  1.62it/s]{'loss': 0.4428, 'learning_rate': 3.7472103265763427e-06, 'epoch': 805.48}\n",
            "{'loss': 0.452, 'learning_rate': 3.7352218434565677e-06, 'epoch': 805.81}\n",
            " 81% 24986/31000 [7:22:07<54:41,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:35:46,712 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:35:46,712 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:35:46,713 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "                                           \n",
            " 81% 24986/31000 [7:22:11<54:41,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]{'eval_loss': 0.42814287543296814, 'eval_runtime': 3.7892, 'eval_samples_per_second': 57.796, 'eval_steps_per_second': 1.056, 'epoch': 806.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:35:50,503 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24986\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:35:50,505 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24986/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:35:52,420 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24986/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:35:52,421 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24986/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:35:57,157 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24924] due to args.save_total_limit\n",
            " 81% 24990/31000 [7:22:24<3:29:15,  2.09s/it]{'loss': 0.4379, 'learning_rate': 3.723250446636726e-06, 'epoch': 806.13}\n",
            " 81% 25000/31000 [7:22:29<1:00:42,  1.65it/s]{'loss': 0.4513, 'learning_rate': 3.7112961497398577e-06, 'epoch': 806.45}\n",
            " 81% 25010/31000 [7:22:35<56:55,  1.75it/s]{'loss': 0.4245, 'learning_rate': 3.6993589663695437e-06, 'epoch': 806.77}\n",
            " 81% 25017/31000 [7:22:39<54:10,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:36:19,048 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:36:19,048 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:36:19,048 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "{'eval_loss': 0.4303683340549469, 'eval_runtime': 3.7711, 'eval_samples_per_second': 58.073, 'eval_steps_per_second': 1.061, 'epoch': 807.0}\n",
            "\n",
            " 81% 25017/31000 [7:22:43<54:10,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:36:22,820 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25017\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:36:22,822 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25017/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:36:24,777 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25017/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:36:24,778 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25017/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:36:29,812 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24955] due to args.save_total_limit\n",
            " 81% 25020/31000 [7:22:56<4:38:44,  2.80s/it]{'loss': 0.4481, 'learning_rate': 3.6874389101098906e-06, 'epoch': 807.1}\n",
            "{'loss': 0.435, 'learning_rate': 3.675535994525502e-06, 'epoch': 807.42}\n",
            "                                           {'loss': 0.4527, 'learning_rate': 3.6636502331614906e-06, 'epoch': 807.74}\n",
            " 81% 25048/31000 [7:23:12<53:21,  1.86it/s][INFO|trainer.py:2625] 2022-05-18 23:36:51,801 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:36:51,801 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:36:51,801 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "                                           \n",
            " 81% 25048/31000 [7:23:16<53:21,  1.86it/s]\n",
            "{'eval_loss': 0.43099990487098694, 'eval_runtime': 3.7127, 'eval_samples_per_second': 58.987, 'eval_steps_per_second': 1.077, 'epoch': 808.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:36:55,516 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25048\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:36:55,517 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25048/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:36:57,392 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25048/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:36:57,393 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25048/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:37:02,040 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24986] due to args.save_total_limit\n",
            " 81% 25050/31000 [7:23:27<5:47:00,  3.50s/it]{'loss': 0.4438, 'learning_rate': 3.651781639543442e-06, 'epoch': 808.06}\n",
            " 81% 25060/31000 [7:23:33<1:05:30,  1.51it/s]{'loss': 0.441, 'learning_rate': 3.6399302271774082e-06, 'epoch': 808.39}\n",
            " 81% 25070/31000 [7:23:40<56:52,  1.74it/s]{'loss': 0.4422, 'learning_rate': 3.6280960095498907e-06, 'epoch': 808.71}\n",
            " 81% 25079/31000 [7:23:44<53:27,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:37:23,798 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:37:23,798 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:37:23,798 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                           \n",
            " 81% 25079/31000 [7:23:48<53:27,  1.85it/s]{'eval_loss': 0.4316709637641907, 'eval_runtime': 3.7538, 'eval_samples_per_second': 58.341, 'eval_steps_per_second': 1.066, 'epoch': 809.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:37:27,554 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25079\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:37:27,556 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25079/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:37:29,448 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25079/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:37:29,449 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25079/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:37:34,298 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25017] due to args.save_total_limit\n",
            " 81% 25080/31000 [7:23:59<7:59:21,  4.86s/it]{'loss': 0.4417, 'learning_rate': 3.6162790001278084e-06, 'epoch': 809.03}\n",
            " 81% 25090/31000 [7:24:06<1:09:08,  1.42it/s]{'loss': 0.4435, 'learning_rate': 3.6044792123585282e-06, 'epoch': 809.35}\n",
            " 81% 25100/31000 [7:24:11<57:09,  1.72it/s]{'loss': 0.4548, 'learning_rate': 3.5926966596697902e-06, 'epoch': 809.68}\n",
            "{'loss': 0.4284, 'learning_rate': 3.580931355469737e-06, 'epoch': 810.0}\n",
            " 81% 25110/31000 [7:24:17<52:45,  1.86it/s][INFO|trainer.py:2625] 2022-05-18 23:37:56,311 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:37:56,311 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:37:56,311 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "                                           \n",
            " 81% 25110/31000 [7:24:21<52:45,  1.86it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "{'eval_loss': 0.4328193962574005, 'eval_runtime': 3.7516, 'eval_samples_per_second': 58.376, 'eval_steps_per_second': 1.066, 'epoch': 810.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:38:00,064 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25110\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:38:00,066 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25110/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:38:01,969 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25110/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:38:01,970 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25110/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:38:07,077 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25048] due to args.save_total_limit\n",
            " 81% 25120/31000 [7:24:37<1:15:28,  1.30it/s]{'loss': 0.4551, 'learning_rate': 3.5691833131468763e-06, 'epoch': 810.32}\n",
            " 81% 25130/31000 [7:24:44<56:47,  1.72it/s]{'loss': 0.4302, 'learning_rate': 3.5574525460700804e-06, 'epoch': 810.65}\n",
            " 81% 25140/31000 [7:24:49<53:22,  1.83it/s]{'loss': 0.4404, 'learning_rate': 3.5457390675885563e-06, 'epoch': 810.97}\n",
            " 81% 25141/31000 [7:24:49<55:28,  1.76it/s][INFO|trainer.py:2625] 2022-05-18 23:38:29,042 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:38:29,042 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:38:29,042 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.86it/s]\u001b[A\n",
            "                                           \n",
            " 81% 25141/31000 [7:24:53<55:28,  1.76it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]{'eval_loss': 0.42509835958480835, 'eval_runtime': 3.7549, 'eval_samples_per_second': 58.324, 'eval_steps_per_second': 1.065, 'epoch': 811.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:38:32,798 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25141\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:38:32,800 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25141/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:38:34,678 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25141/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:38:34,679 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25141/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:38:39,504 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25079] due to args.save_total_limit\n",
            " 81% 25150/31000 [7:25:09<1:22:32,  1.18it/s]{'loss': 0.4348, 'learning_rate': 3.534042891031836e-06, 'epoch': 811.29}\n",
            "{'loss': 0.4496, 'learning_rate': 3.5223640297097634e-06, 'epoch': 811.61}\n",
            " 81% 25170/31000 [7:25:21<52:51,  1.84it/s]{'loss': 0.4385, 'learning_rate': 3.5107024969124856e-06, 'epoch': 811.94}\n",
            " 81% 25172/31000 [7:25:21<53:28,  1.82it/s][INFO|trainer.py:2625] 2022-05-18 23:39:01,288 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:39:01,289 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:39:01,289 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.43321147561073303, 'eval_runtime': 3.7745, 'eval_samples_per_second': 58.021, 'eval_steps_per_second': 1.06, 'epoch': 812.0}\n",
            " 81% 25172/31000 [7:25:26<53:28,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:39:05,065 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25172\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:39:05,066 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25172/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:39:06,987 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25172/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:39:06,989 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25172/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:39:11,648 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25110] due to args.save_total_limit\n",
            " 81% 25180/31000 [7:25:40<1:30:16,  1.07it/s]{'loss': 0.4452, 'learning_rate': 3.499058305910422e-06, 'epoch': 812.26}\n",
            " 81% 25190/31000 [7:25:46<55:26,  1.75it/s]{'loss': 0.4294, 'learning_rate': 3.4874314699542605e-06, 'epoch': 812.58}\n",
            "{'loss': 0.449, 'learning_rate': 3.475822002274941e-06, 'epoch': 812.9}\n",
            " 81% 25203/31000 [7:25:54<52:51,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:39:33,366 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:39:33,366 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:39:33,366 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                           \n",
            " 81% 25203/31000 [7:25:58<52:51,  1.83it/s]\n",
            "{'eval_loss': 0.4307607114315033, 'eval_runtime': 3.7441, 'eval_samples_per_second': 58.491, 'eval_steps_per_second': 1.068, 'epoch': 813.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:39:37,112 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25203\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:39:37,114 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25203/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:39:39,003 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25203/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:39:39,004 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25203/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:39:43,804 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25141] due to args.save_total_limit\n",
            "                                             {'loss': 0.4474, 'learning_rate': 3.464229916083643e-06, 'epoch': 813.23}\n",
            " 81% 25220/31000 [7:26:18<55:48,  1.73it/s]{'loss': 0.4447, 'learning_rate': 3.4526552245717487e-06, 'epoch': 813.55}\n",
            " 81% 25230/31000 [7:26:24<53:37,  1.79it/s]{'loss': 0.4324, 'learning_rate': 3.4410979409108625e-06, 'epoch': 813.87}\n",
            " 81% 25234/31000 [7:26:26<52:25,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:40:05,541 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:40:05,541 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:40:05,541 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                           \n",
            " 81% 25234/31000 [7:26:30<52:25,  1.83it/s]\n",
            "                                 {'eval_loss': 0.42763808369636536, 'eval_runtime': 3.7586, 'eval_samples_per_second': 58.267, 'eval_steps_per_second': 1.064, 'epoch': 814.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-18 23:40:09,301 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25234\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:40:09,303 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25234/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:40:11,195 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25234/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:40:11,198 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25234/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:40:16,416 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25172] due to args.save_total_limit\n",
            "                                             {'loss': 0.4427, 'learning_rate': 3.4295580782527774e-06, 'epoch': 814.19}\n",
            " 81% 25250/31000 [7:26:50<56:59,  1.68it/s]{'loss': 0.4326, 'learning_rate': 3.4180356497294594e-06, 'epoch': 814.52}\n",
            " 81% 25260/31000 [7:26:56<54:44,  1.75it/s]{'loss': 0.4531, 'learning_rate': 3.406530668453036e-06, 'epoch': 814.84}\n",
            " 82% 25265/31000 [7:26:59<51:50,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:40:38,349 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:40:38,349 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:40:38,349 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.43448078632354736, 'eval_runtime': 3.7467, 'eval_samples_per_second': 58.451, 'eval_steps_per_second': 1.068, 'epoch': 815.0}\n",
            "                                           \n",
            " 82% 25265/31000 [7:27:03<51:50,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:40:42,098 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25265\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:40:42,099 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25265/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:40:43,984 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25265/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:40:43,985 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25265/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:40:48,456 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25203] due to args.save_total_limit\n",
            "{'loss': 0.4472, 'learning_rate': 3.395043147515771e-06, 'epoch': 815.16}\n",
            " 82% 25280/31000 [7:27:25<57:13,  1.67it/s]{'loss': 0.4395, 'learning_rate': 3.38357309999008e-06, 'epoch': 815.48}\n",
            "                                           {'loss': 0.4349, 'learning_rate': 3.372120538928472e-06, 'epoch': 815.81}\n",
            " 82% 25296/31000 [7:27:33<51:02,  1.86it/s][INFO|trainer.py:2625] 2022-05-18 23:41:12,783 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:41:12,783 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:41:12,783 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4268982708454132, 'eval_runtime': 3.7234, 'eval_samples_per_second': 58.818, 'eval_steps_per_second': 1.074, 'epoch': 816.0}\n",
            " 82% 25296/31000 [7:27:37<51:02,  1.86it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:41:16,508 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25296\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:41:16,510 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25296/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:41:18,424 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25296/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:41:18,424 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25296/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:41:22,893 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25234] due to args.save_total_limit\n",
            " 82% 25300/31000 [7:27:50<3:10:53,  2.01s/it]{'loss': 0.4387, 'learning_rate': 3.3606854773635732e-06, 'epoch': 816.13}\n",
            " 82% 25310/31000 [7:27:55<58:00,  1.63it/s]{'loss': 0.4334, 'learning_rate': 3.349267928308076e-06, 'epoch': 816.45}\n",
            " 82% 25320/31000 [7:28:02<54:15,  1.74it/s]{'loss': 0.4427, 'learning_rate': 3.337867904754774e-06, 'epoch': 816.77}\n",
            " 82% 25327/31000 [7:28:05<51:17,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:41:44,853 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:41:44,854 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:41:44,854 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.24it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.48it/s]\u001b[A\n",
            "                                           \n",
            " 82% 25327/31000 [7:28:09<51:17,  1.84it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.43205079436302185, 'eval_runtime': 3.8573, 'eval_samples_per_second': 56.775, 'eval_steps_per_second': 1.037, 'epoch': 817.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:41:48,713 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25327\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:41:48,715 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25327/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:41:50,754 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25327/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:41:50,755 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25327/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:41:55,536 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25265] due to args.save_total_limit\n",
            "                                             {'loss': 0.4515, 'learning_rate': 3.3264854196764985e-06, 'epoch': 817.1}\n",
            " 82% 25340/31000 [7:28:27<59:48,  1.58it/s]  {'loss': 0.4409, 'learning_rate': 3.3151204860261176e-06, 'epoch': 817.42}\n",
            "                                           {'loss': 0.4369, 'learning_rate': 3.3037731167365357e-06, 'epoch': 817.74}\n",
            " 82% 25358/31000 [7:28:38<50:53,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:42:17,561 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:42:17,561 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:42:17,561 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.86it/s]\u001b[A\n",
            "                                           \n",
            " 82% 25358/31000 [7:28:42<50:53,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.97it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:42:21,314 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25358\n",
            "{'eval_loss': 0.433118999004364, 'eval_runtime': 3.7509, 'eval_samples_per_second': 58.387, 'eval_steps_per_second': 1.066, 'epoch': 818.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:42:21,315 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25358/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:42:23,198 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25358/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:42:23,199 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25358/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:42:27,853 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25296] due to args.save_total_limit\n",
            " 82% 25360/31000 [7:28:53<5:32:54,  3.54s/it]{'loss': 0.4394, 'learning_rate': 3.2924433247206714e-06, 'epoch': 818.06}\n",
            "                                             {'loss': 0.4356, 'learning_rate': 3.2811311228714388e-06, 'epoch': 818.39}\n",
            " 82% 25380/31000 [7:29:06<54:06,  1.73it/s]{'loss': 0.4514, 'learning_rate': 3.269836524061728e-06, 'epoch': 818.71}\n",
            " 82% 25389/31000 [7:29:10<50:06,  1.87it/s][INFO|trainer.py:2625] 2022-05-18 23:42:49,744 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:42:49,744 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:42:49,744 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "                                           \n",
            " 82% 25389/31000 [7:29:14<50:06,  1.87it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.4287131726741791, 'eval_runtime': 3.7032, 'eval_samples_per_second': 59.138, 'eval_steps_per_second': 1.08, 'epoch': 819.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:42:53,449 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25389\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:42:53,451 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25389/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:42:55,307 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25389/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:42:55,308 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25389/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:42:59,940 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25327] due to args.save_total_limit\n",
            "                                             {'loss': 0.4386, 'learning_rate': 3.2585595411443997e-06, 'epoch': 819.03}\n",
            " 82% 25400/31000 [7:29:31<1:05:12,  1.43it/s]{'loss': 0.4426, 'learning_rate': 3.2473001869522864e-06, 'epoch': 819.35}\n",
            " 82% 25410/31000 [7:29:36<53:11,  1.75it/s]{'loss': 0.4467, 'learning_rate': 3.2360584742981325e-06, 'epoch': 819.68}\n",
            "                                           {'loss': 0.4449, 'learning_rate': 3.224834415974624e-06, 'epoch': 820.0}\n",
            " 82% 25420/31000 [7:29:42<49:52,  1.86it/s][INFO|trainer.py:2625] 2022-05-18 23:43:21,757 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:43:21,758 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:43:21,758 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                           \n",
            " 82% 25420/31000 [7:29:46<49:52,  1.86it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "{'eval_loss': 0.425855427980423, 'eval_runtime': 3.79, 'eval_samples_per_second': 57.783, 'eval_steps_per_second': 1.055, 'epoch': 820.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:43:25,549 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25420\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:43:25,552 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25420/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:43:27,436 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25420/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:43:27,437 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25420/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:43:31,941 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25358] due to args.save_total_limit\n",
            " 82% 25430/31000 [7:30:02<1:10:32,  1.32it/s]{'loss': 0.4285, 'learning_rate': 3.213628024754352e-06, 'epoch': 820.32}\n",
            "{'loss': 0.4459, 'learning_rate': 3.2024393133898056e-06, 'epoch': 820.65}\n",
            "                                           {'loss': 0.4514, 'learning_rate': 3.1912682946133474e-06, 'epoch': 820.97}\n",
            " 82% 25451/31000 [7:30:14<51:50,  1.78it/s][INFO|trainer.py:2625] 2022-05-18 23:43:53,792 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:43:53,792 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:43:53,793 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            " 82% 25451/31000 [7:30:18<51:50,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "{'eval_loss': 0.4265586733818054, 'eval_runtime': 3.7642, 'eval_samples_per_second': 58.18, 'eval_steps_per_second': 1.063, 'epoch': 821.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:43:57,558 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25451\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:43:57,561 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25451/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:43:59,443 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25451/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:43:59,444 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25451/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:44:03,700 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25389] due to args.save_total_limit\n",
            " 82% 25460/31000 [7:30:33<1:17:41,  1.19it/s]{'loss': 0.4391, 'learning_rate': 3.180114981137213e-06, 'epoch': 821.29}\n",
            "                                           {'loss': 0.4372, 'learning_rate': 3.168979385653491e-06, 'epoch': 821.61}\n",
            "{'loss': 0.4453, 'learning_rate': 3.157861520834104e-06, 'epoch': 821.94}\n",
            " 82% 25482/31000 [7:30:46<51:06,  1.80it/s][INFO|trainer.py:2625] 2022-05-18 23:44:25,685 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:44:25,685 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:44:25,685 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                           \n",
            " 82% 25482/31000 [7:30:50<51:06,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]{'eval_loss': 0.4310169517993927, 'eval_runtime': 3.7619, 'eval_samples_per_second': 58.215, 'eval_steps_per_second': 1.063, 'epoch': 822.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:44:29,448 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25482\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:44:29,450 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25482/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:44:31,347 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25482/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:44:31,348 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25482/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:44:35,613 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25420] due to args.save_total_limit\n",
            "                                             {'loss': 0.4422, 'learning_rate': 3.1467613993308044e-06, 'epoch': 822.26}\n",
            "{'loss': 0.436, 'learning_rate': 3.1356790337751334e-06, 'epoch': 822.58}\n",
            " 82% 25510/31000 [7:31:17<50:19,  1.82it/s]{'loss': 0.4417, 'learning_rate': 3.1246144367784576e-06, 'epoch': 822.9}\n",
            " 82% 25513/31000 [7:31:18<50:05,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:44:57,560 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:44:57,560 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:44:57,560 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.59it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "{'eval_loss': 0.42603591084480286, 'eval_runtime': 3.6978, 'eval_samples_per_second': 59.224, 'eval_steps_per_second': 1.082, 'epoch': 823.0}\n",
            "\n",
            " 82% 25513/31000 [7:31:22<50:05,  1.83it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:45:01,259 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25513\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:45:01,261 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25513/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:45:03,204 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25513/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:45:03,204 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25513/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:45:08,060 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25451] due to args.save_total_limit\n",
            " 82% 25520/31000 [7:31:36<1:39:51,  1.09s/it]{'loss': 0.4399, 'learning_rate': 3.1135676209318963e-06, 'epoch': 823.23}\n",
            "{'loss': 0.4486, 'learning_rate': 3.1025385988063487e-06, 'epoch': 823.55}\n",
            " 82% 25540/31000 [7:31:48<50:59,  1.78it/s]{'loss': 0.4479, 'learning_rate': 3.0915273829524597e-06, 'epoch': 823.87}\n",
            " 82% 25544/31000 [7:31:50<49:35,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:45:29,954 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:45:29,954 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:45:29,954 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4286632239818573, 'eval_runtime': 3.7353, 'eval_samples_per_second': 58.629, 'eval_steps_per_second': 1.071, 'epoch': 824.0}\n",
            " 82% 25544/31000 [7:31:54<49:35,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:45:33,691 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25544\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:45:33,693 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25544/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:45:35,623 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25544/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:45:35,624 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25544/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:45:40,690 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25482] due to args.save_total_limit\n",
            " 82% 25550/31000 [7:32:08<1:59:56,  1.32s/it]{'loss': 0.4341, 'learning_rate': 3.0805339859006157e-06, 'epoch': 824.19}\n",
            "{'loss': 0.4417, 'learning_rate': 3.069558420160927e-06, 'epoch': 824.52}\n",
            "{'loss': 0.4409, 'learning_rate': 3.0586006982232e-06, 'epoch': 824.84}\n",
            " 82% 25575/31000 [7:32:23<48:50,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:46:02,394 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:46:02,395 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:46:02,395 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "                                           \n",
            " 82% 25575/31000 [7:32:27<48:50,  1.85it/s]{'eval_loss': 0.426378071308136, 'eval_runtime': 3.8182, 'eval_samples_per_second': 57.357, 'eval_steps_per_second': 1.048, 'epoch': 825.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:46:06,215 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25575\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:46:06,216 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25575/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:46:08,086 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25575/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:46:08,087 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25575/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:46:13,094 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25513] due to args.save_total_limit\n",
            "{'loss': 0.4494, 'learning_rate': 3.0476608325569507e-06, 'epoch': 825.16}\n",
            " 83% 25590/31000 [7:32:46<54:31,  1.65it/s]{'loss': 0.4407, 'learning_rate': 3.0367388356113672e-06, 'epoch': 825.48}\n",
            "                                           {'loss': 0.4347, 'learning_rate': 3.025834719815307e-06, 'epoch': 825.81}\n",
            " 83% 25606/31000 [7:32:55<48:36,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:46:34,853 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:46:34,853 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:46:34,853 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.54it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                           \n",
            " 83% 25606/31000 [7:32:59<48:36,  1.85it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.42449861764907837, 'eval_runtime': 3.8711, 'eval_samples_per_second': 56.573, 'eval_steps_per_second': 1.033, 'epoch': 826.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:46:38,725 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25606\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:46:38,727 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25606/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:46:40,586 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25606/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:46:40,587 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25606/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:46:45,494 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25544] due to args.save_total_limit\n",
            "{'loss': 0.4513, 'learning_rate': 3.0149484975772803e-06, 'epoch': 826.13}\n",
            " 83% 25620/31000 [7:33:18<54:48,  1.64it/s]{'loss': 0.4259, 'learning_rate': 3.004080181285421e-06, 'epoch': 826.45}\n",
            " 83% 25630/31000 [7:33:24<52:01,  1.72it/s]{'loss': 0.4458, 'learning_rate': 2.993229783307517e-06, 'epoch': 826.77}\n",
            " 83% 25637/31000 [7:33:28<48:38,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:47:07,445 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:47:07,445 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:47:07,445 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                           \n",
            " 83% 25637/31000 [7:33:32<48:38,  1.84it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.4284366965293884, 'eval_runtime': 3.7673, 'eval_samples_per_second': 58.132, 'eval_steps_per_second': 1.062, 'epoch': 827.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:47:11,214 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25637\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:47:11,216 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25637/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:47:13,123 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25637/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:47:13,124 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25637/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:47:17,789 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25575] due to args.save_total_limit\n",
            " 83% 25640/31000 [7:33:44<3:56:17,  2.64s/it]{'loss': 0.4457, 'learning_rate': 2.9823973159909317e-06, 'epoch': 827.1}\n",
            " 83% 25650/31000 [7:33:50<58:56,  1.51it/s]{'loss': 0.4531, 'learning_rate': 2.9715827916626453e-06, 'epoch': 827.42}\n",
            "{'loss': 0.4364, 'learning_rate': 2.960786222629212e-06, 'epoch': 827.74}\n",
            " 83% 25668/31000 [7:34:00<47:59,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:47:39,837 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:47:39,837 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:47:39,837 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                           \n",
            " 83% 25668/31000 [7:34:04<47:59,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A{'eval_loss': 0.433291494846344, 'eval_runtime': 3.7663, 'eval_samples_per_second': 58.147, 'eval_steps_per_second': 1.062, 'epoch': 828.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:47:43,605 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25668\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:47:43,607 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25668/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:47:45,518 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25668/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:47:45,519 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25668/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:47:50,587 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25606] due to args.save_total_limit\n",
            "                                             {'loss': 0.4361, 'learning_rate': 2.950007621176758e-06, 'epoch': 828.06}\n",
            "{'loss': 0.4403, 'learning_rate': 2.9392469995709613e-06, 'epoch': 828.39}\n",
            "                                           {'loss': 0.4408, 'learning_rate': 2.9285043700570332e-06, 'epoch': 828.71}\n",
            " 83% 25699/31000 [7:34:33<47:40,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:48:12,622 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:48:12,622 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:48:12,622 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.43252328038215637, 'eval_runtime': 3.7962, 'eval_samples_per_second': 57.69, 'eval_steps_per_second': 1.054, 'epoch': 829.0}\n",
            " 83% 25699/31000 [7:34:37<47:40,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:48:16,420 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25699\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:48:16,422 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25699/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:48:18,336 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25699/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:48:18,337 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25699/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:48:23,454 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25637] due to args.save_total_limit\n",
            "{'loss': 0.4406, 'learning_rate': 2.9177797448597156e-06, 'epoch': 829.03}\n",
            " 83% 25710/31000 [7:34:54<1:02:50,  1.40it/s]{'loss': 0.4396, 'learning_rate': 2.9070731361832666e-06, 'epoch': 829.35}\n",
            "                                           {'loss': 0.4408, 'learning_rate': 2.896384556211441e-06, 'epoch': 829.68}\n",
            "                                           {'loss': 0.4456, 'learning_rate': 2.8857140171074585e-06, 'epoch': 830.0}\n",
            " 83% 25730/31000 [7:35:06<47:24,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:48:45,348 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:48:45,348 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:48:45,348 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                           \n",
            " 83% 25730/31000 [7:35:10<47:24,  1.85it/s]{'eval_loss': 0.4328312277793884, 'eval_runtime': 3.87, 'eval_samples_per_second': 56.589, 'eval_steps_per_second': 1.034, 'epoch': 830.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:48:49,220 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25730\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:48:49,221 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25730/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:48:51,135 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25730/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:48:51,136 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25730/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:48:55,858 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25668] due to args.save_total_limit\n",
            " 83% 25740/31000 [7:35:26<1:06:44,  1.31it/s]{'loss': 0.4495, 'learning_rate': 2.875061531014042e-06, 'epoch': 830.32}\n",
            " 83% 25750/31000 [7:35:32<51:45,  1.69it/s]{'loss': 0.4392, 'learning_rate': 2.8644271100533503e-06, 'epoch': 830.65}\n",
            "{'loss': 0.4355, 'learning_rate': 2.8538107663269805e-06, 'epoch': 830.97}\n",
            " 83% 25761/31000 [7:35:38<49:52,  1.75it/s][INFO|trainer.py:2625] 2022-05-18 23:49:18,052 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:49:18,052 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:49:18,052 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.59it/s]\u001b[A\n",
            "                                           \n",
            " 83% 25761/31000 [7:35:43<49:52,  1.75it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.42639216780662537, 'eval_runtime': 3.883, 'eval_samples_per_second': 56.4, 'eval_steps_per_second': 1.03, 'epoch': 831.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-18 23:49:21,937 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25761\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:49:21,938 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25761/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:49:23,980 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25761/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:49:23,981 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25761/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:49:29,235 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25699] due to args.save_total_limit\n",
            "                                             {'loss': 0.4564, 'learning_rate': 2.843212511915971e-06, 'epoch': 831.29}\n",
            " 83% 25780/31000 [7:36:05<52:03,  1.67it/s]{'loss': 0.4484, 'learning_rate': 2.832632358880768e-06, 'epoch': 831.61}\n",
            "                                           {'loss': 0.4355, 'learning_rate': 2.8220703192612263e-06, 'epoch': 831.94}\n",
            " 83% 25792/31000 [7:36:12<48:08,  1.80it/s][INFO|trainer.py:2625] 2022-05-18 23:49:51,429 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:49:51,429 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:49:51,429 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            " 83% 25792/31000 [7:36:16<48:08,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.4336615204811096, 'eval_runtime': 3.8046, 'eval_samples_per_second': 57.562, 'eval_steps_per_second': 1.051, 'epoch': 832.0}\n",
            "[INFO|trainer.py:2345] 2022-05-18 23:49:55,235 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25792\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:49:55,237 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25792/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:49:57,153 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25792/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:49:57,154 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25792/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:50:02,067 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25730] due to args.save_total_limit\n",
            "                                             {'loss': 0.436, 'learning_rate': 2.811526405076573e-06, 'epoch': 832.26}\n",
            " 83% 25810/31000 [7:36:37<49:43,  1.74it/s]{'loss': 0.4429, 'learning_rate': 2.8010006283254204e-06, 'epoch': 832.58}\n",
            "{'loss': 0.4469, 'learning_rate': 2.790493000985752e-06, 'epoch': 832.9}\n",
            " 83% 25823/31000 [7:36:44<47:20,  1.82it/s][INFO|trainer.py:2625] 2022-05-18 23:50:23,828 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:50:23,828 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:50:23,828 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "                                           \n",
            " 83% 25823/31000 [7:36:48<47:20,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:50:27,615 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25823\n",
            "{'eval_loss': 0.4299268424510956, 'eval_runtime': 3.7855, 'eval_samples_per_second': 57.852, 'eval_steps_per_second': 1.057, 'epoch': 833.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:50:27,617 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25823/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:50:29,570 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25823/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:50:29,571 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25823/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:50:34,604 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25761] due to args.save_total_limit\n",
            "{'loss': 0.44, 'learning_rate': 2.780003535014874e-06, 'epoch': 833.23}\n",
            "                                           {'loss': 0.4414, 'learning_rate': 2.769532242349444e-06, 'epoch': 833.55}\n",
            "                                           {'loss': 0.4343, 'learning_rate': 2.7590791349054197e-06, 'epoch': 833.87}\n",
            " 83% 25854/31000 [7:37:17<46:42,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:50:56,437 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:50:56,437 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:50:56,437 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            " 83% 25854/31000 [7:37:21<46:42,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]{'eval_loss': 0.4270065426826477, 'eval_runtime': 3.7371, 'eval_samples_per_second': 58.602, 'eval_steps_per_second': 1.07, 'epoch': 834.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:51:00,176 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25854\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:51:00,177 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25854/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:51:02,090 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25854/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:51:02,091 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25854/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:51:06,974 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25792] due to args.save_total_limit\n",
            " 83% 25860/31000 [7:37:35<1:52:15,  1.31s/it]{'loss': 0.4394, 'learning_rate': 2.7486442245780942e-06, 'epoch': 834.19}\n",
            "                                           {'loss': 0.4483, 'learning_rate': 2.738227523242021e-06, 'epoch': 834.52}\n",
            "{'loss': 0.4431, 'learning_rate': 2.7278290427510557e-06, 'epoch': 834.84}\n",
            " 84% 25885/31000 [7:37:49<46:25,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:51:28,748 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:51:28,748 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:51:28,748 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.42189037799835205, 'eval_runtime': 3.8236, 'eval_samples_per_second': 57.276, 'eval_steps_per_second': 1.046, 'epoch': 835.0}\n",
            " 84% 25885/31000 [7:37:53<46:25,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:51:32,573 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25885\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:51:32,575 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25885/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:51:34,510 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25885/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:51:34,511 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25885/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:51:39,397 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25823] due to args.save_total_limit\n",
            "                                             {'loss': 0.4471, 'learning_rate': 2.717448794938305e-06, 'epoch': 835.16}\n",
            " 84% 25900/31000 [7:38:12<53:01,  1.60it/s]{'loss': 0.4498, 'learning_rate': 2.70708679161614e-06, 'epoch': 835.48}\n",
            "{'loss': 0.4412, 'learning_rate': 2.6967430445761605e-06, 'epoch': 835.81}\n",
            " 84% 25916/31000 [7:38:22<47:08,  1.80it/s][INFO|trainer.py:2625] 2022-05-18 23:52:01,627 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:52:01,627 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:52:01,627 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "                                           \n",
            " 84% 25916/31000 [7:38:26<47:08,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]{'eval_loss': 0.42468008399009705, 'eval_runtime': 3.8515, 'eval_samples_per_second': 56.861, 'eval_steps_per_second': 1.039, 'epoch': 836.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:52:05,480 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25916\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:52:05,482 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25916/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:52:07,406 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25916/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:52:07,407 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25916/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:52:12,306 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25854] due to args.save_total_limit\n",
            " 84% 25920/31000 [7:38:39<2:55:42,  2.08s/it]{'loss': 0.4421, 'learning_rate': 2.6864175655891892e-06, 'epoch': 836.13}\n",
            " 84% 25930/31000 [7:38:45<54:16,  1.56it/s]{'loss': 0.4373, 'learning_rate': 2.67611036640527e-06, 'epoch': 836.45}\n",
            "{'loss': 0.4376, 'learning_rate': 2.66582145875364e-06, 'epoch': 836.77}\n",
            " 84% 25947/31000 [7:38:55<45:52,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:52:34,419 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:52:34,420 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:52:34,420 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                           \n",
            " 84% 25947/31000 [7:38:59<45:52,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:52:38,189 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25947\n",
            "{'eval_loss': 0.42968985438346863, 'eval_runtime': 3.7679, 'eval_samples_per_second': 58.123, 'eval_steps_per_second': 1.062, 'epoch': 837.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:52:38,191 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25947/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:52:40,151 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25947/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:52:40,151 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25947/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:52:45,056 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25885] due to args.save_total_limit\n",
            "                                             {'loss': 0.4338, 'learning_rate': 2.6555508543427183e-06, 'epoch': 837.1}\n",
            "{'loss': 0.4426, 'learning_rate': 2.6452985648601037e-06, 'epoch': 837.42}\n",
            "{'loss': 0.4455, 'learning_rate': 2.635064601972543e-06, 'epoch': 837.74}\n",
            " 84% 25978/31000 [7:39:27<45:17,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:53:07,158 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:53:07,158 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:53:07,158 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                           \n",
            " 84% 25978/31000 [7:39:32<45:17,  1.85it/s]{'eval_loss': 0.43280845880508423, 'eval_runtime': 3.7911, 'eval_samples_per_second': 57.767, 'eval_steps_per_second': 1.055, 'epoch': 838.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:53:10,951 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25978\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:53:10,952 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25978/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:53:12,883 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25978/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:53:12,884 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25978/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:53:17,924 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25916] due to args.save_total_limit\n",
            "{'loss': 0.4482, 'learning_rate': 2.624848977325942e-06, 'epoch': 838.06}\n",
            " 84% 25990/31000 [7:39:50<56:10,  1.49it/s]{'loss': 0.4405, 'learning_rate': 2.6146517025453173e-06, 'epoch': 838.39}\n",
            " 84% 26000/31000 [7:39:55<47:25,  1.76it/s]{'loss': 0.4355, 'learning_rate': 2.6044727892348226e-06, 'epoch': 838.71}\n",
            " 84% 26009/31000 [7:40:00<44:56,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:53:39,932 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:53:39,933 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:53:39,933 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "\u001b[A{'eval_loss': 0.4297037124633789, 'eval_runtime': 3.8158, 'eval_samples_per_second': 57.392, 'eval_steps_per_second': 1.048, 'epoch': 839.0}\n",
            "                                           \n",
            " 84% 26009/31000 [7:40:04<44:56,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:53:43,750 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26009\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:53:43,752 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26009/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:53:45,665 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26009/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:53:45,666 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26009/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:53:50,678 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25947] due to args.save_total_limit\n",
            "{'loss': 0.4427, 'learning_rate': 2.5943122489777105e-06, 'epoch': 839.03}\n",
            " 84% 26020/31000 [7:40:22<58:19,  1.42it/s]{'loss': 0.4419, 'learning_rate': 2.584170093336324e-06, 'epoch': 839.35}\n",
            "{'loss': 0.4441, 'learning_rate': 2.5740463338520894e-06, 'epoch': 839.68}\n",
            "                                           {'loss': 0.4382, 'learning_rate': 2.563940982045489e-06, 'epoch': 840.0}\n",
            " 84% 26040/31000 [7:40:33<44:45,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:54:12,731 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:54:12,731 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:54:12,731 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                           \n",
            " 84% 26040/31000 [7:40:37<44:45,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]{'eval_loss': 0.4370439052581787, 'eval_runtime': 3.8268, 'eval_samples_per_second': 57.228, 'eval_steps_per_second': 1.045, 'epoch': 840.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:54:16,559 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26040\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:54:16,561 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26040/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:54:18,470 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26040/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:54:18,471 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26040/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:54:23,345 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-25978] due to args.save_total_limit\n",
            "                                             {'loss': 0.4409, 'learning_rate': 2.5538540494160764e-06, 'epoch': 840.32}\n",
            "{'loss': 0.4461, 'learning_rate': 2.5437855474424217e-06, 'epoch': 840.65}\n",
            " 84% 26070/31000 [7:41:06<44:29,  1.85it/s]{'loss': 0.4455, 'learning_rate': 2.5337354875821383e-06, 'epoch': 840.97}\n",
            " 84% 26071/31000 [7:41:06<45:57,  1.79it/s][INFO|trainer.py:2625] 2022-05-18 23:54:45,397 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:54:45,397 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:54:45,397 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A{'eval_loss': 0.43316563963890076, 'eval_runtime': 3.7411, 'eval_samples_per_second': 58.538, 'eval_steps_per_second': 1.069, 'epoch': 841.0}\n",
            " 84% 26071/31000 [7:41:10<45:57,  1.79it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:54:49,140 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26071\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:54:49,141 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26071/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:54:51,055 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26071/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:54:51,056 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26071/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:54:56,019 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26009] due to args.save_total_limit\n",
            " 84% 26080/31000 [7:41:25<1:10:31,  1.16it/s]{'loss': 0.4386, 'learning_rate': 2.5237038812718477e-06, 'epoch': 841.29}\n",
            "                                           {'loss': 0.4441, 'learning_rate': 2.5136907399271717e-06, 'epoch': 841.61}\n",
            "{'loss': 0.4371, 'learning_rate': 2.503696074942723e-06, 'epoch': 841.94}\n",
            " 84% 26102/31000 [7:41:38<45:25,  1.80it/s][INFO|trainer.py:2625] 2022-05-18 23:55:17,885 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:55:17,886 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:55:17,886 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            " 84% 26102/31000 [7:41:42<45:25,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]{'eval_loss': 0.4297509789466858, 'eval_runtime': 3.8197, 'eval_samples_per_second': 57.335, 'eval_steps_per_second': 1.047, 'epoch': 842.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:55:21,707 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26102\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:55:21,709 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26102/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:55:23,611 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26102/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:55:23,612 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26102/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:55:28,587 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26040] due to args.save_total_limit\n",
            "{'loss': 0.4433, 'learning_rate': 2.4937198976920806e-06, 'epoch': 842.26}\n",
            " 84% 26120/31000 [7:42:04<46:47,  1.74it/s]{'loss': 0.4382, 'learning_rate': 2.4837622195277906e-06, 'epoch': 842.58}\n",
            "                                           {'loss': 0.4428, 'learning_rate': 2.4738230517813513e-06, 'epoch': 842.9}\n",
            " 84% 26133/31000 [7:42:11<44:41,  1.82it/s][INFO|trainer.py:2625] 2022-05-18 23:55:50,423 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:55:50,423 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:55:50,423 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.58it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4330502450466156, 'eval_runtime': 3.8913, 'eval_samples_per_second': 56.279, 'eval_steps_per_second': 1.028, 'epoch': 843.0}\n",
            " 84% 26133/31000 [7:42:15<44:41,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:55:54,316 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26133\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:55:54,318 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26133/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:55:56,196 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26133/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:55:56,197 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26133/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:56:01,055 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26071] due to args.save_total_limit\n",
            " 84% 26140/31000 [7:42:30<1:28:34,  1.09s/it]{'loss': 0.449, 'learning_rate': 2.463902405763194e-06, 'epoch': 843.23}\n",
            " 84% 26150/31000 [7:42:35<50:17,  1.61it/s]{'loss': 0.4327, 'learning_rate': 2.4540002927626595e-06, 'epoch': 843.55}\n",
            "                                           {'loss': 0.4443, 'learning_rate': 2.4441167240480277e-06, 'epoch': 843.87}\n",
            " 84% 26164/31000 [7:42:43<44:09,  1.82it/s][INFO|trainer.py:2625] 2022-05-18 23:56:23,284 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:56:23,284 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:56:23,285 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                           \n",
            " 84% 26164/31000 [7:42:48<44:09,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A{'eval_loss': 0.4314518868923187, 'eval_runtime': 3.7783, 'eval_samples_per_second': 57.963, 'eval_steps_per_second': 1.059, 'epoch': 844.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:56:27,065 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26164\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:56:27,066 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26164/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:56:28,978 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26164/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:56:28,980 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26164/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:56:35,864 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26102] due to args.save_total_limit\n",
            "                                             {'loss': 0.4305, 'learning_rate': 2.4342517108664542e-06, 'epoch': 844.19}\n",
            " 84% 26180/31000 [7:43:09<48:17,  1.66it/s]{'loss': 0.4418, 'learning_rate': 2.424405264443977e-06, 'epoch': 844.52}\n",
            "{'loss': 0.4384, 'learning_rate': 2.4145773959855173e-06, 'epoch': 844.84}\n",
            " 84% 26195/31000 [7:43:18<43:31,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:56:57,732 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:56:57,732 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:56:57,732 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.42978635430336, 'eval_runtime': 3.8479, 'eval_samples_per_second': 56.914, 'eval_steps_per_second': 1.04, 'epoch': 845.0}\n",
            " 84% 26195/31000 [7:43:22<43:31,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:57:01,582 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26195\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:57:01,584 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26195/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:57:03,508 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26195/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:57:03,509 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26195/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:57:08,499 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26133] due to args.save_total_limit\n",
            "{'loss': 0.4452, 'learning_rate': 2.4047681166748517e-06, 'epoch': 845.16}\n",
            " 85% 26210/31000 [7:43:42<48:19,  1.65it/s]{'loss': 0.4429, 'learning_rate': 2.3949774376746047e-06, 'epoch': 845.48}\n",
            "{'loss': 0.4533, 'learning_rate': 2.385205370126223e-06, 'epoch': 845.81}\n",
            " 85% 26226/31000 [7:43:51<43:13,  1.84it/s][INFO|trainer.py:2625] 2022-05-18 23:57:30,483 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:57:30,483 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:57:30,483 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4285570979118347, 'eval_runtime': 3.8745, 'eval_samples_per_second': 56.523, 'eval_steps_per_second': 1.032, 'epoch': 846.0}\n",
            " 85% 26226/31000 [7:43:55<43:13,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:57:34,360 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26226\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:57:34,362 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26226/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:57:36,310 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26226/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:57:36,311 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26226/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:57:41,411 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26164] due to args.save_total_limit\n",
            "                                             {'loss': 0.44, 'learning_rate': 2.375451925149982e-06, 'epoch': 846.13}\n",
            " 85% 26240/31000 [7:44:14<50:24,  1.57it/s]{'loss': 0.4328, 'learning_rate': 2.3657171138449825e-06, 'epoch': 846.45}\n",
            "                                           {'loss': 0.4372, 'learning_rate': 2.3560009472890857e-06, 'epoch': 846.77}\n",
            " 85% 26257/31000 [7:44:24<43:05,  1.83it/s][INFO|trainer.py:2625] 2022-05-18 23:58:03,419 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:58:03,419 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:58:03,419 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            " 85% 26257/31000 [7:44:28<43:05,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.4354536235332489, 'eval_runtime': 3.8051, 'eval_samples_per_second': 57.554, 'eval_steps_per_second': 1.051, 'epoch': 847.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-18 23:58:07,226 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26257\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:58:07,227 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26257/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:58:09,171 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26257/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:58:09,172 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26257/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:58:14,101 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26195] due to args.save_total_limit\n",
            "                                             {'loss': 0.4436, 'learning_rate': 2.3463034365389663e-06, 'epoch': 847.1}\n",
            "{'loss': 0.4404, 'learning_rate': 2.3366245926300422e-06, 'epoch': 847.42}\n",
            "{'loss': 0.4487, 'learning_rate': 2.3269644265765248e-06, 'epoch': 847.74}\n",
            " 85% 26288/31000 [7:44:59<42:31,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:58:38,305 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:58:38,306 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:58:38,306 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                           \n",
            " 85% 26288/31000 [7:45:03<42:31,  1.85it/s]{'eval_loss': 0.41941845417022705, 'eval_runtime': 3.824, 'eval_samples_per_second': 57.27, 'eval_steps_per_second': 1.046, 'epoch': 848.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:58:42,131 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26288\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:58:42,133 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26288/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:58:44,029 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26288/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:58:44,030 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26288/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:58:48,937 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-24738] due to args.save_total_limit\n",
            " 85% 26290/31000 [7:45:15<4:39:52,  3.57s/it]{'loss': 0.4436, 'learning_rate': 2.317322949371334e-06, 'epoch': 848.06}\n",
            "{'loss': 0.4415, 'learning_rate': 2.307700171986145e-06, 'epoch': 848.39}\n",
            "{'loss': 0.4393, 'learning_rate': 2.298096105371344e-06, 'epoch': 848.71}\n",
            " 85% 26319/31000 [7:45:31<42:12,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:59:10,913 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:59:10,913 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:59:10,913 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A{'eval_loss': 0.43085193634033203, 'eval_runtime': 3.7701, 'eval_samples_per_second': 58.088, 'eval_steps_per_second': 1.061, 'epoch': 849.0}\n",
            " 85% 26319/31000 [7:45:35<42:12,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:59:14,685 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26319\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:59:14,687 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26319/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:59:16,567 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26319/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:59:16,568 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26319/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:59:21,538 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26226] due to args.save_total_limit\n",
            " 85% 26320/31000 [7:45:46<6:20:25,  4.88s/it]{'loss': 0.4319, 'learning_rate': 2.2885107604560305e-06, 'epoch': 849.03}\n",
            "                                           {'loss': 0.4403, 'learning_rate': 2.2789441481479965e-06, 'epoch': 849.35}\n",
            " 85% 26340/31000 [7:45:58<44:38,  1.74it/s]{'loss': 0.4438, 'learning_rate': 2.2693962793337157e-06, 'epoch': 849.68}\n",
            "                                           {'loss': 0.4464, 'learning_rate': 2.2598671648783317e-06, 'epoch': 850.0}\n",
            " 85% 26350/31000 [7:46:04<41:55,  1.85it/s][INFO|trainer.py:2625] 2022-05-18 23:59:43,513 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-18 23:59:43,513 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-18 23:59:43,513 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.55it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4265633225440979, 'eval_runtime': 3.7849, 'eval_samples_per_second': 57.862, 'eval_steps_per_second': 1.057, 'epoch': 850.0}\n",
            " 85% 26350/31000 [7:46:08<41:55,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.71it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-18 23:59:47,300 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26350\n",
            "[INFO|configuration_utils.py:446] 2022-05-18 23:59:47,301 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26350/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-18 23:59:49,229 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26350/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-18 23:59:49,230 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26350/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-18 23:59:54,366 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26257] due to args.save_total_limit\n",
            "                                           {'loss': 0.4387, 'learning_rate': 2.25035681562565e-06, 'epoch': 850.32}\n",
            " 85% 26370/31000 [7:46:31<44:56,  1.72it/s]{'loss': 0.4375, 'learning_rate': 2.240865242398122e-06, 'epoch': 850.65}\n",
            "                                           {'loss': 0.4405, 'learning_rate': 2.2313924559968275e-06, 'epoch': 850.97}\n",
            " 85% 26381/31000 [7:46:37<43:25,  1.77it/s][INFO|trainer.py:2625] 2022-05-19 00:00:16,367 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:00:16,367 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:00:16,367 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4291066825389862, 'eval_runtime': 3.8082, 'eval_samples_per_second': 57.508, 'eval_steps_per_second': 1.05, 'epoch': 851.0}\n",
            " 85% 26381/31000 [7:46:41<43:25,  1.77it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:00:20,177 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26381\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:00:20,179 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26381/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:00:22,102 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26381/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:00:22,103 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26381/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:00:26,880 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26319] due to args.save_total_limit\n",
            " 85% 26390/31000 [7:46:57<1:05:30,  1.17it/s]{'loss': 0.4396, 'learning_rate': 2.221938467201474e-06, 'epoch': 851.29}\n",
            " 85% 26400/31000 [7:47:02<44:36,  1.72it/s]{'loss': 0.4292, 'learning_rate': 2.2125032867703784e-06, 'epoch': 851.61}\n",
            "                                           {'loss': 0.4467, 'learning_rate': 2.2030869254404415e-06, 'epoch': 851.94}\n",
            " 85% 26412/31000 [7:47:09<42:19,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 00:00:48,793 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:00:48,793 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:00:48,793 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.83it/s]\u001b[A\n",
            "                                           \n",
            " 85% 26412/31000 [7:47:13<42:19,  1.81it/s]\n",
            "{'eval_loss': 0.42252469062805176, 'eval_runtime': 3.761, 'eval_samples_per_second': 58.229, 'eval_steps_per_second': 1.064, 'epoch': 852.0}\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:00:52,556 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26412\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:00:52,558 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26412/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:00:54,467 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26412/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:00:54,468 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26412/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:00:59,179 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26350] due to args.save_total_limit\n",
            " 85% 26420/31000 [7:47:28<1:11:27,  1.07it/s]{'loss': 0.4392, 'learning_rate': 2.193689393927162e-06, 'epoch': 852.26}\n",
            "{'loss': 0.4355, 'learning_rate': 2.1843107029246077e-06, 'epoch': 852.58}\n",
            " 85% 26440/31000 [7:47:40<41:55,  1.81it/s]{'loss': 0.4413, 'learning_rate': 2.1749508631054028e-06, 'epoch': 852.9}\n",
            " 85% 26443/31000 [7:47:41<41:41,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:01:21,062 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:01:21,062 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:01:21,062 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4250400960445404, 'eval_runtime': 3.7274, 'eval_samples_per_second': 58.754, 'eval_steps_per_second': 1.073, 'epoch': 853.0}\n",
            " 85% 26443/31000 [7:47:45<41:41,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:01:24,791 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26443\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:01:24,793 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26443/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:01:26,637 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26443/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:01:26,638 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26443/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:01:31,621 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26381] due to args.save_total_limit\n",
            " 85% 26450/31000 [7:48:00<1:23:06,  1.10s/it]{'loss': 0.4308, 'learning_rate': 2.1656098851207317e-06, 'epoch': 853.23}\n",
            " 85% 26460/31000 [7:48:06<43:46,  1.73it/s]{'loss': 0.4413, 'learning_rate': 2.1562877796002895e-06, 'epoch': 853.55}\n",
            " 85% 26470/31000 [7:48:12<42:20,  1.78it/s]{'loss': 0.4404, 'learning_rate': 2.146984557152326e-06, 'epoch': 853.87}\n",
            " 85% 26474/31000 [7:48:14<41:19,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:01:53,539 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:01:53,540 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:01:53,540 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                           \n",
            " 85% 26474/31000 [7:48:18<41:19,  1.83it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.43142348527908325, 'eval_runtime': 3.7716, 'eval_samples_per_second': 58.066, 'eval_steps_per_second': 1.061, 'epoch': 854.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 00:01:57,313 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26474\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:01:57,315 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26474/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:01:59,241 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26474/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:01:59,242 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26474/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:02:05,511 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26412] due to args.save_total_limit\n",
            "                                             {'loss': 0.441, 'learning_rate': 2.1377002283635786e-06, 'epoch': 854.19}\n",
            " 85% 26490/31000 [7:48:40<45:18,  1.66it/s]{'loss': 0.4425, 'learning_rate': 2.1284348037993005e-06, 'epoch': 854.52}\n",
            " 85% 26500/31000 [7:48:45<43:13,  1.74it/s]{'loss': 0.4413, 'learning_rate': 2.1191882940032137e-06, 'epoch': 854.84}\n",
            " 86% 26505/31000 [7:48:48<40:43,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:02:27,343 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:02:27,343 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:02:27,343 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.56it/s]\u001b[A\n",
            "                                           \n",
            " 86% 26505/31000 [7:48:52<40:43,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.42731255292892456, 'eval_runtime': 3.7702, 'eval_samples_per_second': 58.086, 'eval_steps_per_second': 1.061, 'epoch': 855.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 00:02:31,115 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26505\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:02:31,117 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26505/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:02:33,006 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26505/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:02:33,007 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26505/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:02:37,746 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26443] due to args.save_total_limit\n",
            " 86% 26510/31000 [7:49:05<2:01:35,  1.62s/it]{'loss': 0.4504, 'learning_rate': 2.1099607094975364e-06, 'epoch': 855.16}\n",
            "{'loss': 0.4288, 'learning_rate': 2.100752060782947e-06, 'epoch': 855.48}\n",
            " 86% 26530/31000 [7:49:17<42:34,  1.75it/s]{'loss': 0.4441, 'learning_rate': 2.0915623583385574e-06, 'epoch': 855.81}\n",
            " 86% 26536/31000 [7:49:20<40:22,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:02:59,618 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:02:59,619 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:02:59,619 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "{'eval_loss': 0.4279472529888153, 'eval_runtime': 3.7586, 'eval_samples_per_second': 58.267, 'eval_steps_per_second': 1.064, 'epoch': 856.0}\n",
            "\n",
            " 86% 26536/31000 [7:49:24<40:22,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:03:03,379 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26536\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:03:03,381 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26536/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:03:05,247 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26536/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:03:05,248 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26536/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:03:09,734 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26474] due to args.save_total_limit\n",
            " 86% 26540/31000 [7:49:37<2:29:06,  2.01s/it]{'loss': 0.4323, 'learning_rate': 2.0823916126219414e-06, 'epoch': 856.13}\n",
            " 86% 26550/31000 [7:49:42<45:21,  1.63it/s]{'loss': 0.4521, 'learning_rate': 2.0732398340690855e-06, 'epoch': 856.45}\n",
            "{'loss': 0.4357, 'learning_rate': 2.064107033094407e-06, 'epoch': 856.77}\n",
            " 86% 26567/31000 [7:49:52<40:16,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:03:31,606 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:03:31,607 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:03:31,607 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.42578214406967163, 'eval_runtime': 3.7596, 'eval_samples_per_second': 58.25, 'eval_steps_per_second': 1.064, 'epoch': 857.0}\n",
            " 86% 26567/31000 [7:49:56<40:16,  1.83it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:03:35,368 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26567\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:03:35,370 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26567/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:03:37,281 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26567/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:03:37,282 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26567/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:03:41,898 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26505] due to args.save_total_limit\n",
            "                                             {'loss': 0.4379, 'learning_rate': 2.054993220090708e-06, 'epoch': 857.1}\n",
            "{'loss': 0.4479, 'learning_rate': 2.0458984054291935e-06, 'epoch': 857.42}\n",
            "                                           {'loss': 0.4411, 'learning_rate': 2.0368225994594625e-06, 'epoch': 857.74}\n",
            " 86% 26598/31000 [7:50:24<39:47,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:04:03,943 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:04:03,943 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:04:03,943 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.428055077791214, 'eval_runtime': 3.7795, 'eval_samples_per_second': 57.945, 'eval_steps_per_second': 1.058, 'epoch': 858.0}\n",
            " 86% 26598/31000 [7:50:28<39:47,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:04:07,724 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26598\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:04:07,726 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26598/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:04:09,635 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26598/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:04:09,636 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26598/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:04:14,236 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26536] due to args.save_total_limit\n",
            " 86% 26600/31000 [7:50:39<4:17:34,  3.51s/it]{'loss': 0.445, 'learning_rate': 2.027765812509457e-06, 'epoch': 858.06}\n",
            " 86% 26610/31000 [7:50:45<48:44,  1.50it/s]{'loss': 0.4448, 'learning_rate': 2.0187280548854914e-06, 'epoch': 858.39}\n",
            "{'loss': 0.4374, 'learning_rate': 2.009709336872222e-06, 'epoch': 858.71}\n",
            " 86% 26629/31000 [7:50:56<39:29,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:04:36,133 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:04:36,133 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:04:36,133 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            " 86% 26629/31000 [7:51:01<39:29,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A{'eval_loss': 0.42937928438186646, 'eval_runtime': 3.7608, 'eval_samples_per_second': 58.232, 'eval_steps_per_second': 1.064, 'epoch': 859.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:04:39,895 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26629\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:04:39,897 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26629/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:04:41,767 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26629/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:04:41,768 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26629/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:04:46,418 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26567] due to args.save_total_limit\n",
            "{'loss': 0.4301, 'learning_rate': 2.0007096687326438e-06, 'epoch': 859.03}\n",
            " 86% 26640/31000 [7:51:18<51:16,  1.42it/s]{'loss': 0.4375, 'learning_rate': 1.9917290607080622e-06, 'epoch': 859.35}\n",
            " 86% 26650/31000 [7:51:23<41:18,  1.76it/s]{'loss': 0.4431, 'learning_rate': 1.9827675230181027e-06, 'epoch': 859.68}\n",
            "                                           {'loss': 0.4504, 'learning_rate': 1.973825065860685e-06, 'epoch': 860.0}\n",
            " 86% 26660/31000 [7:51:29<38:56,  1.86it/s][INFO|trainer.py:2625] 2022-05-19 00:05:08,223 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:05:08,223 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:05:08,223 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A{'eval_loss': 0.4329756498336792, 'eval_runtime': 3.8024, 'eval_samples_per_second': 57.596, 'eval_steps_per_second': 1.052, 'epoch': 860.0}\n",
            " 86% 26660/31000 [7:51:33<38:56,  1.86it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:05:12,027 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26660\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:05:12,029 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26660/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:05:13,936 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26660/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:05:13,937 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26660/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:05:18,602 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26598] due to args.save_total_limit\n",
            " 86% 26670/31000 [7:51:49<54:54,  1.31it/s]{'loss': 0.4352, 'learning_rate': 1.964901699412018e-06, 'epoch': 860.32}\n",
            " 86% 26680/31000 [7:51:55<42:06,  1.71it/s]{'loss': 0.4471, 'learning_rate': 1.95599743382659e-06, 'epoch': 860.65}\n",
            " 86% 26690/31000 [7:52:00<38:46,  1.85it/s]{'loss': 0.4536, 'learning_rate': 1.9471122792371334e-06, 'epoch': 860.97}\n",
            " 86% 26691/31000 [7:52:01<40:14,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 00:05:40,490 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:05:40,490 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:05:40,490 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "                                           \n",
            " 86% 26691/31000 [7:52:05<40:14,  1.78it/s]{'eval_loss': 0.42218658328056335, 'eval_runtime': 3.7656, 'eval_samples_per_second': 58.158, 'eval_steps_per_second': 1.062, 'epoch': 861.0}\n",
            "\n",
            "100% 4/4 [00:00<00:00,  8.96it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:05:44,257 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26691\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:05:44,259 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26691/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:05:46,191 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26691/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:05:46,192 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26691/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:05:50,919 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26629] due to args.save_total_limit\n",
            "{'loss': 0.4405, 'learning_rate': 1.938246245754668e-06, 'epoch': 861.29}\n",
            "{'loss': 0.4396, 'learning_rate': 1.9293993434684176e-06, 'epoch': 861.61}\n",
            " 86% 26720/31000 [7:52:32<38:59,  1.83it/s]{'loss': 0.443, 'learning_rate': 1.9205715824458603e-06, 'epoch': 861.94}\n",
            " 86% 26722/31000 [7:52:33<39:34,  1.80it/s][INFO|trainer.py:2625] 2022-05-19 00:06:12,854 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:06:12,855 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:06:12,855 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.42825278639793396, 'eval_runtime': 3.793, 'eval_samples_per_second': 57.737, 'eval_steps_per_second': 1.055, 'epoch': 862.0}\n",
            " 86% 26722/31000 [7:52:37<39:34,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:06:16,649 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26722\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:06:16,651 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26722/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:06:18,578 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26722/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:06:18,579 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26722/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:06:22,975 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26660] due to args.save_total_limit\n",
            "                                             {'loss': 0.4391, 'learning_rate': 1.9117629727326805e-06, 'epoch': 862.26}\n",
            "                                           {'loss': 0.4408, 'learning_rate': 1.9029735243527752e-06, 'epoch': 862.58}\n",
            " 86% 26750/31000 [7:53:03<39:04,  1.81it/s]{'loss': 0.4436, 'learning_rate': 1.8942032473082347e-06, 'epoch': 862.9}\n",
            " 86% 26753/31000 [7:53:05<38:59,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:06:44,853 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:06:44,853 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:06:44,853 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                           \n",
            " 86% 26753/31000 [7:53:09<38:59,  1.82it/s]{'eval_loss': 0.4321753680706024, 'eval_runtime': 3.8545, 'eval_samples_per_second': 56.817, 'eval_steps_per_second': 1.038, 'epoch': 863.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:06:48,709 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26753\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:06:48,711 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26753/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:06:50,626 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26753/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:06:50,627 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26753/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:06:55,225 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26691] due to args.save_total_limit\n",
            "{'loss': 0.4442, 'learning_rate': 1.8854521515793281e-06, 'epoch': 863.23}\n",
            "{'loss': 0.4424, 'learning_rate': 1.8767202471244997e-06, 'epoch': 863.55}\n",
            " 86% 26780/31000 [7:53:37<38:28,  1.83it/s]{'loss': 0.428, 'learning_rate': 1.8680075438803593e-06, 'epoch': 863.87}\n",
            " 86% 26784/31000 [7:53:40<38:05,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:07:19,473 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:07:19,474 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:07:19,474 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            " 86% 26784/31000 [7:53:44<38:05,  1.84it/s]\n",
            "{'eval_loss': 0.4289509057998657, 'eval_runtime': 3.7624, 'eval_samples_per_second': 58.208, 'eval_steps_per_second': 1.063, 'epoch': 864.0}\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:07:23,238 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26784\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:07:23,239 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26784/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:07:25,102 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26784/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:07:25,103 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26784/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:07:29,614 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26722] due to args.save_total_limit\n",
            " 86% 26790/31000 [7:53:58<1:30:11,  1.29s/it]{'loss': 0.4472, 'learning_rate': 1.8593140517616615e-06, 'epoch': 864.19}\n",
            " 86% 26800/31000 [7:54:04<41:43,  1.68it/s]{'loss': 0.4371, 'learning_rate': 1.8506397806613053e-06, 'epoch': 864.52}\n",
            " 86% 26810/31000 [7:54:09<41:17,  1.69it/s]{'loss': 0.4492, 'learning_rate': 1.8419847404502988e-06, 'epoch': 864.84}\n",
            " 86% 26815/31000 [7:54:12<38:20,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:07:51,660 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:07:51,660 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:07:51,660 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                           \n",
            " 86% 26815/31000 [7:54:16<38:20,  1.82it/s]\n",
            "{'eval_loss': 0.43605223298072815, 'eval_runtime': 3.799, 'eval_samples_per_second': 57.647, 'eval_steps_per_second': 1.053, 'epoch': 865.0}\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:07:55,461 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26815\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:07:55,463 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26815/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:07:57,373 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26815/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:07:57,374 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26815/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:08:01,957 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26753] due to args.save_total_limit\n",
            " 87% 26820/31000 [7:54:29<1:53:13,  1.63s/it]{'loss': 0.4391, 'learning_rate': 1.8333489409777968e-06, 'epoch': 865.16}\n",
            " 87% 26830/31000 [7:54:36<42:10,  1.65it/s]{'loss': 0.4443, 'learning_rate': 1.824732392071028e-06, 'epoch': 865.48}\n",
            "                                           {'loss': 0.4354, 'learning_rate': 1.8161351035353344e-06, 'epoch': 865.81}\n",
            " 87% 26846/31000 [7:54:44<37:27,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:08:23,833 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:08:23,834 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:08:23,834 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.85it/s]\u001b[A\n",
            "                                           \n",
            " 87% 26846/31000 [7:54:48<37:27,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A{'eval_loss': 0.4215284585952759, 'eval_runtime': 3.7003, 'eval_samples_per_second': 59.184, 'eval_steps_per_second': 1.081, 'epoch': 866.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:08:27,536 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26846\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:08:27,537 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26846/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:08:29,448 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26846/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:08:29,449 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26846/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:08:33,844 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26784] due to args.save_total_limit\n",
            "{'loss': 0.4404, 'learning_rate': 1.8075570851541313e-06, 'epoch': 866.13}\n",
            "                                           {'loss': 0.443, 'learning_rate': 1.798998346688911e-06, 'epoch': 866.45}\n",
            "                                           {'loss': 0.4429, 'learning_rate': 1.790458897879224e-06, 'epoch': 866.77}\n",
            " 87% 26877/31000 [7:55:16<37:28,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:08:55,708 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:08:55,708 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:08:55,708 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.58it/s]\u001b[A\n",
            "                                           \n",
            " 87% 26877/31000 [7:55:20<37:28,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A{'eval_loss': 0.4319177269935608, 'eval_runtime': 3.7692, 'eval_samples_per_second': 58.102, 'eval_steps_per_second': 1.061, 'epoch': 867.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:08:59,479 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26877\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:08:59,480 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26877/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:09:01,393 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26877/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:09:01,394 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26877/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:09:05,933 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26815] due to args.save_total_limit\n",
            " 87% 26880/31000 [7:55:32<3:00:22,  2.63s/it]{'loss': 0.4455, 'learning_rate': 1.7819387484426615e-06, 'epoch': 867.1}\n",
            " 87% 26890/31000 [7:55:38<43:19,  1.58it/s]{'loss': 0.4423, 'learning_rate': 1.7734379080748636e-06, 'epoch': 867.42}\n",
            " 87% 26900/31000 [7:55:44<39:13,  1.74it/s]{'loss': 0.4309, 'learning_rate': 1.7649563864494924e-06, 'epoch': 867.74}\n",
            " 87% 26908/31000 [7:55:48<36:45,  1.86it/s][INFO|trainer.py:2625] 2022-05-19 00:09:27,861 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:09:27,861 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:09:27,861 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                           \n",
            " 87% 26908/31000 [7:55:52<36:45,  1.86it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "{'eval_loss': 0.42825332283973694, 'eval_runtime': 3.6851, 'eval_samples_per_second': 59.428, 'eval_steps_per_second': 1.085, 'epoch': 868.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:09:31,548 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26908\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:09:31,550 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26908/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:09:33,433 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26908/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:09:33,434 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26908/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:09:37,780 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26846] due to args.save_total_limit\n",
            "{'loss': 0.4461, 'learning_rate': 1.7564941932182358e-06, 'epoch': 868.06}\n",
            " 87% 26920/31000 [7:56:10<45:05,  1.51it/s]{'loss': 0.4421, 'learning_rate': 1.7480513380107613e-06, 'epoch': 868.39}\n",
            " 87% 26930/31000 [7:56:15<38:31,  1.76it/s]{'loss': 0.4377, 'learning_rate': 1.7396278304347608e-06, 'epoch': 868.71}\n",
            " 87% 26939/31000 [7:56:20<36:35,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:09:59,708 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:09:59,708 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:09:59,708 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "                                           \n",
            " 87% 26939/31000 [7:56:24<36:35,  1.85it/s]\n",
            "{'eval_loss': 0.4340894818305969, 'eval_runtime': 3.7863, 'eval_samples_per_second': 57.84, 'eval_steps_per_second': 1.056, 'epoch': 869.0}\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:10:03,496 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26939\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:10:03,498 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26939/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:10:05,421 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26939/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:10:05,421 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26939/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:10:10,219 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26877] due to args.save_total_limit\n",
            " 87% 26940/31000 [7:56:35<5:28:04,  4.85s/it]{'loss': 0.4422, 'learning_rate': 1.731223680075896e-06, 'epoch': 869.03}\n",
            "                                           {'loss': 0.4404, 'learning_rate': 1.722838896497789e-06, 'epoch': 869.35}\n",
            "                                           {'loss': 0.4466, 'learning_rate': 1.7144734892420436e-06, 'epoch': 869.68}\n",
            "{'loss': 0.4397, 'learning_rate': 1.7061274678282028e-06, 'epoch': 870.0}\n",
            " 87% 26970/31000 [7:56:53<36:12,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:10:32,103 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:10:32,103 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:10:32,103 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            " 87% 26970/31000 [7:56:56<36:12,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "{'eval_loss': 0.4304685592651367, 'eval_runtime': 3.7743, 'eval_samples_per_second': 58.025, 'eval_steps_per_second': 1.06, 'epoch': 870.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:10:35,879 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26970\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:10:35,881 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26970/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:10:37,794 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26970/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:10:37,795 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26970/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:10:42,753 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26908] due to args.save_total_limit\n",
            " 87% 26980/31000 [7:57:13<51:37,  1.30it/s]{'loss': 0.4468, 'learning_rate': 1.6978008417537548e-06, 'epoch': 870.32}\n",
            "                                           {'loss': 0.4327, 'learning_rate': 1.6894936204941034e-06, 'epoch': 870.65}\n",
            "{'loss': 0.4385, 'learning_rate': 1.6812058135025827e-06, 'epoch': 870.97}\n",
            " 87% 27001/31000 [7:57:25<37:31,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 00:11:04,646 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:11:04,646 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:11:04,646 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "{'eval_loss': 0.4296017289161682, 'eval_runtime': 3.7424, 'eval_samples_per_second': 58.519, 'eval_steps_per_second': 1.069, 'epoch': 871.0}\n",
            "                                           \n",
            " 87% 27001/31000 [7:57:29<37:31,  1.78it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:11:08,390 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27001\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:11:08,392 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27001/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:11:10,325 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27001/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:11:10,326 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27001/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:11:14,787 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26939] due to args.save_total_limit\n",
            "                                           {'loss': 0.4348, 'learning_rate': 1.672937430210442e-06, 'epoch': 871.29}\n",
            " 87% 27020/31000 [7:57:50<38:47,  1.71it/s]{'loss': 0.4494, 'learning_rate': 1.6646884800268053e-06, 'epoch': 871.61}\n",
            "{'loss': 0.4314, 'learning_rate': 1.6564589723386965e-06, 'epoch': 871.94}\n",
            " 87% 27032/31000 [7:57:57<36:45,  1.80it/s][INFO|trainer.py:2625] 2022-05-19 00:11:36,777 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:11:36,777 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:11:36,777 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "\u001b[A{'eval_loss': 0.42926156520843506, 'eval_runtime': 3.7602, 'eval_samples_per_second': 58.242, 'eval_steps_per_second': 1.064, 'epoch': 872.0}\n",
            "                                           \n",
            " 87% 27032/31000 [7:58:01<36:45,  1.80it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:11:40,539 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27032\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:11:40,541 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27032/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:11:42,511 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27032/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:11:42,512 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27032/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:11:46,883 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26970] due to args.save_total_limit\n",
            "                                             {'loss': 0.4403, 'learning_rate': 1.6482489165110143e-06, 'epoch': 872.26}\n",
            "                                           {'loss': 0.4436, 'learning_rate': 1.6400583218865185e-06, 'epoch': 872.58}\n",
            "{'loss': 0.4447, 'learning_rate': 1.6318871977858181e-06, 'epoch': 872.9}\n",
            " 87% 27063/31000 [7:58:29<36:00,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:12:08,793 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:12:08,794 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:12:08,794 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 {'eval_loss': 0.43385809659957886, 'eval_runtime': 3.7456, 'eval_samples_per_second': 58.468, 'eval_steps_per_second': 1.068, 'epoch': 873.0}\n",
            "                                           \n",
            " 87% 27063/31000 [7:58:33<36:00,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:12:12,541 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27063\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:12:12,543 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27063/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:12:14,427 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27063/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:12:14,429 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27063/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:12:18,759 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27001] due to args.save_total_limit\n",
            "                                             {'loss': 0.4462, 'learning_rate': 1.623735553507377e-06, 'epoch': 873.23}\n",
            " 87% 27080/31000 [7:58:53<38:11,  1.71it/s]{'loss': 0.4319, 'learning_rate': 1.6156033983274814e-06, 'epoch': 873.55}\n",
            "{'loss': 0.4419, 'learning_rate': 1.6074907415002454e-06, 'epoch': 873.87}\n",
            " 87% 27094/31000 [7:59:01<35:32,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:12:40,633 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:12:40,633 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:12:40,633 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A{'eval_loss': 0.43193235993385315, 'eval_runtime': 3.7782, 'eval_samples_per_second': 57.963, 'eval_steps_per_second': 1.059, 'epoch': 874.0}\n",
            "                                           \n",
            " 87% 27094/31000 [7:59:05<35:32,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:12:44,413 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27094\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:12:44,416 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27094/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:12:46,456 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27094/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:12:46,457 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27094/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:12:51,125 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27032] due to args.save_total_limit\n",
            "{'loss': 0.4346, 'learning_rate': 1.5993975922575946e-06, 'epoch': 874.19}\n",
            " 87% 27110/31000 [7:59:25<38:41,  1.68it/s]{'loss': 0.4463, 'learning_rate': 1.5913239598092473e-06, 'epoch': 874.52}\n",
            "{'loss': 0.4508, 'learning_rate': 1.5832698533427273e-06, 'epoch': 874.84}\n",
            " 88% 27125/31000 [7:59:33<35:14,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:13:13,179 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:13:13,179 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:13:13,180 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                           \n",
            " 88% 27125/31000 [7:59:38<35:14,  1.83it/s]\n",
            "                                 {'eval_loss': 0.42172175645828247, 'eval_runtime': 3.7838, 'eval_samples_per_second': 57.879, 'eval_steps_per_second': 1.057, 'epoch': 875.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 00:13:16,965 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27125\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:13:16,967 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27125/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:13:18,916 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27125/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:13:18,917 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27125/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:13:23,543 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27063] due to args.save_total_limit\n",
            "{'loss': 0.4338, 'learning_rate': 1.5752352820233259e-06, 'epoch': 875.16}\n",
            " 88% 27140/31000 [7:59:57<39:03,  1.65it/s]{'loss': 0.4326, 'learning_rate': 1.5672202549941067e-06, 'epoch': 875.48}\n",
            "                                           {'loss': 0.4516, 'learning_rate': 1.5592247813758965e-06, 'epoch': 875.81}\n",
            " 88% 27156/31000 [8:00:06<34:47,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:13:45,450 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:13:45,450 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:13:45,450 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                           \n",
            " 88% 27156/31000 [8:00:10<34:47,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "{'eval_loss': 0.4294445216655731, 'eval_runtime': 3.8198, 'eval_samples_per_second': 57.333, 'eval_steps_per_second': 1.047, 'epoch': 876.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:13:49,271 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27156\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:13:49,273 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27156/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:13:51,208 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27156/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:13:51,209 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27156/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:13:55,760 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27094] due to args.save_total_limit\n",
            "{'loss': 0.4343, 'learning_rate': 1.5512488702672674e-06, 'epoch': 876.13}\n",
            "{'loss': 0.4324, 'learning_rate': 1.543292530744536e-06, 'epoch': 876.45}\n",
            "                                           {'loss': 0.4519, 'learning_rate': 1.535355771861737e-06, 'epoch': 876.77}\n",
            " 88% 27187/31000 [8:00:38<34:42,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:14:17,815 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:14:17,815 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:14:17,815 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.24it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "                                           \n",
            " 88% 27187/31000 [8:00:42<34:42,  1.83it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.4292029142379761, 'eval_runtime': 3.8061, 'eval_samples_per_second': 57.539, 'eval_steps_per_second': 1.051, 'epoch': 877.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 00:14:21,623 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27187\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:14:21,625 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27187/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:14:23,561 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27187/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:14:23,562 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27187/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:14:27,983 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27125] due to args.save_total_limit\n",
            " 88% 27190/31000 [8:00:54<2:46:51,  2.63s/it]{'loss': 0.4445, 'learning_rate': 1.5274386026506268e-06, 'epoch': 877.1}\n",
            "{'loss': 0.4397, 'learning_rate': 1.5195410321206761e-06, 'epoch': 877.42}\n",
            " 88% 27210/31000 [8:01:06<36:35,  1.73it/s]{'loss': 0.4345, 'learning_rate': 1.5116630692590456e-06, 'epoch': 877.74}\n",
            " 88% 27218/31000 [8:01:10<34:03,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:14:50,094 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:14:50,095 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:14:50,095 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            " 88% 27218/31000 [8:01:14<34:03,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A{'eval_loss': 0.42903733253479004, 'eval_runtime': 3.7865, 'eval_samples_per_second': 57.837, 'eval_steps_per_second': 1.056, 'epoch': 878.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:14:53,883 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27218\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:14:53,885 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27218/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:14:55,839 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27218/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:14:55,840 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27218/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:15:00,737 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27156] due to args.save_total_limit\n",
            "{'loss': 0.4409, 'learning_rate': 1.5038047230305889e-06, 'epoch': 878.06}\n",
            " 88% 27230/31000 [8:01:32<41:55,  1.50it/s]{'loss': 0.4436, 'learning_rate': 1.495966002377822e-06, 'epoch': 878.39}\n",
            " 88% 27240/31000 [8:01:38<35:40,  1.76it/s]{'loss': 0.4292, 'learning_rate': 1.488146916220955e-06, 'epoch': 878.71}\n",
            " 88% 27249/31000 [8:01:43<33:51,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:15:22,626 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:15:22,626 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:15:22,626 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A{'eval_loss': 0.42232218384742737, 'eval_runtime': 3.8152, 'eval_samples_per_second': 57.401, 'eval_steps_per_second': 1.048, 'epoch': 879.0}\n",
            "                                           \n",
            " 88% 27249/31000 [8:01:47<33:51,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:15:26,443 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27249\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:15:26,445 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27249/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:15:28,378 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27249/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:15:28,380 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27249/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:15:33,505 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27187] due to args.save_total_limit\n",
            "                                             {'loss': 0.4405, 'learning_rate': 1.4803474734578268e-06, 'epoch': 879.03}\n",
            " 88% 27260/31000 [8:02:04<44:41,  1.39it/s]{'loss': 0.4412, 'learning_rate': 1.4725676829639376e-06, 'epoch': 879.35}\n",
            " 88% 27270/31000 [8:02:10<35:33,  1.75it/s]{'loss': 0.4362, 'learning_rate': 1.4648075535924203e-06, 'epoch': 879.68}\n",
            " 88% 27280/31000 [8:02:16<33:29,  1.85it/s]{'loss': 0.437, 'learning_rate': 1.4570670941740359e-06, 'epoch': 880.0}\n",
            " 88% 27280/31000 [8:02:16<33:29,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:15:55,440 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:15:55,440 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:15:55,440 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.54it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4319785535335541, 'eval_runtime': 3.8384, 'eval_samples_per_second': 57.054, 'eval_steps_per_second': 1.042, 'epoch': 880.0}\n",
            " 88% 27280/31000 [8:02:20<33:29,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:15:59,280 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27280\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:15:59,282 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27280/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:16:01,205 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27280/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:16:01,206 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27280/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:16:05,858 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27218] due to args.save_total_limit\n",
            "                                           {'loss': 0.4451, 'learning_rate': 1.4493463135171634e-06, 'epoch': 880.32}\n",
            "{'loss': 0.424, 'learning_rate': 1.4416452204077771e-06, 'epoch': 880.65}\n",
            "                                           {'loss': 0.4412, 'learning_rate': 1.4339638236094602e-06, 'epoch': 880.97}\n",
            " 88% 27311/31000 [8:02:48<34:42,  1.77it/s][INFO|trainer.py:2625] 2022-05-19 00:16:27,835 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:16:27,836 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:16:27,836 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "{'eval_loss': 0.4268897473812103, 'eval_runtime': 3.7579, 'eval_samples_per_second': 58.277, 'eval_steps_per_second': 1.064, 'epoch': 881.0}\n",
            "\n",
            " 88% 27311/31000 [8:02:52<34:42,  1.77it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:16:31,595 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27311\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:16:31,597 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27311/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:16:33,533 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27311/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:16:33,534 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27311/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:16:38,433 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27249] due to args.save_total_limit\n",
            " 88% 27320/31000 [8:03:08<52:40,  1.16it/s]{'loss': 0.4512, 'learning_rate': 1.426302131863377e-06, 'epoch': 881.29}\n",
            "{'loss': 0.4239, 'learning_rate': 1.4186601538882723e-06, 'epoch': 881.61}\n",
            "                                           {'loss': 0.4431, 'learning_rate': 1.4110378983804412e-06, 'epoch': 881.94}\n",
            " 88% 27342/31000 [8:03:21<34:21,  1.77it/s][INFO|trainer.py:2625] 2022-05-19 00:17:00,766 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:17:00,766 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:17:00,766 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                           \n",
            " 88% 27342/31000 [8:03:25<34:21,  1.77it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]{'eval_loss': 0.4260638654232025, 'eval_runtime': 3.7951, 'eval_samples_per_second': 57.706, 'eval_steps_per_second': 1.054, 'epoch': 882.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:17:04,563 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27342\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:17:04,565 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27342/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:17:06,493 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27342/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:17:06,494 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27342/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:17:11,402 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27280] due to args.save_total_limit\n",
            " 88% 27350/31000 [8:03:41<57:51,  1.05it/s]{'loss': 0.4457, 'learning_rate': 1.4034353740137624e-06, 'epoch': 882.26}\n",
            "{'loss': 0.4361, 'learning_rate': 1.3958525894396424e-06, 'epoch': 882.58}\n",
            " 88% 27370/31000 [8:03:52<33:27,  1.81it/s]{'loss': 0.4469, 'learning_rate': 1.3882895532870245e-06, 'epoch': 882.9}\n",
            " 88% 27373/31000 [8:03:54<33:16,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:17:33,634 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:17:33,634 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:17:33,634 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.32it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4196995198726654, 'eval_runtime': 3.8396, 'eval_samples_per_second': 57.037, 'eval_steps_per_second': 1.042, 'epoch': 883.0}\n",
            " 88% 27373/31000 [8:03:58<33:16,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:17:37,475 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27373\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:17:37,477 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27373/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:17:39,403 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27373/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:17:39,404 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27373/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:17:43,873 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27311] due to args.save_total_limit\n",
            " 88% 27380/31000 [8:04:13<1:07:13,  1.11s/it]{'loss': 0.4345, 'learning_rate': 1.3807462741623831e-06, 'epoch': 883.23}\n",
            " 88% 27390/31000 [8:04:19<35:31,  1.69it/s]{'loss': 0.446, 'learning_rate': 1.3732227606497135e-06, 'epoch': 883.55}\n",
            "                                           {'loss': 0.4359, 'learning_rate': 1.3657190213105178e-06, 'epoch': 883.87}\n",
            " 88% 27404/31000 [8:04:26<32:43,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:18:06,069 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:18:06,070 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:18:06,070 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            " 88% 27404/31000 [8:04:31<32:43,  1.83it/s]\n",
            "{'eval_loss': 0.4248308539390564, 'eval_runtime': 3.8185, 'eval_samples_per_second': 57.352, 'eval_steps_per_second': 1.048, 'epoch': 884.0}\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:18:09,890 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27404\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:18:09,892 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27404/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:18:11,790 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27404/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:18:11,791 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27404/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:18:16,624 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27342] due to args.save_total_limit\n",
            " 88% 27410/31000 [8:04:45<1:18:16,  1.31s/it]{'loss': 0.4454, 'learning_rate': 1.358235064683784e-06, 'epoch': 884.19}\n",
            " 88% 27420/31000 [8:04:51<36:15,  1.65it/s]{'loss': 0.4546, 'learning_rate': 1.3507708992859973e-06, 'epoch': 884.52}\n",
            "{'loss': 0.43, 'learning_rate': 1.3433265336111304e-06, 'epoch': 884.84}\n",
            " 88% 27435/31000 [8:04:59<32:20,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:18:38,556 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:18:38,556 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:18:38,557 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.60it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4278773367404938, 'eval_runtime': 3.7832, 'eval_samples_per_second': 57.888, 'eval_steps_per_second': 1.057, 'epoch': 885.0}\n",
            " 88% 27435/31000 [8:05:03<32:20,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:18:42,341 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27435\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:18:42,343 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27435/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:18:44,249 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27435/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:18:44,250 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27435/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:18:48,773 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27373] due to args.save_total_limit\n",
            " 89% 27440/31000 [8:05:16<1:36:06,  1.62s/it]{'loss': 0.4385, 'learning_rate': 1.3359019761306065e-06, 'epoch': 885.16}\n",
            " 89% 27450/31000 [8:05:22<36:55,  1.60it/s]{'loss': 0.4424, 'learning_rate': 1.3284972352933215e-06, 'epoch': 885.48}\n",
            " 89% 27460/31000 [8:05:28<33:47,  1.75it/s]{'loss': 0.4392, 'learning_rate': 1.3211123195256058e-06, 'epoch': 885.81}\n",
            " 89% 27466/31000 [8:05:31<31:59,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:19:10,828 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:19:10,828 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:19:10,828 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "{'eval_loss': 0.4257446825504303, 'eval_runtime': 3.7509, 'eval_samples_per_second': 58.386, 'eval_steps_per_second': 1.066, 'epoch': 886.0}\n",
            "\n",
            " 89% 27466/31000 [8:05:35<31:59,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:19:14,581 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27466\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:19:14,583 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27466/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:19:16,485 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27466/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:19:16,486 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27466/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:19:21,050 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27404] due to args.save_total_limit\n",
            "                                             {'loss': 0.4379, 'learning_rate': 1.3137472372312497e-06, 'epoch': 886.13}\n",
            " 89% 27480/31000 [8:05:53<37:07,  1.58it/s]{'loss': 0.4453, 'learning_rate': 1.306401996791457e-06, 'epoch': 886.45}\n",
            "{'loss': 0.4392, 'learning_rate': 1.2990766065648584e-06, 'epoch': 886.77}\n",
            " 89% 27497/31000 [8:06:03<31:42,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:19:43,154 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:19:43,154 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:19:43,154 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4249756634235382, 'eval_runtime': 3.7523, 'eval_samples_per_second': 58.363, 'eval_steps_per_second': 1.066, 'epoch': 887.0}\n",
            " 89% 27497/31000 [8:06:08<31:42,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:19:46,908 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27497\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:19:46,910 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27497/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:19:48,887 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27497/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:19:48,888 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27497/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:19:53,067 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27435] due to args.save_total_limit\n",
            " 89% 27500/31000 [8:06:19<2:30:13,  2.58s/it]{'loss': 0.4326, 'learning_rate': 1.291771074887498e-06, 'epoch': 887.1}\n",
            "                                           {'loss': 0.4391, 'learning_rate': 1.2844854100728156e-06, 'epoch': 887.42}\n",
            " 89% 27520/31000 [8:06:31<33:28,  1.73it/s]{'loss': 0.4449, 'learning_rate': 1.2772196204116521e-06, 'epoch': 887.74}\n",
            " 89% 27528/31000 [8:06:35<31:17,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:20:14,992 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:20:14,992 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:20:14,992 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4365818500518799, 'eval_runtime': 3.7597, 'eval_samples_per_second': 58.249, 'eval_steps_per_second': 1.064, 'epoch': 888.0}\n",
            " 89% 27528/31000 [8:06:39<31:17,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:20:18,753 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27528\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:20:18,755 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27528/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:20:20,676 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27528/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:20:20,677 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27528/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:20:24,833 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27466] due to args.save_total_limit\n",
            " 89% 27530/31000 [8:06:50<3:17:47,  3.42s/it]{'loss': 0.4386, 'learning_rate': 1.2699737141722205e-06, 'epoch': 888.06}\n",
            " 89% 27540/31000 [8:06:56<38:01,  1.52it/s]{'loss': 0.4555, 'learning_rate': 1.2627476996001126e-06, 'epoch': 888.39}\n",
            "{'loss': 0.443, 'learning_rate': 1.2555415849182862e-06, 'epoch': 888.71}\n",
            " 89% 27559/31000 [8:07:07<30:54,  1.86it/s][INFO|trainer.py:2625] 2022-05-19 00:20:46,600 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:20:46,600 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:20:46,600 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "{'eval_loss': 0.42642417550086975, 'eval_runtime': 3.7517, 'eval_samples_per_second': 58.374, 'eval_steps_per_second': 1.066, 'epoch': 889.0}\n",
            "\n",
            " 89% 27559/31000 [8:07:11<30:54,  1.86it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:20:50,353 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27559\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:20:50,355 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27559/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:20:52,291 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27559/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:20:52,292 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27559/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:20:56,520 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27497] due to args.save_total_limit\n",
            " 89% 27560/31000 [8:07:21<4:28:23,  4.68s/it]{'loss': 0.4361, 'learning_rate': 1.2483553783270484e-06, 'epoch': 889.03}\n",
            " 89% 27570/31000 [8:07:27<40:02,  1.43it/s]{'loss': 0.4348, 'learning_rate': 1.2411890880040597e-06, 'epoch': 889.35}\n",
            "{'loss': 0.4449, 'learning_rate': 1.2340427221043053e-06, 'epoch': 889.68}\n",
            "{'loss': 0.4354, 'learning_rate': 1.226916288760111e-06, 'epoch': 890.0}\n",
            " 89% 27590/31000 [8:07:39<30:26,  1.87it/s][INFO|trainer.py:2625] 2022-05-19 00:21:18,527 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:21:18,527 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:21:18,527 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "{'eval_loss': 0.43205589056015015, 'eval_runtime': 3.7596, 'eval_samples_per_second': 58.25, 'eval_steps_per_second': 1.064, 'epoch': 890.0}\n",
            "\n",
            " 89% 27590/31000 [8:07:43<30:26,  1.87it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:21:22,288 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27590\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:21:22,290 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27590/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:21:24,181 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27590/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:21:24,182 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27590/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:21:28,560 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27528] due to args.save_total_limit\n",
            "                                           {'loss': 0.4487, 'learning_rate': 1.2198097960811025e-06, 'epoch': 890.32}\n",
            "                                           {'loss': 0.4375, 'learning_rate': 1.2127232521542317e-06, 'epoch': 890.65}\n",
            " 89% 27620/31000 [8:08:10<30:25,  1.85it/s]{'loss': 0.4322, 'learning_rate': 1.2056566650437353e-06, 'epoch': 890.97}\n",
            " 89% 27621/31000 [8:08:11<31:36,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 00:21:50,510 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:21:50,511 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:21:50,511 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.86it/s]{'eval_loss': 0.42711594700813293, 'eval_runtime': 3.7629, 'eval_samples_per_second': 58.199, 'eval_steps_per_second': 1.063, 'epoch': 891.0}\n",
            " 89% 27621/31000 [8:08:15<31:36,  1.78it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:21:54,276 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27621\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:21:54,277 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27621/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:21:56,163 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27621/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:21:56,164 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27621/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:22:00,861 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27559] due to args.save_total_limit\n",
            "                                           {'loss': 0.433, 'learning_rate': 1.1986100427911496e-06, 'epoch': 891.29}\n",
            " 89% 27640/31000 [8:08:37<32:51,  1.70it/s]{'loss': 0.437, 'learning_rate': 1.1915833934152894e-06, 'epoch': 891.61}\n",
            " 89% 27650/31000 [8:08:42<30:09,  1.85it/s]{'loss': 0.4566, 'learning_rate': 1.1845767249122313e-06, 'epoch': 891.94}\n",
            " 89% 27652/31000 [8:08:43<30:45,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 00:22:22,701 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:22:22,701 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:22:22,701 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4286835491657257, 'eval_runtime': 3.7106, 'eval_samples_per_second': 59.019, 'eval_steps_per_second': 1.078, 'epoch': 892.0}\n",
            " 89% 27652/31000 [8:08:47<30:45,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:22:26,413 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27652\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:22:26,415 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27652/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:22:28,318 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27652/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:22:28,319 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27652/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:22:33,002 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27590] due to args.save_total_limit\n",
            "{'loss': 0.4374, 'learning_rate': 1.1775900452553346e-06, 'epoch': 892.26}\n",
            " 89% 27670/31000 [8:09:08<31:52,  1.74it/s]{'loss': 0.4423, 'learning_rate': 1.17062336239519e-06, 'epoch': 892.58}\n",
            " 89% 27680/31000 [8:09:13<30:49,  1.80it/s]{'loss': 0.443, 'learning_rate': 1.163676684259645e-06, 'epoch': 892.9}\n",
            " 89% 27683/31000 [8:09:15<30:27,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 00:22:54,764 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:22:54,765 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:22:54,765 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.82it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A{'eval_loss': 0.42411935329437256, 'eval_runtime': 3.7428, 'eval_samples_per_second': 58.513, 'eval_steps_per_second': 1.069, 'epoch': 893.0}\n",
            " 89% 27683/31000 [8:09:19<30:27,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:22:58,509 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27683\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:22:58,511 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27683/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:23:00,470 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27683/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:23:00,471 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27683/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:23:04,774 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27621] due to args.save_total_limit\n",
            "                                           {'loss': 0.443, 'learning_rate': 1.1567500187537825e-06, 'epoch': 893.23}\n",
            "{'loss': 0.4418, 'learning_rate': 1.149843373759906e-06, 'epoch': 893.55}\n",
            "                                           {'loss': 0.4448, 'learning_rate': 1.1429567571375447e-06, 'epoch': 893.87}\n",
            " 89% 27714/31000 [8:09:47<29:49,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:23:26,690 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:23:26,690 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:23:26,690 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                           \n",
            " 89% 27714/31000 [8:09:51<29:49,  1.84it/s]\n",
            "                                 {'eval_loss': 0.4378966689109802, 'eval_runtime': 3.7767, 'eval_samples_per_second': 57.987, 'eval_steps_per_second': 1.059, 'epoch': 894.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 00:23:30,469 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27714\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:23:30,470 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27714/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:23:32,354 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27714/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:23:32,355 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27714/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:23:36,609 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27652] due to args.save_total_limit\n",
            "{'loss': 0.4354, 'learning_rate': 1.136090176723422e-06, 'epoch': 894.19}\n",
            "                                           {'loss': 0.4379, 'learning_rate': 1.1292436403314766e-06, 'epoch': 894.52}\n",
            " 89% 27740/31000 [8:10:17<31:00,  1.75it/s]{'loss': 0.4397, 'learning_rate': 1.1224171557528263e-06, 'epoch': 894.84}\n",
            " 90% 27745/31000 [8:10:19<29:26,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:23:58,558 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:23:58,558 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:23:58,558 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                           \n",
            " 90% 27745/31000 [8:10:23<29:26,  1.84it/s]{'eval_loss': 0.4276416301727295, 'eval_runtime': 3.7799, 'eval_samples_per_second': 57.938, 'eval_steps_per_second': 1.058, 'epoch': 895.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:24:02,340 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27745\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:24:02,342 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27745/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:24:04,237 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27745/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:24:04,238 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27745/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:24:08,582 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27683] due to args.save_total_limit\n",
            "{'loss': 0.4423, 'learning_rate': 1.1156107307557824e-06, 'epoch': 895.16}\n",
            " 90% 27760/31000 [8:10:42<33:12,  1.63it/s]{'loss': 0.4307, 'learning_rate': 1.1088243730858155e-06, 'epoch': 895.48}\n",
            "                                           {'loss': 0.4472, 'learning_rate': 1.102058090465563e-06, 'epoch': 895.81}\n",
            " 90% 27776/31000 [8:10:51<29:11,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:24:30,538 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:24:30,538 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:24:30,538 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.428697407245636, 'eval_runtime': 3.8107, 'eval_samples_per_second': 57.47, 'eval_steps_per_second': 1.05, 'epoch': 896.0}\n",
            " 90% 27776/31000 [8:10:55<29:11,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:24:34,350 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27776\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:24:34,352 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27776/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:24:36,265 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27776/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:24:36,266 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27776/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:24:40,976 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27714] due to args.save_total_limit\n",
            "                                             {'loss': 0.4374, 'learning_rate': 1.095311890594834e-06, 'epoch': 896.13}\n",
            " 90% 27790/31000 [8:11:13<34:02,  1.57it/s]{'loss': 0.4417, 'learning_rate': 1.088585781150561e-06, 'epoch': 896.45}\n",
            "                                           {'loss': 0.4518, 'learning_rate': 1.0818797697868275e-06, 'epoch': 896.77}\n",
            " 90% 27807/31000 [8:11:23<29:02,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:25:03,067 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:25:03,068 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:25:03,068 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                           \n",
            " 90% 27807/31000 [8:11:27<29:02,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A{'eval_loss': 0.42671382427215576, 'eval_runtime': 3.797, 'eval_samples_per_second': 57.678, 'eval_steps_per_second': 1.053, 'epoch': 897.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:25:06,866 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27807\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:25:06,868 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27807/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:25:08,773 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27807/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:25:08,774 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27807/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:25:13,094 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27745] due to args.save_total_limit\n",
            "{'loss': 0.4405, 'learning_rate': 1.0751938641348456e-06, 'epoch': 897.1}\n",
            "                                           {'loss': 0.4447, 'learning_rate': 1.068528071802944e-06, 'epoch': 897.42}\n",
            "                                           {'loss': 0.4292, 'learning_rate': 1.061882400376561e-06, 'epoch': 897.74}\n",
            " 90% 27838/31000 [8:11:55<28:28,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:25:35,014 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:25:35,014 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:25:35,014 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                           \n",
            " 90% 27838/31000 [8:11:59<28:28,  1.85it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.43498119711875916, 'eval_runtime': 3.7881, 'eval_samples_per_second': 57.812, 'eval_steps_per_second': 1.056, 'epoch': 898.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 00:25:38,804 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27838\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:25:38,806 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27838/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:25:40,741 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27838/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:25:40,742 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27838/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:25:45,144 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27776] due to args.save_total_limit\n",
            " 90% 27840/31000 [8:12:11<3:04:22,  3.50s/it]{'loss': 0.4404, 'learning_rate': 1.0552568574182456e-06, 'epoch': 898.06}\n",
            " 90% 27850/31000 [8:12:17<35:04,  1.50it/s]{'loss': 0.4289, 'learning_rate': 1.0486514504676372e-06, 'epoch': 898.39}\n",
            " 90% 27860/31000 [8:12:23<29:48,  1.76it/s]{'loss': 0.4417, 'learning_rate': 1.0420661870414584e-06, 'epoch': 898.71}\n",
            " 90% 27869/31000 [8:12:27<28:16,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:26:07,168 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:26:07,169 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:26:07,169 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A{'eval_loss': 0.42358195781707764, 'eval_runtime': 3.7979, 'eval_samples_per_second': 57.664, 'eval_steps_per_second': 1.053, 'epoch': 899.0}\n",
            " 90% 27869/31000 [8:12:32<28:16,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:26:10,968 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27869\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:26:10,970 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27869/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:26:12,916 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27869/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:26:12,917 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27869/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:26:17,633 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27807] due to args.save_total_limit\n",
            " 90% 27870/31000 [8:12:42<4:12:45,  4.85s/it]{'loss': 0.4464, 'learning_rate': 1.035501074633517e-06, 'epoch': 899.03}\n",
            " 90% 27880/31000 [8:12:49<37:04,  1.40it/s]{'loss': 0.4504, 'learning_rate': 1.0289561207146767e-06, 'epoch': 899.35}\n",
            "{'loss': 0.4293, 'learning_rate': 1.0224313327328795e-06, 'epoch': 899.68}\n",
            "                                           {'loss': 0.4438, 'learning_rate': 1.0159267181131005e-06, 'epoch': 900.0}\n",
            " 90% 27900/31000 [8:13:00<27:54,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:26:39,559 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:26:39,559 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:26:39,559 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.89it/s]\u001b[A\n",
            "                                           \n",
            " 90% 27900/31000 [8:13:04<27:54,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.96it/s]{'eval_loss': 0.43191564083099365, 'eval_runtime': 3.8274, 'eval_samples_per_second': 57.218, 'eval_steps_per_second': 1.045, 'epoch': 900.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:26:43,388 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27900\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:26:43,390 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27900/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:26:45,297 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27900/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:26:45,298 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27900/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:26:49,698 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27838] due to args.save_total_limit\n",
            " 90% 27910/31000 [8:13:20<39:19,  1.31it/s]{'loss': 0.4436, 'learning_rate': 1.009442284257371e-06, 'epoch': 900.32}\n",
            "                                           {'loss': 0.4287, 'learning_rate': 1.0029780385447529e-06, 'epoch': 900.65}\n",
            "                                           {'loss': 0.4464, 'learning_rate': 9.965339883313328e-07, 'epoch': 900.97}\n",
            " 90% 27931/31000 [8:13:32<28:43,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 00:27:11,724 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:27:11,724 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:27:11,725 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                           \n",
            " 90% 27931/31000 [8:13:36<28:43,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.42760178446769714, 'eval_runtime': 3.7672, 'eval_samples_per_second': 58.134, 'eval_steps_per_second': 1.062, 'epoch': 901.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 00:27:15,493 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27931\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:27:15,495 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27931/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:27:17,418 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27931/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:27:17,420 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27931/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:27:21,913 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27869] due to args.save_total_limit\n",
            " 90% 27940/31000 [8:13:51<43:27,  1.17it/s]{'loss': 0.4394, 'learning_rate': 9.90110140950224e-07, 'epoch': 901.29}\n",
            "{'loss': 0.4359, 'learning_rate': 9.83706503711535e-07, 'epoch': 901.61}\n",
            "                                           {'loss': 0.4457, 'learning_rate': 9.773230839023871e-07, 'epoch': 901.94}\n",
            " 90% 27962/31000 [8:14:04<28:07,  1.80it/s][INFO|trainer.py:2625] 2022-05-19 00:27:43,834 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:27:43,834 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:27:43,835 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4281134009361267, 'eval_runtime': 3.8608, 'eval_samples_per_second': 56.724, 'eval_steps_per_second': 1.036, 'epoch': 902.0}\n",
            " 90% 27962/31000 [8:14:08<28:07,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:27:47,697 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27962\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:27:47,699 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27962/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:27:49,651 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27962/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:27:49,652 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27962/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:27:54,245 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27900] due to args.save_total_limit\n",
            "{'loss': 0.4453, 'learning_rate': 9.709598887868943e-07, 'epoch': 902.26}\n",
            "{'loss': 0.4436, 'learning_rate': 9.646169256061522e-07, 'epoch': 902.58}\n",
            " 90% 27990/31000 [8:14:38<27:10,  1.85it/s]{'loss': 0.4401, 'learning_rate': 9.582942015782345e-07, 'epoch': 902.9}\n",
            " 90% 27993/31000 [8:14:39<27:20,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:28:18,695 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:28:18,696 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:28:18,696 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.42375150322914124, 'eval_runtime': 3.7421, 'eval_samples_per_second': 58.523, 'eval_steps_per_second': 1.069, 'epoch': 903.0}\n",
            " 90% 27993/31000 [8:14:43<27:20,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:28:22,440 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27993\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:28:22,441 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27993/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:28:24,351 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27993/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:28:24,352 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27993/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:28:28,588 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27931] due to args.save_total_limit\n",
            "{'loss': 0.4356, 'learning_rate': 9.519917238981794e-07, 'epoch': 903.23}\n",
            " 90% 28010/31000 [8:15:03<29:18,  1.70it/s]{'loss': 0.4453, 'learning_rate': 9.457094997379983e-07, 'epoch': 903.55}\n",
            "                                           {'loss': 0.4417, 'learning_rate': 9.394475362466367e-07, 'epoch': 903.87}\n",
            " 90% 28024/31000 [8:15:11<27:11,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:28:50,718 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:28:50,719 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:28:50,719 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                           \n",
            " 90% 28024/31000 [8:15:15<27:11,  1.82it/s]\n",
            "                                 {'eval_loss': 0.4227841794490814, 'eval_runtime': 3.7986, 'eval_samples_per_second': 57.653, 'eval_steps_per_second': 1.053, 'epoch': 904.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 00:28:54,519 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28024\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:28:54,521 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28024/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:28:56,414 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28024/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:28:56,416 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28024/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:29:00,728 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27962] due to args.save_total_limit\n",
            "                                             {'loss': 0.434, 'learning_rate': 9.332058405499949e-07, 'epoch': 904.19}\n",
            "{'loss': 0.4415, 'learning_rate': 9.269844197509097e-07, 'epoch': 904.52}\n",
            " 90% 28050/31000 [8:15:41<27:59,  1.76it/s]{'loss': 0.4381, 'learning_rate': 9.207832809291431e-07, 'epoch': 904.84}\n",
            " 90% 28055/31000 [8:15:43<26:43,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:29:22,552 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:29:22,552 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:29:22,552 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            " 90% 28055/31000 [8:15:47<26:43,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]{'eval_loss': 0.4280044436454773, 'eval_runtime': 3.7874, 'eval_samples_per_second': 57.824, 'eval_steps_per_second': 1.056, 'epoch': 905.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:29:26,341 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28055\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:29:26,343 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28055/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:29:28,225 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28055/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:29:28,226 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28055/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:29:32,663 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-27993] due to args.save_total_limit\n",
            "{'loss': 0.4456, 'learning_rate': 9.146024311413792e-07, 'epoch': 905.16}\n",
            "{'loss': 0.4425, 'learning_rate': 9.084418774212065e-07, 'epoch': 905.48}\n",
            " 91% 28080/31000 [8:16:12<27:50,  1.75it/s]{'loss': 0.4299, 'learning_rate': 9.023016267791276e-07, 'epoch': 905.81}\n",
            " 91% 28086/31000 [8:16:15<26:19,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:29:54,444 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:29:54,444 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:29:54,444 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "                                           \n",
            " 91% 28086/31000 [8:16:19<26:19,  1.84it/s]\n",
            "{'eval_loss': 0.4314707815647125, 'eval_runtime': 3.7713, 'eval_samples_per_second': 58.07, 'eval_steps_per_second': 1.061, 'epoch': 906.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:29:58,217 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28086\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:29:58,219 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28086/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:30:00,151 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28086/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:30:00,152 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28086/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:30:04,355 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28024] due to args.save_total_limit\n",
            " 91% 28090/31000 [8:16:31<1:37:05,  2.00s/it]{'loss': 0.4436, 'learning_rate': 8.961816862025343e-07, 'epoch': 906.13}\n",
            " 91% 28100/31000 [8:16:37<30:23,  1.59it/s]{'loss': 0.4298, 'learning_rate': 8.900820626557101e-07, 'epoch': 906.45}\n",
            "                                           {'loss': 0.4436, 'learning_rate': 8.840027630798087e-07, 'epoch': 906.77}\n",
            " 91% 28117/31000 [8:16:47<26:13,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:30:26,505 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:30:26,505 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:30:26,506 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.47it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4301353693008423, 'eval_runtime': 3.7986, 'eval_samples_per_second': 57.653, 'eval_steps_per_second': 1.053, 'epoch': 907.0}\n",
            " 91% 28117/31000 [8:16:51<26:13,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:30:30,306 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28117\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:30:30,308 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28117/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:30:32,227 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28117/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:30:32,228 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28117/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:30:36,495 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28055] due to args.save_total_limit\n",
            "                                             {'loss': 0.4429, 'learning_rate': 8.779437943928734e-07, 'epoch': 907.1}\n",
            "{'loss': 0.4401, 'learning_rate': 8.719051634897972e-07, 'epoch': 907.42}\n",
            "{'loss': 0.4434, 'learning_rate': 8.658868772423332e-07, 'epoch': 907.74}\n",
            " 91% 28148/31000 [8:17:19<25:43,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:30:58,707 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:30:58,707 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:30:58,708 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.43it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "{'eval_loss': 0.4249262809753418, 'eval_runtime': 3.7712, 'eval_samples_per_second': 58.071, 'eval_steps_per_second': 1.061, 'epoch': 908.0}\n",
            "\n",
            " 91% 28148/31000 [8:17:23<25:43,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:31:02,481 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28148\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:31:02,482 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28148/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:31:04,373 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28148/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:31:04,374 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28148/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:31:08,759 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28086] due to args.save_total_limit\n",
            "                                             {'loss': 0.4442, 'learning_rate': 8.598889424990846e-07, 'epoch': 908.06}\n",
            "{'loss': 0.4332, 'learning_rate': 8.539113660854934e-07, 'epoch': 908.39}\n",
            " 91% 28170/31000 [8:17:46<26:52,  1.76it/s]{'loss': 0.4433, 'learning_rate': 8.479541548038391e-07, 'epoch': 908.71}\n",
            " 91% 28179/31000 [8:17:51<25:25,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:31:30,608 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:31:30,608 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:31:30,608 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.52it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                           \n",
            " 91% 28179/31000 [8:17:55<25:25,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:31:34,422 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28179\n",
            "{'eval_loss': 0.43009865283966064, 'eval_runtime': 3.8119, 'eval_samples_per_second': 57.452, 'eval_steps_per_second': 1.049, 'epoch': 909.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:31:34,424 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28179/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:31:36,362 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28179/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:31:36,363 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28179/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:31:40,871 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28117] due to args.save_total_limit\n",
            "{'loss': 0.4445, 'learning_rate': 8.420173154332181e-07, 'epoch': 909.03}\n",
            " 91% 28190/31000 [8:18:12<33:26,  1.40it/s]{'loss': 0.4374, 'learning_rate': 8.361008547295489e-07, 'epoch': 909.35}\n",
            "                                           {'loss': 0.4376, 'learning_rate': 8.302047794255667e-07, 'epoch': 909.68}\n",
            " 91% 28210/31000 [8:18:24<25:08,  1.85it/s]{'loss': 0.4528, 'learning_rate': 8.243290962307941e-07, 'epoch': 910.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 00:32:03,083 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:32:03,083 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:32:03,083 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                           \n",
            " 91% 28210/31000 [8:18:28<25:08,  1.85it/s]{'eval_loss': 0.43182477355003357, 'eval_runtime': 3.8631, 'eval_samples_per_second': 56.691, 'eval_steps_per_second': 1.035, 'epoch': 910.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:32:06,948 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28210\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:32:06,949 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28210/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:32:08,893 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28210/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:32:08,894 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28210/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:32:13,519 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28148] due to args.save_total_limit\n",
            " 91% 28220/31000 [8:18:44<35:29,  1.31it/s]{'loss': 0.4549, 'learning_rate': 8.184738118315596e-07, 'epoch': 910.32}\n",
            "                                           {'loss': 0.4186, 'learning_rate': 8.12638932890975e-07, 'epoch': 910.65}\n",
            "                                           {'loss': 0.4441, 'learning_rate': 8.06824466048931e-07, 'epoch': 910.97}\n",
            " 91% 28241/31000 [8:18:56<25:42,  1.79it/s][INFO|trainer.py:2625] 2022-05-19 00:32:35,480 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:32:35,480 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:32:35,480 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.76it/s]\u001b[A\n",
            "                                           \n",
            " 91% 28241/31000 [8:19:00<25:42,  1.79it/s]{'eval_loss': 0.4306952655315399, 'eval_runtime': 3.7724, 'eval_samples_per_second': 58.053, 'eval_steps_per_second': 1.06, 'epoch': 911.0}\n",
            "\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:32:39,255 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28241\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:32:39,257 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28241/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:32:41,173 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28241/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:32:41,174 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28241/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:32:45,681 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28179] due to args.save_total_limit\n",
            "{'loss': 0.4572, 'learning_rate': 8.01030417922089e-07, 'epoch': 911.29}\n",
            "                                           {'loss': 0.4354, 'learning_rate': 7.95256795103879e-07, 'epoch': 911.61}\n",
            " 91% 28270/31000 [8:19:27<24:45,  1.84it/s]{'loss': 0.4347, 'learning_rate': 7.895036041644806e-07, 'epoch': 911.94}\n",
            " 91% 28272/31000 [8:19:28<25:10,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 00:33:07,542 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:33:07,542 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:33:07,542 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "{'eval_loss': 0.42743101716041565, 'eval_runtime': 3.7952, 'eval_samples_per_second': 57.704, 'eval_steps_per_second': 1.054, 'epoch': 912.0}\n",
            "\n",
            " 91% 28272/31000 [8:19:32<25:10,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:33:11,339 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28272\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:33:11,341 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28272/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:33:13,245 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28272/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:33:13,246 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28272/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:33:17,399 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28210] due to args.save_total_limit\n",
            " 91% 28280/31000 [8:19:46<41:49,  1.08it/s]{'loss': 0.4354, 'learning_rate': 7.8377085165083e-07, 'epoch': 912.26}\n",
            " 91% 28290/31000 [8:19:53<27:00,  1.67it/s]{'loss': 0.441, 'learning_rate': 7.780585440866044e-07, 'epoch': 912.58}\n",
            " 91% 28300/31000 [8:19:58<24:47,  1.82it/s]{'loss': 0.4395, 'learning_rate': 7.723666879722042e-07, 'epoch': 912.9}\n",
            " 91% 28303/31000 [8:20:00<24:40,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:33:39,426 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:33:39,426 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:33:39,426 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "{'eval_loss': 0.4296436011791229, 'eval_runtime': 3.729, 'eval_samples_per_second': 58.729, 'eval_steps_per_second': 1.073, 'epoch': 913.0}\n",
            "\n",
            " 91% 28303/31000 [8:20:04<24:40,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:33:43,157 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28303\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:33:43,158 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28303/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:33:45,056 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28303/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:33:45,057 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28303/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:33:49,322 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28241] due to args.save_total_limit\n",
            "                                           {'loss': 0.4428, 'learning_rate': 7.666952897847751e-07, 'epoch': 913.23}\n",
            "{'loss': 0.4377, 'learning_rate': 7.610443559781653e-07, 'epoch': 913.55}\n",
            " 91% 28330/31000 [8:20:30<24:53,  1.79it/s]{'loss': 0.4409, 'learning_rate': 7.554138929829471e-07, 'epoch': 913.87}\n",
            " 91% 28334/31000 [8:20:31<24:18,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:34:11,243 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:34:11,243 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:34:11,243 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            "                                 {'eval_loss': 0.41910648345947266, 'eval_runtime': 3.7931, 'eval_samples_per_second': 57.737, 'eval_steps_per_second': 1.055, 'epoch': 914.0}\n",
            " 91% 28334/31000 [8:20:36<24:18,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:34:15,038 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28334\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:34:15,040 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28334/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:34:16,978 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28334/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:34:16,979 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28334/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:34:21,611 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-26288] due to args.save_total_limit\n",
            "                                           {'loss': 0.4492, 'learning_rate': 7.498039072063914e-07, 'epoch': 914.19}\n",
            " 91% 28350/31000 [8:20:56<26:44,  1.65it/s]{'loss': 0.4309, 'learning_rate': 7.442144050324706e-07, 'epoch': 914.52}\n",
            "{'loss': 0.4467, 'learning_rate': 7.386453928218448e-07, 'epoch': 914.84}\n",
            " 92% 28365/31000 [8:21:04<24:11,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:34:43,514 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:34:43,514 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:34:43,515 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.20it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.44it/s]\u001b[A{'eval_loss': 0.4298502504825592, 'eval_runtime': 3.838, 'eval_samples_per_second': 57.061, 'eval_steps_per_second': 1.042, 'epoch': 915.0}\n",
            "\n",
            "                                           \n",
            " 92% 28365/31000 [8:21:08<24:11,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:34:47,355 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28365\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:34:47,357 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28365/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:34:49,321 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28365/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:34:49,322 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28365/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:34:53,831 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28272] due to args.save_total_limit\n",
            "                                             {'loss': 0.4331, 'learning_rate': 7.330968769118567e-07, 'epoch': 915.16}\n",
            "                                           {'loss': 0.4475, 'learning_rate': 7.275688636165247e-07, 'epoch': 915.48}\n",
            "                                           {'loss': 0.4413, 'learning_rate': 7.220613592265418e-07, 'epoch': 915.81}\n",
            " 92% 28396/31000 [8:21:36<23:31,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:35:15,838 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:35:15,839 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:35:15,839 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "{'eval_loss': 0.42990320920944214, 'eval_runtime': 3.7437, 'eval_samples_per_second': 58.498, 'eval_steps_per_second': 1.068, 'epoch': 916.0}\n",
            "\n",
            " 92% 28396/31000 [8:21:40<23:31,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:35:19,584 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28396\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:35:19,586 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28396/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:35:21,445 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28396/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:35:21,446 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28396/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:35:26,062 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28303] due to args.save_total_limit\n",
            " 92% 28400/31000 [8:21:53<1:27:11,  2.01s/it]{'loss': 0.4359, 'learning_rate': 7.165743700092527e-07, 'epoch': 916.13}\n",
            "                                           {'loss': 0.4454, 'learning_rate': 7.111079022086692e-07, 'epoch': 916.45}\n",
            "                                           {'loss': 0.4363, 'learning_rate': 7.056619620454341e-07, 'epoch': 916.77}\n",
            " 92% 28427/31000 [8:22:08<23:20,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:35:48,030 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:35:48,030 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:35:48,030 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            " 92% 28427/31000 [8:22:13<23:20,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "{'eval_loss': 0.43367499113082886, 'eval_runtime': 3.8706, 'eval_samples_per_second': 56.58, 'eval_steps_per_second': 1.033, 'epoch': 917.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:35:51,902 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28427\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:35:51,904 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28427/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:35:53,824 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28427/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:35:53,825 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28427/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:35:58,734 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28365] due to args.save_total_limit\n",
            "                                             {'loss': 0.4392, 'learning_rate': 7.002365557168481e-07, 'epoch': 917.1}\n",
            "                                           {'loss': 0.4365, 'learning_rate': 6.948316893968332e-07, 'epoch': 917.42}\n",
            "                                           {'loss': 0.4451, 'learning_rate': 6.894473692359383e-07, 'epoch': 917.74}\n",
            " 92% 28458/31000 [8:22:41<22:54,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:36:20,887 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:36:20,888 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:36:20,888 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                           \n",
            " 92% 28458/31000 [8:22:45<22:54,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "{'eval_loss': 0.42765483260154724, 'eval_runtime': 3.8349, 'eval_samples_per_second': 57.107, 'eval_steps_per_second': 1.043, 'epoch': 918.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:36:24,724 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28458\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:36:24,726 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28458/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:36:26,628 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28458/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:36:26,629 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28458/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:36:31,458 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28396] due to args.save_total_limit\n",
            "                                             {'loss': 0.4414, 'learning_rate': 6.84083601361337e-07, 'epoch': 918.06}\n",
            "{'loss': 0.4414, 'learning_rate': 6.787403918768114e-07, 'epoch': 918.39}\n",
            " 92% 28480/31000 [8:23:09<23:49,  1.76it/s]{'loss': 0.4424, 'learning_rate': 6.73417746862752e-07, 'epoch': 918.71}\n",
            " 92% 28489/31000 [8:23:13<22:34,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:36:53,265 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:36:53,265 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:36:53,265 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.42355284094810486, 'eval_runtime': 3.7781, 'eval_samples_per_second': 57.965, 'eval_steps_per_second': 1.059, 'epoch': 919.0}\n",
            " 92% 28489/31000 [8:23:18<22:34,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:36:57,045 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28489\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:36:57,046 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28489/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:36:58,898 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28489/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:36:58,899 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28489/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:37:03,458 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28427] due to args.save_total_limit\n",
            "{'loss': 0.4366, 'learning_rate': 6.681156723761426e-07, 'epoch': 919.03}\n",
            " 92% 28500/31000 [8:23:34<29:07,  1.43it/s]{'loss': 0.444, 'learning_rate': 6.628341744505612e-07, 'epoch': 919.35}\n",
            "{'loss': 0.4492, 'learning_rate': 6.57573259096171e-07, 'epoch': 919.68}\n",
            " 92% 28520/31000 [8:23:46<22:10,  1.86it/s]{'loss': 0.4323, 'learning_rate': 6.523329322997144e-07, 'epoch': 920.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 00:37:25,176 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:37:25,176 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:37:25,176 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "{'eval_loss': 0.42931342124938965, 'eval_runtime': 3.7828, 'eval_samples_per_second': 57.894, 'eval_steps_per_second': 1.057, 'epoch': 920.0}\n",
            "\n",
            " 92% 28520/31000 [8:23:50<22:10,  1.86it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:37:28,961 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28520\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:37:28,963 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28520/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:37:30,837 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28520/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:37:30,838 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28520/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:37:35,390 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28458] due to args.save_total_limit\n",
            " 92% 28530/31000 [8:24:05<31:16,  1.32it/s]{'loss': 0.4396, 'learning_rate': 6.47113200024496e-07, 'epoch': 920.32}\n",
            "{'loss': 0.4328, 'learning_rate': 6.419140682103981e-07, 'epoch': 920.65}\n",
            " 92% 28550/31000 [8:24:17<22:06,  1.85it/s]{'loss': 0.4532, 'learning_rate': 6.367355427738543e-07, 'epoch': 920.97}\n",
            " 92% 28551/31000 [8:24:17<22:57,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 00:37:57,208 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:37:57,209 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:37:57,209 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                           \n",
            " 92% 28551/31000 [8:24:22<22:57,  1.78it/s]{'eval_loss': 0.4277874529361725, 'eval_runtime': 3.7827, 'eval_samples_per_second': 57.895, 'eval_steps_per_second': 1.057, 'epoch': 921.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:38:00,993 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28551\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:38:00,995 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28551/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:38:02,886 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28551/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:38:02,887 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28551/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:38:07,354 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28489] due to args.save_total_limit\n",
            "                                           {'loss': 0.434, 'learning_rate': 6.315776296078427e-07, 'epoch': 921.29}\n",
            " 92% 28570/31000 [8:24:43<23:49,  1.70it/s]{'loss': 0.4414, 'learning_rate': 6.26440334581893e-07, 'epoch': 921.61}\n",
            "                                           {'loss': 0.434, 'learning_rate': 6.213236635420684e-07, 'epoch': 921.94}\n",
            " 92% 28582/31000 [8:24:49<22:17,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 00:38:29,190 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:38:29,190 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:38:29,190 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "{'eval_loss': 0.43015480041503906, 'eval_runtime': 3.7971, 'eval_samples_per_second': 57.676, 'eval_steps_per_second': 1.053, 'epoch': 922.0}\n",
            "\n",
            " 92% 28582/31000 [8:24:54<22:17,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:38:32,989 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28582\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:38:32,991 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28582/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:38:34,900 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28582/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:38:34,901 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28582/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:38:39,471 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28520] due to args.save_total_limit\n",
            "                                           {'loss': 0.4381, 'learning_rate': 6.162276223109698e-07, 'epoch': 922.26}\n",
            "                                           {'loss': 0.4373, 'learning_rate': 6.111522166877082e-07, 'epoch': 922.58}\n",
            " 92% 28610/31000 [8:25:20<22:01,  1.81it/s]{'loss': 0.4504, 'learning_rate': 6.060974524479261e-07, 'epoch': 922.9}\n",
            " 92% 28613/31000 [8:25:22<21:49,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:39:01,459 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:39:01,459 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:39:01,459 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4379931688308716, 'eval_runtime': 3.7661, 'eval_samples_per_second': 58.151, 'eval_steps_per_second': 1.062, 'epoch': 923.0}\n",
            " 92% 28613/31000 [8:25:26<21:49,  1.82it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:39:05,227 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28613\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:39:05,229 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28613/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:39:07,126 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28613/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:39:07,127 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28613/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:39:11,538 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28551] due to args.save_total_limit\n",
            " 92% 28620/31000 [8:25:40<42:45,  1.08s/it]{'loss': 0.4339, 'learning_rate': 6.010633353437683e-07, 'epoch': 923.23}\n",
            " 92% 28630/31000 [8:25:46<22:56,  1.72it/s]{'loss': 0.4305, 'learning_rate': 5.960498711038854e-07, 'epoch': 923.55}\n",
            "                                           {'loss': 0.445, 'learning_rate': 5.910570654334305e-07, 'epoch': 923.87}\n",
            " 92% 28644/31000 [8:25:54<21:22,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:39:33,450 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:39:33,450 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:39:33,450 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A{'eval_loss': 0.4340265691280365, 'eval_runtime': 3.7635, 'eval_samples_per_second': 58.19, 'eval_steps_per_second': 1.063, 'epoch': 924.0}\n",
            " 92% 28644/31000 [8:25:58<21:22,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:39:37,215 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28644\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:39:37,217 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28644/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:39:39,132 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28644/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:39:39,133 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28644/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:39:43,576 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28582] due to args.save_total_limit\n",
            " 92% 28650/31000 [8:26:11<50:19,  1.28s/it]  {'loss': 0.4409, 'learning_rate': 5.860849240140423e-07, 'epoch': 924.19}\n",
            "{'loss': 0.451, 'learning_rate': 5.811334525038506e-07, 'epoch': 924.52}\n",
            "{'loss': 0.44, 'learning_rate': 5.762026565374508e-07, 'epoch': 924.84}\n",
            " 92% 28675/31000 [8:26:25<21:00,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:40:05,232 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:40:05,232 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:40:05,232 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.28it/s]\u001b[A\n",
            "                                           \n",
            " 92% 28675/31000 [8:26:30<21:00,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.51it/s]\u001b[A{'eval_loss': 0.4359789490699768, 'eval_runtime': 3.7784, 'eval_samples_per_second': 57.962, 'eval_steps_per_second': 1.059, 'epoch': 925.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:40:09,013 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28675\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:40:09,015 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28675/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:40:10,948 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28675/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:40:10,949 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28675/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:40:15,509 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28613] due to args.save_total_limit\n",
            "                                             {'loss': 0.4342, 'learning_rate': 5.712925417259248e-07, 'epoch': 925.16}\n",
            "{'loss': 0.4393, 'learning_rate': 5.664031136568117e-07, 'epoch': 925.48}\n",
            "                                           {'loss': 0.4449, 'learning_rate': 5.615343778941129e-07, 'epoch': 925.81}\n",
            " 93% 28706/31000 [8:26:58<20:42,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:40:37,417 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:40:37,417 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:40:37,417 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "{'eval_loss': 0.42565417289733887, 'eval_runtime': 3.7241, 'eval_samples_per_second': 58.806, 'eval_steps_per_second': 1.074, 'epoch': 926.0}\n",
            "\n",
            " 93% 28706/31000 [8:27:02<20:42,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:40:41,143 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28706\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:40:41,146 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28706/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:40:43,137 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28706/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:40:43,138 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28706/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:40:47,856 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28644] due to args.save_total_limit\n",
            " 93% 28710/31000 [8:27:14<1:17:50,  2.04s/it]{'loss': 0.4372, 'learning_rate': 5.566863399782837e-07, 'epoch': 926.13}\n",
            " 93% 28720/31000 [8:27:20<23:19,  1.63it/s]{'loss': 0.4384, 'learning_rate': 5.518590054262196e-07, 'epoch': 926.45}\n",
            "                                           {'loss': 0.4426, 'learning_rate': 5.470523797312637e-07, 'epoch': 926.77}\n",
            " 93% 28737/31000 [8:27:30<20:30,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:41:09,767 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:41:09,767 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:41:09,767 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.43275344371795654, 'eval_runtime': 3.7502, 'eval_samples_per_second': 58.397, 'eval_steps_per_second': 1.067, 'epoch': 927.0}\n",
            " 93% 28737/31000 [8:27:34<20:30,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:41:13,519 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28737\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:41:13,521 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28737/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:41:15,418 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28737/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:41:15,419 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28737/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:41:20,609 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28675] due to args.save_total_limit\n",
            "{'loss': 0.4379, 'learning_rate': 5.422664683631908e-07, 'epoch': 927.1}\n",
            "                                           {'loss': 0.4527, 'learning_rate': 5.375012767682048e-07, 'epoch': 927.42}\n",
            "                                           {'loss': 0.4414, 'learning_rate': 5.327568103689222e-07, 'epoch': 927.74}\n",
            " 93% 28768/31000 [8:28:03<20:03,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:41:42,590 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:41:42,591 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:41:42,591 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                           \n",
            " 93% 28768/31000 [8:28:07<20:03,  1.85it/s]\n",
            "                                 {'eval_loss': 0.42264145612716675, 'eval_runtime': 3.7982, 'eval_samples_per_second': 57.659, 'eval_steps_per_second': 1.053, 'epoch': 928.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 00:41:46,390 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28768\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:41:46,392 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28768/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:41:48,290 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28768/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:41:48,291 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28768/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:41:53,192 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28706] due to args.save_total_limit\n",
            "                                             {'loss': 0.4397, 'learning_rate': 5.280330745643886e-07, 'epoch': 928.06}\n",
            "{'loss': 0.4323, 'learning_rate': 5.233300747300544e-07, 'epoch': 928.39}\n",
            " 93% 28790/31000 [8:28:30<21:22,  1.72it/s]{'loss': 0.4505, 'learning_rate': 5.186478162177654e-07, 'epoch': 928.71}\n",
            " 93% 28799/31000 [8:28:35<19:40,  1.86it/s][INFO|trainer.py:2625] 2022-05-19 00:42:14,957 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:42:14,957 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:42:14,957 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.86it/s]\u001b[A\n",
            "                                           \n",
            " 93% 28799/31000 [8:28:39<19:40,  1.86it/s]\n",
            "100% 4/4 [00:00<00:00,  8.96it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:42:18,693 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28799\n",
            "{'eval_loss': 0.43204665184020996, 'eval_runtime': 3.7342, 'eval_samples_per_second': 58.647, 'eval_steps_per_second': 1.071, 'epoch': 929.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:42:18,695 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28799/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:42:20,589 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28799/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:42:20,590 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28799/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:42:25,351 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28737] due to args.save_total_limit\n",
            " 93% 28800/31000 [8:28:51<3:04:52,  5.04s/it]{'loss': 0.4415, 'learning_rate': 5.139863043557721e-07, 'epoch': 929.03}\n",
            "                                           {'loss': 0.4453, 'learning_rate': 5.093455444487165e-07, 'epoch': 929.35}\n",
            " 93% 28820/31000 [8:29:03<20:49,  1.75it/s]{'loss': 0.4291, 'learning_rate': 5.047255417776223e-07, 'epoch': 929.68}\n",
            " 93% 28830/31000 [8:29:08<19:28,  1.86it/s]{'loss': 0.4367, 'learning_rate': 5.001263015998922e-07, 'epoch': 930.0}\n",
            " 93% 28830/31000 [8:29:09<19:28,  1.86it/s][INFO|trainer.py:2625] 2022-05-19 00:42:48,035 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:42:48,035 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:42:48,035 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4286668300628662, 'eval_runtime': 3.7623, 'eval_samples_per_second': 58.209, 'eval_steps_per_second': 1.063, 'epoch': 930.0}\n",
            " 93% 28830/31000 [8:29:12<19:28,  1.86it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:42:51,799 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28830\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:42:51,801 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28830/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:42:53,712 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28830/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:42:53,713 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28830/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:42:58,217 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28768] due to args.save_total_limit\n",
            " 93% 28840/31000 [8:29:29<27:05,  1.33it/s]{'loss': 0.4402, 'learning_rate': 4.955478291493023e-07, 'epoch': 930.32}\n",
            " 93% 28850/31000 [8:29:34<20:34,  1.74it/s]{'loss': 0.4499, 'learning_rate': 4.909901296359993e-07, 'epoch': 930.65}\n",
            "                                           {'loss': 0.4252, 'learning_rate': 4.864532082464866e-07, 'epoch': 930.97}\n",
            " 93% 28861/31000 [8:29:40<19:58,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 00:43:20,062 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:43:20,062 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:43:20,062 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A{'eval_loss': 0.4266606271266937, 'eval_runtime': 3.7228, 'eval_samples_per_second': 58.827, 'eval_steps_per_second': 1.074, 'epoch': 931.0}\n",
            "\n",
            "                                           \n",
            " 93% 28861/31000 [8:29:44<19:58,  1.78it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:43:23,786 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28861\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:43:23,787 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28861/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:43:25,670 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28861/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:43:25,671 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28861/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:43:30,161 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28799] due to args.save_total_limit\n",
            " 93% 28870/31000 [8:30:00<29:48,  1.19it/s]{'loss': 0.444, 'learning_rate': 4.819370701436239e-07, 'epoch': 931.29}\n",
            " 93% 28880/31000 [8:30:06<20:50,  1.70it/s]{'loss': 0.4415, 'learning_rate': 4.77441720466621e-07, 'epoch': 931.61}\n",
            " 93% 28890/31000 [8:30:12<18:57,  1.86it/s]{'loss': 0.436, 'learning_rate': 4.72967164331034e-07, 'epoch': 931.94}\n",
            " 93% 28892/31000 [8:30:12<19:22,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 00:43:52,055 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:43:52,055 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:43:52,055 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                           \n",
            " 93% 28892/31000 [8:30:16<19:22,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.69it/s]\u001b[A\n",
            "{'eval_loss': 0.41873711347579956, 'eval_runtime': 3.7545, 'eval_samples_per_second': 58.329, 'eval_steps_per_second': 1.065, 'epoch': 932.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:43:55,811 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28892\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:43:55,813 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28892/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:43:57,749 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28892/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:43:57,750 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28892/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:44:02,365 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28334] due to args.save_total_limit\n",
            " 93% 28900/31000 [8:30:31<32:38,  1.07it/s]{'loss': 0.4418, 'learning_rate': 4.68513406828748e-07, 'epoch': 932.26}\n",
            "                                           {'loss': 0.4373, 'learning_rate': 4.6408045302798805e-07, 'epoch': 932.58}\n",
            "{'loss': 0.4453, 'learning_rate': 4.5966830797330225e-07, 'epoch': 932.9}\n",
            " 93% 28923/31000 [8:30:44<18:57,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:44:23,944 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:44:23,944 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:44:23,945 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.85it/s]\u001b[A\n",
            "                                           \n",
            " 93% 28923/31000 [8:30:48<18:57,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]{'eval_loss': 0.42424410581588745, 'eval_runtime': 3.7204, 'eval_samples_per_second': 58.865, 'eval_steps_per_second': 1.075, 'epoch': 933.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:44:27,667 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28923\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:44:27,668 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28923/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:44:29,526 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28923/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:44:29,527 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28923/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:44:33,852 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28830] due to args.save_total_limit\n",
            "                                           {'loss': 0.4378, 'learning_rate': 4.5527697668555975e-07, 'epoch': 933.23}\n",
            "{'loss': 0.4479, 'learning_rate': 4.5090646416194655e-07, 'epoch': 933.55}\n",
            "{'loss': 0.4402, 'learning_rate': 4.4655677537595097e-07, 'epoch': 933.87}\n",
            " 93% 28954/31000 [8:31:16<18:45,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:44:55,991 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:44:55,991 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:44:55,991 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            " 93% 28954/31000 [8:31:20<18:45,  1.82it/s]\n",
            "{'eval_loss': 0.4324769973754883, 'eval_runtime': 3.7407, 'eval_samples_per_second': 58.545, 'eval_steps_per_second': 1.069, 'epoch': 934.0}\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:44:59,733 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28954\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:44:59,735 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28954/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:45:01,593 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28954/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:45:01,594 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28954/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:45:06,115 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28861] due to args.save_total_limit\n",
            "{'loss': 0.4425, 'learning_rate': 4.4222791527737387e-07, 'epoch': 934.19}\n",
            "{'loss': 0.4364, 'learning_rate': 4.379198887923041e-07, 'epoch': 934.52}\n",
            "                                           {'loss': 0.4443, 'learning_rate': 4.336327008231284e-07, 'epoch': 934.84}\n",
            " 94% 28985/31000 [8:31:48<18:17,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:45:27,908 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:45:27,909 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:45:27,909 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4258793294429779, 'eval_runtime': 3.7923, 'eval_samples_per_second': 57.749, 'eval_steps_per_second': 1.055, 'epoch': 935.0}\n",
            " 94% 28985/31000 [8:31:52<18:17,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.80it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:45:31,702 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28985\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:45:31,704 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28985/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:45:33,623 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28985/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:45:33,624 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28985/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:45:38,201 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28923] due to args.save_total_limit\n",
            " 94% 28990/31000 [8:32:06<54:17,  1.62s/it]{'loss': 0.4317, 'learning_rate': 4.293663562485214e-07, 'epoch': 935.16}\n",
            " 94% 29000/31000 [8:32:11<20:37,  1.62it/s]{'loss': 0.4401, 'learning_rate': 4.251208599234328e-07, 'epoch': 935.48}\n",
            " 94% 29010/31000 [8:32:18<18:55,  1.75it/s]{'loss': 0.4441, 'learning_rate': 4.208962166790959e-07, 'epoch': 935.81}\n",
            " 94% 29016/31000 [8:32:20<17:52,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:46:00,184 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:46:00,184 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:46:00,185 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                           \n",
            " 94% 29016/31000 [8:32:25<17:52,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]{'eval_loss': 0.4258323311805725, 'eval_runtime': 3.7844, 'eval_samples_per_second': 57.869, 'eval_steps_per_second': 1.057, 'epoch': 936.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:46:03,971 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29016\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:46:03,972 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29016/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:46:05,858 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29016/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:46:05,859 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29016/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:46:10,541 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28954] due to args.save_total_limit\n",
            "{'loss': 0.4508, 'learning_rate': 4.1669243132300483e-07, 'epoch': 936.13}\n",
            " 94% 29030/31000 [8:32:43<20:45,  1.58it/s]{'loss': 0.4366, 'learning_rate': 4.125095086389224e-07, 'epoch': 936.45}\n",
            "                                           {'loss': 0.4404, 'learning_rate': 4.0834745338687235e-07, 'epoch': 936.77}\n",
            " 94% 29047/31000 [8:32:53<17:44,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:46:32,517 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:46:32,517 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:46:32,517 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.42716917395591736, 'eval_runtime': 3.7399, 'eval_samples_per_second': 58.557, 'eval_steps_per_second': 1.07, 'epoch': 937.0}\n",
            " 94% 29047/31000 [8:32:57<17:44,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:46:36,258 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29047\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:46:36,260 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29047/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:46:38,188 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29047/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:46:38,189 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29047/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:46:42,639 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28985] due to args.save_total_limit\n",
            " 94% 29050/31000 [8:33:09<1:25:12,  2.62s/it]{'loss': 0.4382, 'learning_rate': 4.042062703031285e-07, 'epoch': 937.1}\n",
            " 94% 29060/31000 [8:33:15<20:12,  1.60it/s]{'loss': 0.4446, 'learning_rate': 4.00085964100217e-07, 'epoch': 937.42}\n",
            " 94% 29070/31000 [8:33:20<18:39,  1.72it/s]{'loss': 0.4425, 'learning_rate': 3.959865394668978e-07, 'epoch': 937.74}\n",
            " 94% 29078/31000 [8:33:25<17:19,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:47:04,546 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:47:04,547 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:47:04,547 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A{'eval_loss': 0.42854082584381104, 'eval_runtime': 3.7545, 'eval_samples_per_second': 58.33, 'eval_steps_per_second': 1.065, 'epoch': 938.0}\n",
            "                                           \n",
            " 94% 29078/31000 [8:33:29<17:19,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:47:08,303 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29078\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:47:08,304 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29078/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:47:10,165 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29078/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:47:10,166 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29078/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:47:14,680 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29016] due to args.save_total_limit\n",
            " 94% 29080/31000 [8:33:40<1:51:54,  3.50s/it]{'loss': 0.4349, 'learning_rate': 3.919080010681849e-07, 'epoch': 938.06}\n",
            "{'loss': 0.4431, 'learning_rate': 3.8785035354530756e-07, 'epoch': 938.39}\n",
            "                                           {'loss': 0.4405, 'learning_rate': 3.838136015157284e-07, 'epoch': 938.71}\n",
            " 94% 29109/31000 [8:33:57<17:01,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:47:36,601 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:47:36,601 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:47:36,601 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.79it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "{'eval_loss': 0.4317639470100403, 'eval_runtime': 3.7704, 'eval_samples_per_second': 58.084, 'eval_steps_per_second': 1.061, 'epoch': 939.0}\n",
            "\n",
            " 94% 29109/31000 [8:34:01<17:01,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:47:40,373 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29109\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:47:40,375 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29109/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:47:42,285 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29109/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:47:42,286 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29109/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:47:47,410 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29047] due to args.save_total_limit\n",
            " 94% 29110/31000 [8:34:13<2:35:53,  4.95s/it]{'loss': 0.4388, 'learning_rate': 3.7979774957313525e-07, 'epoch': 939.03}\n",
            "{'loss': 0.4411, 'learning_rate': 3.7580280228742683e-07, 'epoch': 939.35}\n",
            "                                           {'loss': 0.4464, 'learning_rate': 3.718287642047187e-07, 'epoch': 939.68}\n",
            " 94% 29140/31000 [8:34:30<16:47,  1.85it/s]{'loss': 0.4384, 'learning_rate': 3.6787563984732453e-07, 'epoch': 940.0}\n",
            " 94% 29140/31000 [8:34:30<16:47,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:48:09,588 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:48:09,588 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:48:09,588 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.42160606384277344, 'eval_runtime': 3.7827, 'eval_samples_per_second': 57.895, 'eval_steps_per_second': 1.057, 'epoch': 940.0}\n",
            " 94% 29140/31000 [8:34:34<16:47,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:48:13,373 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29140\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:48:13,374 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29140/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:48:15,318 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29140/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:48:15,319 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29140/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:48:20,382 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29078] due to args.save_total_limit\n",
            "                                           {'loss': 0.4423, 'learning_rate': 3.6394343371376685e-07, 'epoch': 940.32}\n",
            " 94% 29160/31000 [8:34:56<17:57,  1.71it/s]{'loss': 0.4348, 'learning_rate': 3.6003215027875996e-07, 'epoch': 940.65}\n",
            " 94% 29170/31000 [8:35:02<16:32,  1.84it/s]{'loss': 0.4434, 'learning_rate': 3.5614179399321014e-07, 'epoch': 940.97}\n",
            " 94% 29171/31000 [8:35:02<17:06,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 00:48:42,259 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:48:42,259 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:48:42,259 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.43756186962127686, 'eval_runtime': 3.795, 'eval_samples_per_second': 57.708, 'eval_steps_per_second': 1.054, 'epoch': 941.0}\n",
            " 94% 29171/31000 [8:35:07<17:06,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:48:46,055 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29171\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:48:46,057 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29171/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:48:47,933 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29171/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:48:47,934 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29171/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:48:52,948 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29109] due to args.save_total_limit\n",
            "                                           {'loss': 0.4499, 'learning_rate': 3.5227236928420526e-07, 'epoch': 941.29}\n",
            "                                           {'loss': 0.4358, 'learning_rate': 3.484238805550231e-07, 'epoch': 941.61}\n",
            "                                           {'loss': 0.4448, 'learning_rate': 3.4459633218510834e-07, 'epoch': 941.94}\n",
            " 94% 29202/31000 [8:35:35<16:39,  1.80it/s][INFO|trainer.py:2625] 2022-05-19 00:49:14,968 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:49:14,968 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:49:14,968 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                           \n",
            " 94% 29202/31000 [8:35:39<16:39,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]{'eval_loss': 0.4284622073173523, 'eval_runtime': 3.7792, 'eval_samples_per_second': 57.948, 'eval_steps_per_second': 1.058, 'epoch': 942.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:49:18,749 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29202\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:49:18,751 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29202/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:49:20,653 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29202/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:49:20,654 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29202/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:49:25,702 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29140] due to args.save_total_limit\n",
            "                                           {'loss': 0.4218, 'learning_rate': 3.4078972853007686e-07, 'epoch': 942.26}\n",
            " 94% 29220/31000 [8:36:01<18:05,  1.64it/s]{'loss': 0.4417, 'learning_rate': 3.3700407392171364e-07, 'epoch': 942.58}\n",
            "                                           {'loss': 0.4498, 'learning_rate': 3.332393726679643e-07, 'epoch': 942.9}\n",
            " 94% 29233/31000 [8:36:08<16:19,  1.80it/s][INFO|trainer.py:2625] 2022-05-19 00:49:47,882 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:49:47,882 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:49:47,882 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.43360015749931335, 'eval_runtime': 3.8545, 'eval_samples_per_second': 56.816, 'eval_steps_per_second': 1.038, 'epoch': 943.0}\n",
            " 94% 29233/31000 [8:36:12<16:19,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:49:51,739 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29233\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:49:51,740 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29233/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:49:53,683 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29233/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:49:53,684 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29233/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:49:58,862 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29171] due to args.save_total_limit\n",
            " 94% 29240/31000 [8:36:27<32:45,  1.12s/it]{'loss': 0.4355, 'learning_rate': 3.2949562905293104e-07, 'epoch': 943.23}\n",
            "{'loss': 0.4346, 'learning_rate': 3.2577284733686023e-07, 'epoch': 943.55}\n",
            "{'loss': 0.4422, 'learning_rate': 3.220710317561506e-07, 'epoch': 943.87}\n",
            " 94% 29264/31000 [8:36:41<15:52,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:50:20,939 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:50:20,939 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:50:20,939 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.60it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "                                           \n",
            " 94% 29264/31000 [8:36:45<15:52,  1.82it/s]\n",
            "                                 \u001b[A{'eval_loss': 0.4393758773803711, 'eval_runtime': 3.8254, 'eval_samples_per_second': 57.248, 'eval_steps_per_second': 1.046, 'epoch': 944.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 00:50:24,766 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29264\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:50:24,768 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29264/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:50:26,655 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29264/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:50:26,656 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29264/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:50:31,830 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29202] due to args.save_total_limit\n",
            "                                           {'loss': 0.4435, 'learning_rate': 3.18390186523345e-07, 'epoch': 944.19}\n",
            "{'loss': 0.4361, 'learning_rate': 3.1473031582711364e-07, 'epoch': 944.52}\n",
            "{'loss': 0.4386, 'learning_rate': 3.1109142383226455e-07, 'epoch': 944.84}\n",
            " 94% 29295/31000 [8:37:14<15:27,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:50:53,717 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:50:53,717 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:50:53,717 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4257717728614807, 'eval_runtime': 3.7861, 'eval_samples_per_second': 57.844, 'eval_steps_per_second': 1.057, 'epoch': 945.0}\n",
            " 94% 29295/31000 [8:37:18<15:27,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:50:57,505 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29295\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:50:57,507 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29295/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:50:59,439 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29295/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:50:59,440 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29295/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:51:04,317 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29233] due to args.save_total_limit\n",
            " 95% 29300/31000 [8:37:31<46:29,  1.64s/it]{'loss': 0.4468, 'learning_rate': 3.0747351467972915e-07, 'epoch': 945.16}\n",
            "                                           {'loss': 0.4401, 'learning_rate': 3.0387659248656825e-07, 'epoch': 945.48}\n",
            "{'loss': 0.4443, 'learning_rate': 3.0030066134595147e-07, 'epoch': 945.81}\n",
            " 95% 29326/31000 [8:37:47<15:08,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 00:51:26,382 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:51:26,382 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:51:26,382 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.51it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "                                           \n",
            " 95% 29326/31000 [8:37:51<15:08,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:51:30,189 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29326\n",
            "{'eval_loss': 0.4244672656059265, 'eval_runtime': 3.8051, 'eval_samples_per_second': 57.554, 'eval_steps_per_second': 1.051, 'epoch': 946.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:51:30,190 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29326/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:51:32,166 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29326/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:51:32,168 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29326/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:51:37,163 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29264] due to args.save_total_limit\n",
            "                                           {'loss': 0.441, 'learning_rate': 2.967457253271653e-07, 'epoch': 946.13}\n",
            " 95% 29340/31000 [8:38:10<17:00,  1.63it/s]{'loss': 0.4366, 'learning_rate': 2.932117884756072e-07, 'epoch': 946.45}\n",
            " 95% 29350/31000 [8:38:15<16:00,  1.72it/s]{'loss': 0.4411, 'learning_rate': 2.8969885481277267e-07, 'epoch': 946.77}\n",
            " 95% 29357/31000 [8:38:19<14:57,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:51:59,155 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:51:59,156 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:51:59,156 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.49it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A{'eval_loss': 0.4333001971244812, 'eval_runtime': 3.9971, 'eval_samples_per_second': 54.789, 'eval_steps_per_second': 1.001, 'epoch': 947.0}\n",
            "                                           \n",
            " 95% 29357/31000 [8:38:24<14:57,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:52:03,154 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29357\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:52:03,156 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29357/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:52:05,067 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29357/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:52:05,068 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29357/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:52:09,951 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29295] due to args.save_total_limit\n",
            "                                             {'loss': 0.4385, 'learning_rate': 2.8620692833626196e-07, 'epoch': 947.1}\n",
            " 95% 29370/31000 [8:38:42<17:12,  1.58it/s]{'loss': 0.4386, 'learning_rate': 2.827360130197631e-07, 'epoch': 947.42}\n",
            " 95% 29380/31000 [8:38:48<15:39,  1.72it/s]{'loss': 0.4379, 'learning_rate': 2.792861128130604e-07, 'epoch': 947.74}\n",
            " 95% 29388/31000 [8:38:52<14:31,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:52:31,876 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:52:31,877 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:52:31,877 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.19it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.62it/s]\u001b[A\n",
            "\u001b[A{'eval_loss': 0.42913204431533813, 'eval_runtime': 3.7776, 'eval_samples_per_second': 57.973, 'eval_steps_per_second': 1.059, 'epoch': 948.0}\n",
            "                                           \n",
            " 95% 29388/31000 [8:38:56<14:31,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:52:35,656 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29388\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:52:35,658 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29388/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:52:37,637 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29388/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:52:37,638 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29388/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:52:42,637 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29326] due to args.save_total_limit\n",
            "{'loss': 0.4406, 'learning_rate': 2.758572316420217e-07, 'epoch': 948.06}\n",
            " 95% 29400/31000 [8:39:15<17:57,  1.49it/s]{'loss': 0.4314, 'learning_rate': 2.7244937340859253e-07, 'epoch': 948.39}\n",
            "                                           {'loss': 0.4424, 'learning_rate': 2.690625419908041e-07, 'epoch': 948.71}\n",
            " 95% 29419/31000 [8:39:25<14:15,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:53:04,815 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:53:04,815 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:53:04,815 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.42420172691345215, 'eval_runtime': 3.7495, 'eval_samples_per_second': 58.407, 'eval_steps_per_second': 1.067, 'epoch': 949.0}\n",
            " 95% 29419/31000 [8:39:29<14:15,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:53:08,567 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29419\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:53:08,569 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29419/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:53:10,443 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29419/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:53:10,444 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29419/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:53:15,346 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29357] due to args.save_total_limit\n",
            " 95% 29420/31000 [8:39:40<2:07:22,  4.84s/it]{'loss': 0.4479, 'learning_rate': 2.656967412427484e-07, 'epoch': 949.03}\n",
            "{'loss': 0.4423, 'learning_rate': 2.6235197499459295e-07, 'epoch': 949.35}\n",
            " 95% 29440/31000 [8:39:52<15:07,  1.72it/s]{'loss': 0.445, 'learning_rate': 2.590282470525659e-07, 'epoch': 949.68}\n",
            "{'loss': 0.4407, 'learning_rate': 2.557255611989564e-07, 'epoch': 950.0}\n",
            " 95% 29450/31000 [8:39:58<13:51,  1.86it/s][INFO|trainer.py:2625] 2022-05-19 00:53:37,200 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:53:37,200 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:53:37,200 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.42549848556518555, 'eval_runtime': 3.7219, 'eval_samples_per_second': 58.841, 'eval_steps_per_second': 1.075, 'epoch': 950.0}\n",
            " 95% 29450/31000 [8:40:02<13:51,  1.86it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:53:40,924 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29450\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:53:40,926 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29450/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:53:42,851 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29450/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:53:42,852 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29450/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:53:47,644 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29388] due to args.save_total_limit\n",
            " 95% 29460/31000 [8:40:18<19:38,  1.31it/s]{'loss': 0.4363, 'learning_rate': 2.5244392119210393e-07, 'epoch': 950.32}\n",
            " 95% 29470/31000 [8:40:24<14:58,  1.70it/s]{'loss': 0.4438, 'learning_rate': 2.491833307664046e-07, 'epoch': 950.65}\n",
            "{'loss': 0.4423, 'learning_rate': 2.459437936322986e-07, 'epoch': 950.97}\n",
            " 95% 29481/31000 [8:40:30<14:11,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 00:54:09,600 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:54:09,601 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:54:09,601 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "{'eval_loss': 0.42252638936042786, 'eval_runtime': 3.783, 'eval_samples_per_second': 57.89, 'eval_steps_per_second': 1.057, 'epoch': 951.0}\n",
            "                                           \n",
            " 95% 29481/31000 [8:40:34<14:11,  1.78it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:54:13,385 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29481\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:54:13,387 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29481/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:54:15,317 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29481/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:54:15,318 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29481/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:54:20,466 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29419] due to args.save_total_limit\n",
            "{'loss': 0.4334, 'learning_rate': 2.4272531347626216e-07, 'epoch': 951.29}\n",
            " 95% 29500/31000 [8:40:56<14:53,  1.68it/s]{'loss': 0.442, 'learning_rate': 2.395278939608218e-07, 'epoch': 951.61}\n",
            " 95% 29510/31000 [8:41:01<13:29,  1.84it/s]{'loss': 0.4529, 'learning_rate': 2.3635153872452532e-07, 'epoch': 951.94}\n",
            " 95% 29512/31000 [8:41:03<13:43,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 00:54:42,386 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:54:42,386 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:54:42,386 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "\u001b[A{'eval_loss': 0.42485108971595764, 'eval_runtime': 3.7479, 'eval_samples_per_second': 58.433, 'eval_steps_per_second': 1.067, 'epoch': 952.0}\n",
            "                                           \n",
            " 95% 29512/31000 [8:41:07<13:43,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:54:46,136 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29512\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:54:46,137 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29512/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:54:48,035 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29512/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:54:48,036 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29512/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:54:52,996 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29450] due to args.save_total_limit\n",
            " 95% 29520/31000 [8:41:22<23:16,  1.06it/s]{'loss': 0.4365, 'learning_rate': 2.331962513819563e-07, 'epoch': 952.26}\n",
            "{'loss': 0.4364, 'learning_rate': 2.3006203552372593e-07, 'epoch': 952.58}\n",
            "                                           {'loss': 0.4474, 'learning_rate': 2.2694889471646244e-07, 'epoch': 952.9}\n",
            " 95% 29543/31000 [8:41:35<13:25,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 00:55:15,041 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:55:15,041 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:55:15,041 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "                                           \n",
            " 95% 29543/31000 [8:41:39<13:25,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 0.4306611716747284, 'eval_runtime': 3.7827, 'eval_samples_per_second': 57.895, 'eval_steps_per_second': 1.057, 'epoch': 953.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 00:55:18,826 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29543\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:55:18,828 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29543/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:55:20,750 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29543/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:55:20,751 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29543/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:55:25,732 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29481] due to args.save_total_limit\n",
            " 95% 29550/31000 [8:41:55<26:38,  1.10s/it]{'loss': 0.4328, 'learning_rate': 2.2385683250281325e-07, 'epoch': 953.23}\n",
            "{'loss': 0.4484, 'learning_rate': 2.2078585240143665e-07, 'epoch': 953.55}\n",
            "                                           {'loss': 0.4285, 'learning_rate': 2.1773595790700176e-07, 'epoch': 953.87}\n",
            " 95% 29574/31000 [8:42:08<13:03,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 00:55:47,932 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:55:47,932 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:55:47,932 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.69it/s]\u001b[A\n",
            "{'eval_loss': 0.431819349527359, 'eval_runtime': 3.8475, 'eval_samples_per_second': 56.92, 'eval_steps_per_second': 1.04, 'epoch': 954.0}\n",
            "\n",
            " 95% 29574/31000 [8:42:12<13:03,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:55:51,781 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29574\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:55:51,783 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29574/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:55:53,722 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29574/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:55:53,723 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29574/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:55:58,573 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29512] due to args.save_total_limit\n",
            "{'loss': 0.4452, 'learning_rate': 2.1470715249018444e-07, 'epoch': 954.19}\n",
            "                                           {'loss': 0.4406, 'learning_rate': 2.116994395976651e-07, 'epoch': 954.52}\n",
            " 95% 29600/31000 [8:42:38<13:27,  1.73it/s]{'loss': 0.4433, 'learning_rate': 2.0871282265211009e-07, 'epoch': 954.84}\n",
            " 96% 29605/31000 [8:42:41<12:41,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 00:56:20,697 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:56:20,697 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:56:20,697 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.42614543437957764, 'eval_runtime': 3.7618, 'eval_samples_per_second': 58.217, 'eval_steps_per_second': 1.063, 'epoch': 955.0}\n",
            " 96% 29605/31000 [8:42:45<12:41,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:56:24,461 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29605\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:56:24,463 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29605/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:56:26,398 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29605/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:56:26,399 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29605/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:56:31,335 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29543] due to args.save_total_limit\n",
            " 96% 29610/31000 [8:42:58<37:58,  1.64s/it]{'loss': 0.4348, 'learning_rate': 2.0574730505218823e-07, 'epoch': 955.16}\n",
            "{'loss': 0.442, 'learning_rate': 2.0280289017256257e-07, 'epoch': 955.48}\n",
            "                                           {'loss': 0.4465, 'learning_rate': 1.9987958136386746e-07, 'epoch': 955.81}\n",
            " 96% 29636/31000 [8:43:13<12:18,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:56:53,176 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:56:53,176 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:56:53,176 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                           \n",
            " 96% 29636/31000 [8:43:18<12:18,  1.85it/s]\n",
            "                                 {'eval_loss': 0.431937575340271, 'eval_runtime': 3.7496, 'eval_samples_per_second': 58.407, 'eval_steps_per_second': 1.067, 'epoch': 956.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 00:56:56,927 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29636\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:56:56,929 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29636/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:56:58,842 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29636/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:56:58,843 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29636/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:57:05,627 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29574] due to args.save_total_limit\n",
            "                                           {'loss': 0.433, 'learning_rate': 1.9697738195273356e-07, 'epoch': 956.13}\n",
            " 96% 29650/31000 [8:43:38<13:57,  1.61it/s]{'loss': 0.4335, 'learning_rate': 1.9409629524175867e-07, 'epoch': 956.45}\n",
            " 96% 29660/31000 [8:43:44<12:46,  1.75it/s]{'loss': 0.4433, 'learning_rate': 1.9123632450952636e-07, 'epoch': 956.77}\n",
            " 96% 29667/31000 [8:43:48<12:14,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 00:57:27,698 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:57:27,699 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:57:27,699 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4302458167076111, 'eval_runtime': 3.7915, 'eval_samples_per_second': 57.76, 'eval_steps_per_second': 1.055, 'epoch': 957.0}\n",
            " 96% 29667/31000 [8:43:52<12:14,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:57:31,492 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29667\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:57:31,494 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29667/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:57:33,433 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29667/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:57:33,434 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29667/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:57:38,046 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29605] due to args.save_total_limit\n",
            " 96% 29670/31000 [8:44:04<58:51,  2.66s/it]  {'loss': 0.4441, 'learning_rate': 1.8839747301057916e-07, 'epoch': 957.1}\n",
            " 96% 29680/31000 [8:44:10<13:59,  1.57it/s]{'loss': 0.446, 'learning_rate': 1.85579743975435e-07, 'epoch': 957.42}\n",
            "{'loss': 0.4381, 'learning_rate': 1.8278314061057063e-07, 'epoch': 957.74}\n",
            " 96% 29698/31000 [8:44:20<11:43,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:58:00,071 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:58:00,071 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:58:00,071 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "                                           \n",
            " 96% 29698/31000 [8:44:24<11:43,  1.85it/s]\n",
            "{'eval_loss': 0.42266181111335754, 'eval_runtime': 3.7755, 'eval_samples_per_second': 58.006, 'eval_steps_per_second': 1.059, 'epoch': 958.0}\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:58:03,848 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29698\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:58:03,850 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29698/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:58:05,797 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29698/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:58:05,798 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29698/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:58:10,597 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29636] due to args.save_total_limit\n",
            "                                             {'loss': 0.4337, 'learning_rate': 1.800076660984258e-07, 'epoch': 958.06}\n",
            " 96% 29710/31000 [8:44:42<14:22,  1.50it/s]{'loss': 0.4337, 'learning_rate': 1.7725332359739483e-07, 'epoch': 958.39}\n",
            "                                           {'loss': 0.4396, 'learning_rate': 1.7452011624182264e-07, 'epoch': 958.71}\n",
            " 96% 29729/31000 [8:44:53<11:28,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:58:32,569 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:58:32,569 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:58:32,569 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4182209074497223, 'eval_runtime': 3.7592, 'eval_samples_per_second': 58.256, 'eval_steps_per_second': 1.064, 'epoch': 959.0}\n",
            " 96% 29729/31000 [8:44:57<11:28,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:58:36,330 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29729\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:58:36,332 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29729/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:58:38,245 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29729/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:58:38,246 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29729/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:58:42,937 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-28892] due to args.save_total_limit\n",
            " 96% 29730/31000 [8:45:08<1:41:19,  4.79s/it]{'loss': 0.4424, 'learning_rate': 1.718080471420087e-07, 'epoch': 959.03}\n",
            "{'loss': 0.4494, 'learning_rate': 1.6911711938419059e-07, 'epoch': 959.35}\n",
            " 96% 29750/31000 [8:45:19<11:50,  1.76it/s]{'loss': 0.4371, 'learning_rate': 1.664473360305521e-07, 'epoch': 959.68}\n",
            "{'loss': 0.4361, 'learning_rate': 1.637987001192151e-07, 'epoch': 960.0}\n",
            " 96% 29760/31000 [8:45:25<11:09,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 00:59:04,762 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:59:04,762 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:59:04,762 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.80it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.426902174949646, 'eval_runtime': 3.7863, 'eval_samples_per_second': 57.84, 'eval_steps_per_second': 1.056, 'epoch': 960.0}\n",
            " 96% 29760/31000 [8:45:29<11:09,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:59:08,550 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29760\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:59:08,552 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29760/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:59:10,415 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29760/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:59:10,416 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29760/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:59:15,173 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29667] due to args.save_total_limit\n",
            "                                           {'loss': 0.427, 'learning_rate': 1.611712146642352e-07, 'epoch': 960.32}\n",
            "                                           {'loss': 0.455, 'learning_rate': 1.5856488265560407e-07, 'epoch': 960.65}\n",
            "{'loss': 0.4309, 'learning_rate': 1.559797070592325e-07, 'epoch': 960.97}\n",
            " 96% 29791/31000 [8:45:57<11:17,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 00:59:36,996 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 00:59:36,996 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 00:59:36,996 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4298289120197296, 'eval_runtime': 3.7554, 'eval_samples_per_second': 58.316, 'eval_steps_per_second': 1.065, 'epoch': 961.0}\n",
            " 96% 29791/31000 [8:46:01<11:17,  1.78it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 00:59:40,753 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29791\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 00:59:40,754 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29791/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 00:59:42,614 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29791/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 00:59:42,615 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29791/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 00:59:47,170 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29698] due to args.save_total_limit\n",
            "                                           {'loss': 0.4388, 'learning_rate': 1.5341569081696313e-07, 'epoch': 961.29}\n",
            "                                           {'loss': 0.4396, 'learning_rate': 1.5087283684655783e-07, 'epoch': 961.61}\n",
            "                                           {'loss': 0.4381, 'learning_rate': 1.4835114804169976e-07, 'epoch': 961.94}\n",
            " 96% 29822/31000 [8:46:29<10:49,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 01:00:09,290 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:00:09,290 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:00:09,290 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.70it/s]\u001b[A\n",
            "                                           \n",
            " 96% 29822/31000 [8:46:34<10:49,  1.81it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:00:13,036 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29822\n",
            "{'eval_loss': 0.4270392954349518, 'eval_runtime': 3.7449, 'eval_samples_per_second': 58.48, 'eval_steps_per_second': 1.068, 'epoch': 962.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:00:13,038 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29822/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:00:14,969 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29822/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:00:14,969 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29822/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:00:19,547 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29760] due to args.save_total_limit\n",
            " 96% 29830/31000 [8:46:49<18:27,  1.06it/s]{'loss': 0.4418, 'learning_rate': 1.4585062727197894e-07, 'epoch': 962.26}\n",
            "                                           {'loss': 0.4418, 'learning_rate': 1.433712773829046e-07, 'epoch': 962.58}\n",
            " 96% 29850/31000 [8:47:01<10:34,  1.81it/s]{'loss': 0.4382, 'learning_rate': 1.4091310119589276e-07, 'epoch': 962.9}\n",
            " 96% 29853/31000 [8:47:02<10:30,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 01:00:41,491 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:00:41,491 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:00:41,491 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.78it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.97it/s]\u001b[A\n",
            "                                           \n",
            " 96% 29853/31000 [8:47:06<10:30,  1.82it/s]{'eval_loss': 0.42467057704925537, 'eval_runtime': 3.7107, 'eval_samples_per_second': 59.018, 'eval_steps_per_second': 1.078, 'epoch': 963.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:00:45,203 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29853\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:00:45,205 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29853/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:00:47,102 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29853/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:00:47,103 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29853/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:00:51,824 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29791] due to args.save_total_limit\n",
            " 96% 29860/31000 [8:47:20<20:34,  1.08s/it]{'loss': 0.4395, 'learning_rate': 1.3847610150826e-07, 'epoch': 963.23}\n",
            "{'loss': 0.4344, 'learning_rate': 1.3606028109322958e-07, 'epoch': 963.55}\n",
            " 96% 29880/31000 [8:47:32<10:31,  1.77it/s]{'loss': 0.4481, 'learning_rate': 1.3366564269992122e-07, 'epoch': 963.87}\n",
            " 96% 29884/31000 [8:47:34<10:09,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 01:01:13,723 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:01:13,724 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:01:13,724 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4265355169773102, 'eval_runtime': 3.734, 'eval_samples_per_second': 58.651, 'eval_steps_per_second': 1.071, 'epoch': 964.0}\n",
            " 96% 29884/31000 [8:47:38<10:09,  1.83it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:01:17,459 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29884\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:01:17,461 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29884/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:01:19,370 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29884/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:01:19,371 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29884/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:01:23,822 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29822] due to args.save_total_limit\n",
            " 96% 29890/31000 [8:47:51<23:53,  1.29s/it]{'loss': 0.434, 'learning_rate': 1.3129218905335096e-07, 'epoch': 964.19}\n",
            "{'loss': 0.4443, 'learning_rate': 1.289399228544291e-07, 'epoch': 964.52}\n",
            " 96% 29910/31000 [8:48:03<10:28,  1.73it/s]{'loss': 0.4332, 'learning_rate': 1.2660884677994989e-07, 'epoch': 964.84}\n",
            " 96% 29915/31000 [8:48:06<09:52,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 01:01:45,669 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:01:45,669 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:01:45,669 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.22it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.4325712323188782, 'eval_runtime': 3.8091, 'eval_samples_per_second': 57.494, 'eval_steps_per_second': 1.05, 'epoch': 965.0}\n",
            " 96% 29915/31000 [8:48:10<09:52,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:01:49,480 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29915\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:01:49,482 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29915/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:01:51,407 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29915/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:01:51,408 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29915/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:01:56,078 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29853] due to args.save_total_limit\n",
            " 97% 29920/31000 [8:48:26<36:02,  2.00s/it]{'loss': 0.4429, 'learning_rate': 1.242989634826018e-07, 'epoch': 965.16}\n",
            " 97% 29930/31000 [8:48:32<10:46,  1.65it/s]{'loss': 0.4535, 'learning_rate': 1.2201027559094882e-07, 'epoch': 965.48}\n",
            "{'loss': 0.4341, 'learning_rate': 1.1974278570944102e-07, 'epoch': 965.81}\n",
            " 97% 29946/31000 [8:48:40<09:28,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:02:20,242 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:02:20,242 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:02:20,242 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "{'eval_loss': 0.4251581132411957, 'eval_runtime': 3.7713, 'eval_samples_per_second': 58.07, 'eval_steps_per_second': 1.061, 'epoch': 966.0}\n",
            "\n",
            " 97% 29946/31000 [8:48:45<09:28,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:02:24,015 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29946\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:02:24,016 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29946/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:02:25,930 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29946/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:02:25,930 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29946/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:02:30,511 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29884] due to args.save_total_limit\n",
            " 97% 29950/31000 [8:48:57<36:18,  2.07s/it]{'loss': 0.4381, 'learning_rate': 1.1749649641840178e-07, 'epoch': 966.13}\n",
            " 97% 29960/31000 [8:49:03<10:33,  1.64it/s]{'loss': 0.4456, 'learning_rate': 1.152714102740343e-07, 'epoch': 966.45}\n",
            "{'loss': 0.4333, 'learning_rate': 1.1306752980840688e-07, 'epoch': 966.77}\n",
            " 97% 29977/31000 [8:49:13<09:16,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 01:02:52,451 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:02:52,451 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:02:52,451 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                           \n",
            " 97% 29977/31000 [8:49:17<09:16,  1.84it/s]\n",
            "{'eval_loss': 0.42478328943252563, 'eval_runtime': 3.8031, 'eval_samples_per_second': 57.585, 'eval_steps_per_second': 1.052, 'epoch': 967.0}\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:02:56,256 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29977\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:02:56,258 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29977/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:02:58,163 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29977/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:02:58,164 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29977/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:03:03,009 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29915] due to args.save_total_limit\n",
            " 97% 29980/31000 [8:49:29<45:39,  2.69s/it]  {'loss': 0.4436, 'learning_rate': 1.1088485752945923e-07, 'epoch': 967.1}\n",
            "{'loss': 0.4354, 'learning_rate': 1.0872339592099821e-07, 'epoch': 967.42}\n",
            " 97% 30000/31000 [8:49:41<09:37,  1.73it/s]{'loss': 0.4475, 'learning_rate': 1.0658314744269383e-07, 'epoch': 967.74}\n",
            " 97% 30008/31000 [8:49:45<08:57,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:03:25,207 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:03:25,207 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:03:25,207 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "                                           \n",
            " 97% 30008/31000 [8:49:50<08:57,  1.85it/s]\n",
            "{'eval_loss': 0.43078112602233887, 'eval_runtime': 3.8392, 'eval_samples_per_second': 57.043, 'eval_steps_per_second': 1.042, 'epoch': 968.0}\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:03:29,048 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30008\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:03:29,050 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30008/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:03:30,950 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30008/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:03:30,951 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30008/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:03:35,788 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29946] due to args.save_total_limit\n",
            "                                           {'loss': 0.4387, 'learning_rate': 1.0446411453007287e-07, 'epoch': 968.06}\n",
            " 97% 30020/31000 [8:50:08<10:55,  1.50it/s]{'loss': 0.4436, 'learning_rate': 1.0236629959452306e-07, 'epoch': 968.39}\n",
            " 97% 30030/31000 [8:50:13<09:12,  1.76it/s]{'loss': 0.441, 'learning_rate': 1.0028970502328481e-07, 'epoch': 968.71}\n",
            " 97% 30039/31000 [8:50:18<08:38,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:03:57,704 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:03:57,704 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:03:57,704 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "                                           \n",
            " 97% 30039/31000 [8:50:22<08:38,  1.85it/s]\n",
            "                                 {'eval_loss': 0.4278930127620697, 'eval_runtime': 3.8148, 'eval_samples_per_second': 57.409, 'eval_steps_per_second': 1.049, 'epoch': 969.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 01:04:01,521 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30039\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:04:01,523 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30039/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:04:03,469 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30039/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:04:03,470 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30039/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:04:08,370 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29977] due to args.save_total_limit\n",
            " 97% 30040/31000 [8:50:33<1:18:17,  4.89s/it]{'loss': 0.4417, 'learning_rate': 9.82343331794512e-08, 'epoch': 969.03}\n",
            "{'loss': 0.4452, 'learning_rate': 9.620018640196584e-08, 'epoch': 969.35}\n",
            " 97% 30060/31000 [8:50:45<08:59,  1.74it/s]{'loss': 0.439, 'learning_rate': 9.418726700561666e-08, 'epoch': 969.68}\n",
            " 97% 30070/31000 [8:50:51<08:22,  1.85it/s]{'loss': 0.4342, 'learning_rate': 9.219557728103594e-08, 'epoch': 970.0}\n",
            " 97% 30070/31000 [8:50:51<08:22,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:04:30,329 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:04:30,329 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:04:30,329 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.85it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "\u001b[A{'eval_loss': 0.43100184202194214, 'eval_runtime': 3.7329, 'eval_samples_per_second': 58.668, 'eval_steps_per_second': 1.072, 'epoch': 970.0}\n",
            "                                           \n",
            " 97% 30070/31000 [8:50:55<08:22,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:04:34,063 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30070\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:04:34,065 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30070/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:04:35,950 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30070/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:04:35,951 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30070/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:04:40,760 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30008] due to args.save_total_limit\n",
            " 97% 30080/31000 [8:51:11<11:47,  1.30it/s]{'loss': 0.4405, 'learning_rate': 9.022511949470026e-08, 'epoch': 970.32}\n",
            "{'loss': 0.4443, 'learning_rate': 8.827589588891806e-08, 'epoch': 970.65}\n",
            " 97% 30100/31000 [8:51:23<08:14,  1.82it/s]{'loss': 0.4371, 'learning_rate': 8.634790868184416e-08, 'epoch': 970.97}\n",
            " 97% 30101/31000 [8:51:23<08:29,  1.76it/s][INFO|trainer.py:2625] 2022-05-19 01:05:02,940 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:05:02,940 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:05:02,940 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.68it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "{'eval_loss': 0.4280080795288086, 'eval_runtime': 3.7807, 'eval_samples_per_second': 57.926, 'eval_steps_per_second': 1.058, 'epoch': 971.0}\n",
            "\n",
            " 97% 30101/31000 [8:51:27<08:29,  1.76it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:05:06,722 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30101\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:05:06,724 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30101/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:05:08,605 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30101/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:05:08,606 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30101/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:05:13,499 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30039] due to args.save_total_limit\n",
            " 97% 30110/31000 [8:51:43<12:43,  1.17it/s]{'loss': 0.4396, 'learning_rate': 8.444116006745688e-08, 'epoch': 971.29}\n",
            " 97% 30120/31000 [8:51:49<08:34,  1.71it/s]{'loss': 0.4454, 'learning_rate': 8.255565221557265e-08, 'epoch': 971.61}\n",
            " 97% 30130/31000 [8:51:55<07:52,  1.84it/s]{'loss': 0.4336, 'learning_rate': 8.069138727183552e-08, 'epoch': 971.94}\n",
            " 97% 30132/31000 [8:51:56<07:59,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 01:05:35,522 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:05:35,523 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:05:35,523 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                           \n",
            " 97% 30132/31000 [8:52:00<07:59,  1.81it/s]\n",
            "                                 {'eval_loss': 0.4329468905925751, 'eval_runtime': 3.7644, 'eval_samples_per_second': 58.177, 'eval_steps_per_second': 1.063, 'epoch': 972.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 01:05:39,289 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30132\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:05:39,290 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30132/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:05:41,176 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30132/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:05:41,177 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30132/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:05:45,861 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30070] due to args.save_total_limit\n",
            "                                           {'loss': 0.4329, 'learning_rate': 7.884836735771103e-08, 'epoch': 972.26}\n",
            " 97% 30150/31000 [8:52:21<08:26,  1.68it/s]{'loss': 0.4344, 'learning_rate': 7.702659457049648e-08, 'epoch': 972.58}\n",
            "                                           {'loss': 0.4412, 'learning_rate': 7.52260709833044e-08, 'epoch': 972.9}\n",
            " 97% 30163/31000 [8:52:28<07:46,  1.79it/s][INFO|trainer.py:2625] 2022-05-19 01:06:08,089 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:06:08,089 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:06:08,089 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                           \n",
            " 97% 30163/31000 [8:52:32<07:46,  1.79it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:06:11,854 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30163\n",
            "{'eval_loss': 0.424812376499176, 'eval_runtime': 3.7633, 'eval_samples_per_second': 58.193, 'eval_steps_per_second': 1.063, 'epoch': 973.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:06:11,855 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30163/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:06:13,821 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30163/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:06:13,822 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30163/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:06:18,504 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30101] due to args.save_total_limit\n",
            " 97% 30170/31000 [8:52:47<15:00,  1.09s/it]{'loss': 0.4362, 'learning_rate': 7.344679864506667e-08, 'epoch': 973.23}\n",
            "{'loss': 0.4425, 'learning_rate': 7.168877958053861e-08, 'epoch': 973.55}\n",
            " 97% 30190/31000 [8:52:59<07:59,  1.69it/s]{'loss': 0.4399, 'learning_rate': 6.99520157902804e-08, 'epoch': 973.87}\n",
            " 97% 30194/31000 [8:53:01<07:26,  1.80it/s][INFO|trainer.py:2625] 2022-05-19 01:06:40,975 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:06:40,975 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:06:40,976 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "                                           \n",
            " 97% 30194/31000 [8:53:05<07:26,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A{'eval_loss': 0.42328596115112305, 'eval_runtime': 3.8433, 'eval_samples_per_second': 56.982, 'eval_steps_per_second': 1.041, 'epoch': 974.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:06:44,820 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30194\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:06:44,822 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30194/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:06:46,801 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30194/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:06:46,802 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30194/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:06:51,894 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30132] due to args.save_total_limit\n",
            "{'loss': 0.4567, 'learning_rate': 6.823650925067564e-08, 'epoch': 974.19}\n",
            "                                           {'loss': 0.4345, 'learning_rate': 6.654226191390443e-08, 'epoch': 974.52}\n",
            "                                           {'loss': 0.4367, 'learning_rate': 6.486927570796413e-08, 'epoch': 974.84}\n",
            " 98% 30225/31000 [8:53:34<07:03,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 01:07:14,222 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:07:14,222 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:07:14,222 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A{'eval_loss': 0.43356794118881226, 'eval_runtime': 3.8281, 'eval_samples_per_second': 57.208, 'eval_steps_per_second': 1.045, 'epoch': 975.0}\n",
            "                                           \n",
            " 98% 30225/31000 [8:53:39<07:03,  1.83it/s]\n",
            "100% 4/4 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:07:18,052 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30225\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:07:18,054 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30225/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:07:19,975 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30225/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:07:19,977 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30225/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:07:24,917 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30163] due to args.save_total_limit\n",
            "{'loss': 0.4435, 'learning_rate': 6.321755253665892e-08, 'epoch': 975.16}\n",
            " 98% 30240/31000 [8:53:59<07:47,  1.63it/s]{'loss': 0.4338, 'learning_rate': 6.158709427958943e-08, 'epoch': 975.48}\n",
            "{'loss': 0.4401, 'learning_rate': 5.99779027921611e-08, 'epoch': 975.81}\n",
            " 98% 30256/31000 [8:54:08<06:53,  1.80it/s][INFO|trainer.py:2625] 2022-05-19 01:07:47,534 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:07:47,535 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:07:47,535 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "                                           \n",
            " 98% 30256/31000 [8:54:12<06:53,  1.80it/s]\n",
            "100% 4/4 [00:00<00:00,  8.92it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:07:51,428 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30256\n",
            "{'eval_loss': 0.43175098299980164, 'eval_runtime': 3.8916, 'eval_samples_per_second': 56.275, 'eval_steps_per_second': 1.028, 'epoch': 976.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:07:51,429 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30256/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:07:53,295 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30256/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:07:53,296 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30256/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:07:58,564 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30194] due to args.save_total_limit\n",
            " 98% 30260/31000 [8:54:25<25:58,  2.11s/it]{'loss': 0.4456, 'learning_rate': 5.838997990557995e-08, 'epoch': 976.13}\n",
            "{'loss': 0.4405, 'learning_rate': 5.682332742684634e-08, 'epoch': 976.45}\n",
            "                                           {'loss': 0.4403, 'learning_rate': 5.527794713875502e-08, 'epoch': 976.77}\n",
            " 98% 30287/31000 [8:54:41<06:29,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 01:08:20,646 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:08:20,646 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:08:20,646 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "{'eval_loss': 0.42629653215408325, 'eval_runtime': 3.8031, 'eval_samples_per_second': 57.585, 'eval_steps_per_second': 1.052, 'epoch': 977.0}\n",
            "\n",
            " 98% 30287/31000 [8:54:45<06:29,  1.83it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:08:24,451 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30287\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:08:24,453 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30287/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:08:26,354 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30287/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:08:26,355 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30287/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:08:31,071 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30225] due to args.save_total_limit\n",
            "                                           {'loss': 0.4394, 'learning_rate': 5.375384079989928e-08, 'epoch': 977.1}\n",
            " 98% 30300/31000 [8:55:03<07:25,  1.57it/s]{'loss': 0.4339, 'learning_rate': 5.2251010144658403e-08, 'epoch': 977.42}\n",
            " 98% 30310/31000 [8:55:09<06:44,  1.70it/s]{'loss': 0.4444, 'learning_rate': 5.0769456883199834e-08, 'epoch': 977.74}\n",
            " 98% 30318/31000 [8:55:13<06:08,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:08:53,169 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:08:53,169 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:08:53,170 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "                                           \n",
            " 98% 30318/31000 [8:55:18<06:08,  1.85it/s]{'eval_loss': 0.42669251561164856, 'eval_runtime': 3.8044, 'eval_samples_per_second': 57.565, 'eval_steps_per_second': 1.051, 'epoch': 978.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:08:56,976 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30318\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:08:56,977 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30318/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:08:58,922 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30318/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:08:58,923 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30318/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:09:05,543 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30256] due to args.save_total_limit\n",
            "{'loss': 0.4463, 'learning_rate': 4.930918270148118e-08, 'epoch': 978.06}\n",
            " 98% 30330/31000 [8:55:37<07:38,  1.46it/s]{'loss': 0.4408, 'learning_rate': 4.787018926124611e-08, 'epoch': 978.39}\n",
            "{'loss': 0.4439, 'learning_rate': 4.645247820001599e-08, 'epoch': 978.71}\n",
            " 98% 30349/31000 [8:55:48<05:52,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:09:27,598 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:09:27,599 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:09:27,599 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.61it/s]\u001b[A\n",
            "                                           \n",
            " 98% 30349/31000 [8:55:52<05:52,  1.85it/s]\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]{'eval_loss': 0.42219439148902893, 'eval_runtime': 3.771, 'eval_samples_per_second': 58.074, 'eval_steps_per_second': 1.061, 'epoch': 979.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:09:31,371 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30349\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:09:31,373 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30349/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:09:33,292 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30349/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:09:33,293 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30349/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:09:38,274 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30287] due to args.save_total_limit\n",
            "                                           {'loss': 0.4309, 'learning_rate': 4.505605113110031e-08, 'epoch': 979.03}\n",
            "                                           {'loss': 0.4428, 'learning_rate': 4.368090964358417e-08, 'epoch': 979.35}\n",
            "{'loss': 0.4382, 'learning_rate': 4.2327055302328327e-08, 'epoch': 979.68}\n",
            "                                           {'loss': 0.4337, 'learning_rate': 4.099448964797747e-08, 'epoch': 980.0}\n",
            " 98% 30380/31000 [8:56:21<05:33,  1.86it/s][INFO|trainer.py:2625] 2022-05-19 01:10:00,806 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:10:00,806 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:10:00,806 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.91it/s]\u001b[A\n",
            "                                           \n",
            " 98% 30380/31000 [8:56:25<05:33,  1.86it/s]\n",
            "                                 {'eval_loss': 0.42924466729164124, 'eval_runtime': 3.8225, 'eval_samples_per_second': 57.293, 'eval_steps_per_second': 1.046, 'epoch': 980.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 01:10:04,630 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30380\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:10:04,632 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30380/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:10:06,530 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30380/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:10:06,531 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30380/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:10:11,505 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30318] due to args.save_total_limit\n",
            "{'loss': 0.4398, 'learning_rate': 3.968321419694151e-08, 'epoch': 980.32}\n",
            "{'loss': 0.4367, 'learning_rate': 3.8393230441405994e-08, 'epoch': 980.65}\n",
            "{'loss': 0.4443, 'learning_rate': 3.7124539849332084e-08, 'epoch': 980.97}\n",
            " 98% 30411/31000 [8:56:54<05:31,  1.78it/s][INFO|trainer.py:2625] 2022-05-19 01:10:33,537 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:10:33,538 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:10:33,538 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.74it/s]\u001b[A\n",
            "                                           \n",
            " 98% 30411/31000 [8:56:58<05:31,  1.78it/s]\n",
            "100% 4/4 [00:00<00:00,  8.90it/s]{'eval_loss': 0.43100571632385254, 'eval_runtime': 3.8409, 'eval_samples_per_second': 57.018, 'eval_steps_per_second': 1.041, 'epoch': 981.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:10:37,380 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30411\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:10:37,382 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30411/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:10:39,295 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30411/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:10:39,296 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30411/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:10:44,177 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30349] due to args.save_total_limit\n",
            "{'loss': 0.4411, 'learning_rate': 3.5877143864442016e-08, 'epoch': 981.29}\n",
            "                                           {'loss': 0.452, 'learning_rate': 3.4651043906233636e-08, 'epoch': 981.61}\n",
            " 98% 30440/31000 [8:57:26<05:05,  1.83it/s]{'loss': 0.4351, 'learning_rate': 3.3446241369961685e-08, 'epoch': 981.94}\n",
            " 98% 30442/31000 [8:57:27<05:08,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 01:11:06,330 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:11:06,331 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:11:06,331 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4336245357990265, 'eval_runtime': 3.7729, 'eval_samples_per_second': 58.045, 'eval_steps_per_second': 1.06, 'epoch': 982.0}\n",
            " 98% 30442/31000 [8:57:31<05:08,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:11:10,105 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30442\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:11:10,107 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30442/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:11:11,985 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30442/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:11:11,986 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30442/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:11:16,751 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30380] due to args.save_total_limit\n",
            " 98% 30450/31000 [8:57:46<08:37,  1.06it/s]{'loss': 0.4399, 'learning_rate': 3.2262737626650295e-08, 'epoch': 982.26}\n",
            "                                           {'loss': 0.4341, 'learning_rate': 3.11005340230909e-08, 'epoch': 982.58}\n",
            "                                           {'loss': 0.4417, 'learning_rate': 2.995963188182765e-08, 'epoch': 982.9}\n",
            " 98% 30473/31000 [8:57:59<04:50,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 01:11:38,811 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:11:38,811 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:11:38,811 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                           \n",
            " 98% 30473/31000 [8:58:03<04:50,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:11:42,692 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30473\n",
            "{'eval_loss': 0.42574119567871094, 'eval_runtime': 3.8794, 'eval_samples_per_second': 56.452, 'eval_steps_per_second': 1.031, 'epoch': 983.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:11:42,694 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30473/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:11:44,662 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30473/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:11:44,663 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30473/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:11:49,717 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30411] due to args.save_total_limit\n",
            " 98% 30480/31000 [8:58:19<09:35,  1.11s/it]{'loss': 0.4521, 'learning_rate': 2.884003250116576e-08, 'epoch': 983.23}\n",
            " 98% 30490/31000 [8:58:24<05:09,  1.65it/s]{'loss': 0.4393, 'learning_rate': 2.7741737155175665e-08, 'epoch': 983.55}\n",
            "{'loss': 0.4309, 'learning_rate': 2.6664747093676358e-08, 'epoch': 983.87}\n",
            " 98% 30504/31000 [8:58:32<04:31,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 01:12:11,812 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:12:11,812 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:12:11,812 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.58it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4183427691459656, 'eval_runtime': 3.7768, 'eval_samples_per_second': 57.986, 'eval_steps_per_second': 1.059, 'epoch': 984.0}\n",
            " 98% 30504/31000 [8:58:36<04:31,  1.83it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:12:15,590 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30504\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:12:15,593 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30504/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:12:17,515 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30504/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:12:17,516 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30504/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:12:22,456 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30442] due to args.save_total_limit\n",
            "{'loss': 0.4435, 'learning_rate': 2.5609063542243737e-08, 'epoch': 984.19}\n",
            "                                           {'loss': 0.4398, 'learning_rate': 2.457468770221474e-08, 'epoch': 984.52}\n",
            "{'loss': 0.4381, 'learning_rate': 2.356162075066864e-08, 'epoch': 984.84}\n",
            " 98% 30535/31000 [8:59:05<04:15,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 01:12:44,429 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:12:44,429 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:12:44,429 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                           \n",
            " 98% 30535/31000 [8:59:09<04:15,  1.82it/s]{'eval_loss': 0.4314035475254059, 'eval_runtime': 3.8283, 'eval_samples_per_second': 57.205, 'eval_steps_per_second': 1.045, 'epoch': 985.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:12:48,259 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30535\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:12:48,261 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30535/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:12:50,164 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30535/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:12:50,165 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30535/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:12:55,294 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30473] due to args.save_total_limit\n",
            " 99% 30540/31000 [8:59:23<13:04,  1.71s/it]{'loss': 0.4285, 'learning_rate': 2.2569863840441597e-08, 'epoch': 985.16}\n",
            " 99% 30550/31000 [8:59:29<04:39,  1.61it/s]{'loss': 0.437, 'learning_rate': 2.159941810012042e-08, 'epoch': 985.48}\n",
            "                                           {'loss': 0.4591, 'learning_rate': 2.0650284634038394e-08, 'epoch': 985.81}\n",
            " 99% 30566/31000 [8:59:38<03:56,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 01:13:17,565 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:13:17,565 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:13:17,565 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A{'eval_loss': 0.43000245094299316, 'eval_runtime': 3.8403, 'eval_samples_per_second': 57.026, 'eval_steps_per_second': 1.042, 'epoch': 986.0}\n",
            " 99% 30566/31000 [8:59:42<03:56,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:13:21,407 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30566\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:13:21,409 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30566/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:13:23,309 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30566/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:13:23,310 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30566/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:13:28,678 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30504] due to args.save_total_limit\n",
            " 99% 30570/31000 [8:59:55<15:14,  2.13s/it]{'loss': 0.4342, 'learning_rate': 1.972246452227738e-08, 'epoch': 986.13}\n",
            "                                           {'loss': 0.4434, 'learning_rate': 1.881595882066156e-08, 'epoch': 986.45}\n",
            "                                           {'loss': 0.4333, 'learning_rate': 1.793076856076575e-08, 'epoch': 986.77}\n",
            " 99% 30597/31000 [9:00:11<03:39,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 01:13:50,858 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:13:50,858 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:13:50,858 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\u001b[A{'eval_loss': 0.4274001717567444, 'eval_runtime': 3.8442, 'eval_samples_per_second': 56.97, 'eval_steps_per_second': 1.041, 'epoch': 987.0}\n",
            " 99% 30597/31000 [9:00:15<03:39,  1.84it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:13:54,704 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30597\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:13:54,706 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30597/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:13:56,630 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30597/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:13:56,631 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30597/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:14:01,533 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30535] due to args.save_total_limit\n",
            "                                           {'loss': 0.449, 'learning_rate': 1.7066894749905025e-08, 'epoch': 987.1}\n",
            " 99% 30610/31000 [9:00:34<04:08,  1.57it/s]{'loss': 0.444, 'learning_rate': 1.6224338371140928e-08, 'epoch': 987.42}\n",
            "{'loss': 0.4287, 'learning_rate': 1.5403100383269003e-08, 'epoch': 987.74}\n",
            " 99% 30628/31000 [9:00:44<03:21,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 01:14:23,605 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:14:23,605 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:14:23,605 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                           \n",
            " 99% 30628/31000 [9:00:48<03:21,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:14:27,389 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30628\n",
            "{'eval_loss': 0.4304349720478058, 'eval_runtime': 3.7825, 'eval_samples_per_second': 57.898, 'eval_steps_per_second': 1.057, 'epoch': 988.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:14:27,391 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30628/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:14:29,298 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30628/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:14:29,299 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30628/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:14:35,600 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30566] due to args.save_total_limit\n",
            "{'loss': 0.4511, 'learning_rate': 1.4603181720831275e-08, 'epoch': 988.06}\n",
            "{'loss': 0.4349, 'learning_rate': 1.3824583294110015e-08, 'epoch': 988.39}\n",
            "                                           {'loss': 0.4411, 'learning_rate': 1.3067305989119397e-08, 'epoch': 988.71}\n",
            " 99% 30659/31000 [9:01:18<03:03,  1.86it/s][INFO|trainer.py:2625] 2022-05-19 01:14:57,481 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:14:57,482 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:14:57,482 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "                                           \n",
            " 99% 30659/31000 [9:01:22<03:03,  1.86it/s]\n",
            "{'eval_loss': 0.4308498799800873, 'eval_runtime': 3.7755, 'eval_samples_per_second': 58.005, 'eval_steps_per_second': 1.059, 'epoch': 989.0}\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:15:01,259 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30659\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:15:01,260 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30659/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:15:03,151 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30659/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:15:03,151 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30659/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:15:08,053 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30597] due to args.save_total_limit\n",
            "                                           {'loss': 0.446, 'learning_rate': 1.2331350667618006e-08, 'epoch': 989.03}\n",
            " 99% 30670/31000 [9:01:39<03:54,  1.41it/s]{'loss': 0.4399, 'learning_rate': 1.1616718167096334e-08, 'epoch': 989.35}\n",
            "{'loss': 0.4437, 'learning_rate': 1.0923409300783032e-08, 'epoch': 989.68}\n",
            "{'loss': 0.4351, 'learning_rate': 1.025142485763658e-08, 'epoch': 990.0}\n",
            " 99% 30690/31000 [9:01:50<02:46,  1.86it/s][INFO|trainer.py:2625] 2022-05-19 01:15:29,861 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:15:29,861 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:15:29,861 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.69it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.86it/s]\u001b[A{'eval_loss': 0.427616149187088, 'eval_runtime': 3.8579, 'eval_samples_per_second': 56.766, 'eval_steps_per_second': 1.037, 'epoch': 990.0}\n",
            " 99% 30690/31000 [9:01:54<02:46,  1.86it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:15:33,721 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30690\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:15:33,724 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30690/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:15:35,662 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30690/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:15:35,663 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30690/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:15:40,639 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30628] due to args.save_total_limit\n",
            " 99% 30700/31000 [9:02:11<03:49,  1.31it/s]{'loss': 0.4425, 'learning_rate': 9.600765602355704e-09, 'epoch': 990.32}\n",
            "                                           {'loss': 0.4324, 'learning_rate': 8.97143227536687e-09, 'epoch': 990.65}\n",
            "{'loss': 0.4443, 'learning_rate': 8.36342559282846e-09, 'epoch': 990.97}\n",
            " 99% 30721/31000 [9:02:23<02:36,  1.79it/s][INFO|trainer.py:2625] 2022-05-19 01:16:02,662 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:16:02,662 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:16:02,662 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.73it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.42681068181991577, 'eval_runtime': 3.7074, 'eval_samples_per_second': 59.072, 'eval_steps_per_second': 1.079, 'epoch': 991.0}\n",
            " 99% 30721/31000 [9:02:27<02:36,  1.79it/s]\n",
            "100% 4/4 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:16:06,371 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30721\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:16:06,373 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30721/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:16:08,320 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30721/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:16:08,321 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30721/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:16:13,067 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30659] due to args.save_total_limit\n",
            " 99% 30730/31000 [9:02:43<03:44,  1.20it/s]{'loss': 0.4402, 'learning_rate': 7.776746246630771e-09, 'epoch': 991.29}\n",
            "                                           {'loss': 0.4463, 'learning_rate': 7.211394904398093e-09, 'epoch': 991.61}\n",
            "{'loss': 0.4369, 'learning_rate': 6.667372209476218e-09, 'epoch': 991.94}\n",
            " 99% 30752/31000 [9:02:55<02:17,  1.81it/s][INFO|trainer.py:2625] 2022-05-19 01:16:35,057 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:16:35,057 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:16:35,057 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.60it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "                                           \n",
            " 99% 30752/31000 [9:03:00<02:17,  1.81it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:16:38,900 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30752\n",
            "{'eval_loss': 0.4269988238811493, 'eval_runtime': 3.841, 'eval_samples_per_second': 57.016, 'eval_steps_per_second': 1.041, 'epoch': 992.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:16:38,901 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30752/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:16:40,822 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30752/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:16:40,823 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30752/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:16:45,858 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30690] due to args.save_total_limit\n",
            " 99% 30760/31000 [9:03:15<03:46,  1.06it/s]{'loss': 0.4445, 'learning_rate': 6.144678780949097e-09, 'epoch': 992.26}\n",
            "{'loss': 0.437, 'learning_rate': 5.643315213620103e-09, 'epoch': 992.58}\n",
            "{'loss': 0.4371, 'learning_rate': 5.163282078024522e-09, 'epoch': 992.9}\n",
            " 99% 30783/31000 [9:03:28<01:59,  1.82it/s][INFO|trainer.py:2625] 2022-05-19 01:17:07,876 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:17:07,877 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:17:07,877 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.65it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "                                           \n",
            " 99% 30783/31000 [9:03:32<01:59,  1.82it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:17:11,691 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30783\n",
            "{'eval_loss': 0.4261305630207062, 'eval_runtime': 3.8131, 'eval_samples_per_second': 57.434, 'eval_steps_per_second': 1.049, 'epoch': 993.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:17:11,693 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30783/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:17:13,563 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30783/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:17:13,564 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30783/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:17:18,228 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30721] due to args.save_total_limit\n",
            "                                           {'loss': 0.4404, 'learning_rate': 4.704579920425389e-09, 'epoch': 993.23}\n",
            " 99% 30800/31000 [9:03:53<01:56,  1.72it/s]{'loss': 0.4511, 'learning_rate': 4.2672092628072415e-09, 'epoch': 993.55}\n",
            "{'loss': 0.4372, 'learning_rate': 3.8511706028844514e-09, 'epoch': 993.87}\n",
            " 99% 30814/31000 [9:04:00<01:41,  1.83it/s][INFO|trainer.py:2625] 2022-05-19 01:17:40,142 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:17:40,143 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:17:40,143 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.54it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                           \n",
            " 99% 30814/31000 [9:04:05<01:41,  1.83it/s]{'eval_loss': 0.42984893918037415, 'eval_runtime': 3.7757, 'eval_samples_per_second': 58.003, 'eval_steps_per_second': 1.059, 'epoch': 994.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:17:43,920 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30814\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:17:43,922 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30814/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:17:45,864 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30814/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:17:45,865 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30814/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:17:50,701 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30752] due to args.save_total_limit\n",
            " 99% 30820/31000 [9:04:18<03:57,  1.32s/it]{'loss': 0.4358, 'learning_rate': 3.4564644140928908e-09, 'epoch': 994.19}\n",
            "                                           {'loss': 0.4415, 'learning_rate': 3.0830911455982654e-09, 'epoch': 994.52}\n",
            "{'loss': 0.4405, 'learning_rate': 2.73105122228362e-09, 'epoch': 994.84}\n",
            "100% 30845/31000 [9:04:33<01:26,  1.79it/s][INFO|trainer.py:2625] 2022-05-19 01:18:12,716 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:18:12,716 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:18:12,716 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.67it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.434014230966568, 'eval_runtime': 3.8398, 'eval_samples_per_second': 57.035, 'eval_steps_per_second': 1.042, 'epoch': 995.0}\n",
            "100% 30845/31000 [9:04:37<01:26,  1.79it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:18:16,558 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30845\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:18:16,560 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30845/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:18:18,483 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30845/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:18:18,484 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30845/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:18:23,429 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30783] due to args.save_total_limit\n",
            "100% 30850/31000 [9:04:51<04:07,  1.65s/it]{'loss': 0.442, 'learning_rate': 2.4003450447576676e-09, 'epoch': 995.16}\n",
            "100% 30860/31000 [9:04:56<01:26,  1.61it/s]{'loss': 0.4366, 'learning_rate': 2.090972989354789e-09, 'epoch': 995.48}\n",
            "{'loss': 0.4401, 'learning_rate': 1.8029354081287861e-09, 'epoch': 995.81}\n",
            "100% 30876/31000 [9:05:06<01:06,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:18:45,345 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:18:45,346 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:18:45,346 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.72it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 0.4198409616947174, 'eval_runtime': 3.8171, 'eval_samples_per_second': 57.373, 'eval_steps_per_second': 1.048, 'epoch': 996.0}\n",
            "100% 30876/31000 [9:05:10<01:06,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:18:49,164 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30876\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:18:49,166 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30876/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:18:51,062 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30876/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:18:51,063 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30876/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:18:55,978 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30814] due to args.save_total_limit\n",
            "100% 30880/31000 [9:05:23<04:06,  2.06s/it]{'loss': 0.4405, 'learning_rate': 1.5362326288549665e-09, 'epoch': 996.13}\n",
            "100% 30890/31000 [9:05:29<01:09,  1.58it/s]{'loss': 0.4314, 'learning_rate': 1.2908649550343043e-09, 'epoch': 996.45}\n",
            "                                           {'loss': 0.4393, 'learning_rate': 1.0668326658871962e-09, 'epoch': 996.77}\n",
            "100% 30907/31000 [9:05:38<00:50,  1.84it/s][INFO|trainer.py:2625] 2022-05-19 01:19:17,992 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:19:17,992 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:19:17,993 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "                                           \n",
            "100% 30907/31000 [9:05:42<00:50,  1.84it/s]\n",
            "100% 4/4 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "{'eval_loss': 0.4251507520675659, 'eval_runtime': 3.8103, 'eval_samples_per_second': 57.476, 'eval_steps_per_second': 1.05, 'epoch': 997.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:19:21,804 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30907\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:19:21,806 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30907/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:19:23,697 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30907/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:19:23,698 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30907/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:19:28,693 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30845] due to args.save_total_limit\n",
            "100% 30910/31000 [9:05:55<04:03,  2.70s/it]{'loss': 0.4444, 'learning_rate': 8.6413601635138e-10, 'epoch': 997.1}\n",
            "                                           {'loss': 0.4486, 'learning_rate': 6.827752370923423e-10, 'epoch': 997.42}\n",
            "{'loss': 0.4377, 'learning_rate': 5.227505344887473e-10, 'epoch': 997.74}\n",
            "100% 30938/31000 [9:06:11<00:33,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:19:50,813 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:19:50,813 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:19:50,813 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "{'eval_loss': 0.42607465386390686, 'eval_runtime': 3.8386, 'eval_samples_per_second': 57.052, 'eval_steps_per_second': 1.042, 'epoch': 998.0}\n",
            "\n",
            "100% 30938/31000 [9:06:15<00:33,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:19:54,653 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30938\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:19:54,655 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30938/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:19:56,604 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30938/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:19:56,605 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30938/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:20:01,654 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30876] due to args.save_total_limit\n",
            "100% 30940/31000 [9:06:27<03:38,  3.64s/it]{'loss': 0.434, 'learning_rate': 3.8406209064492653e-10, 'epoch': 998.06}\n",
            "                                           {'loss': 0.4651, 'learning_rate': 2.6671006338463417e-10, 'epoch': 998.39}\n",
            "100% 30960/31000 [9:06:39<00:22,  1.76it/s]{'loss': 0.4222, 'learning_rate': 1.7069458624896503e-10, 'epoch': 998.71}\n",
            "100% 30969/31000 [9:06:44<00:16,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:20:23,609 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:20:23,609 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:20:23,609 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.77it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.87it/s]\u001b[A{'eval_loss': 0.4211817979812622, 'eval_runtime': 3.7292, 'eval_samples_per_second': 58.725, 'eval_steps_per_second': 1.073, 'epoch': 999.0}\n",
            "\n",
            "                                           \n",
            "100% 30969/31000 [9:06:48<00:16,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:20:27,340 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30969\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:20:27,342 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30969/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:20:29,252 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30969/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:20:29,253 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30969/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:20:35,456 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30907] due to args.save_total_limit\n",
            "100% 30970/31000 [9:07:00<02:37,  5.26s/it]{'loss': 0.4358, 'learning_rate': 9.601576850259973e-11, 'epoch': 999.03}\n",
            "100% 30980/31000 [9:07:07<00:14,  1.38it/s]{'loss': 0.4382, 'learning_rate': 4.2673695123396355e-11, 'epoch': 999.35}\n",
            "100% 30990/31000 [9:07:12<00:05,  1.76it/s]{'loss': 0.4461, 'learning_rate': 1.0668426816962117e-11, 'epoch': 999.68}\n",
            "100% 31000/31000 [9:07:18<00:00,  1.85it/s]{'loss': 0.4371, 'learning_rate': 0.0, 'epoch': 1000.0}\n",
            "100% 31000/31000 [9:07:18<00:00,  1.85it/s][INFO|trainer.py:2625] 2022-05-19 01:20:57,439 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:20:57,439 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:20:57,439 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00, 11.64it/s]\u001b[A\n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                           \n",
            "100% 4/4 [00:00<00:00,  8.81it/s]\u001b[A{'eval_loss': 0.42821770906448364, 'eval_runtime': 3.8517, 'eval_samples_per_second': 56.859, 'eval_steps_per_second': 1.039, 'epoch': 1000.0}\n",
            "100% 31000/31000 [9:07:22<00:00,  1.85it/s]\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 01:21:01,292 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae/checkpoint-31000\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:21:01,294 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-31000/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:21:03,187 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-31000/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:21:03,188 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/checkpoint-31000/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 01:21:08,072 >> Deleting older checkpoint [orchid219_pretrain_data2vec-vision-base-mae/checkpoint-30938] due to args.save_total_limit\n",
            "[INFO|trainer.py:1671] 2022-05-19 01:21:08,239 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1744] 2022-05-19 01:21:08,239 >> Loading best model from orchid219_pretrain_data2vec-vision-base-mae/checkpoint-29729 (score: 0.4182209074497223).\n",
            "100% 31000/31000 [9:07:39<00:00,  1.85it/s]{'train_runtime': 32863.333, 'train_samples_per_second': 59.976, 'train_steps_per_second': 0.943, 'train_loss': 0.4802593483771047, 'epoch': 1000.0}\n",
            "100% 31000/31000 [9:07:39<00:00,  1.06s/it]\n",
            "[INFO|trainer.py:2345] 2022-05-19 01:21:18,460 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:21:18,462 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:21:21,047 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:21:21,048 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/preprocessor_config.json\n",
            "[INFO|trainer.py:2345] 2022-05-19 01:21:21,048 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:21:21,050 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:21:23,107 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:21:23,108 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/preprocessor_config.json\n",
            "Several commits (2) will be pushed upstream.\n",
            "WARNING:huggingface_hub.repository:Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n",
            "WARNING:huggingface_hub.repository:The progress bars may be unreliable.\n",
            "Upload file pytorch_model.bin:   0% 3.34k/427M [00:00<?, ?B/s]\n",
            "Upload file pytorch_model.bin:   0% 507k/427M [00:01<14:28, 515kB/s]\n",
            "Upload file pytorch_model.bin:   4% 19.2M/427M [00:18<05:56, 1.20MB/s]\n",
            "Upload file pytorch_model.bin: 100% 427M/427M [06:18<00:00, 1.15MB/s]remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_data2vec-vision-base-mae\n",
            "   67d0791..76a6b9e  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_data2vec-vision-base-mae\n",
            "   67d0791..76a6b9e  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 427M/427M [06:19<00:00, 1.18MB/s]\n",
            "\n",
            "Upload file runs/May18_16-10-11_b937b5e1c6ef/events.out.tfevents.1652890415.b937b5e1c6ef.1308.0: 100% 751k/751k [06:19<00:00, 1.68kB/s]\u001b[A\n",
            "Upload file runs/May18_16-10-11_b937b5e1c6ef/events.out.tfevents.1652890415.b937b5e1c6ef.1308.0: 100% 751k/751k [06:19<00:00, 2.02kB/s]\n",
            "[INFO|modelcard.py:460] 2022-05-19 01:27:58,169 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'dataset': {'name': 'orchid219', 'type': 'orchid219', 'args': 'orchid219'}}\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_data2vec-vision-base-mae\n",
            "   76a6b9e..3e12ec3  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_data2vec-vision-base-mae\n",
            "   76a6b9e..3e12ec3  main -> main\n",
            "\n",
            "***** train metrics *****\n",
            "  epoch                    =     1000.0\n",
            "  train_loss               =     0.4803\n",
            "  train_runtime            = 9:07:43.33\n",
            "  train_samples_per_second =     59.976\n",
            "  train_steps_per_second   =      0.943\n",
            "[INFO|trainer.py:2625] 2022-05-19 01:28:07,090 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 01:28:07,091 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 01:28:07,091 >>   Batch size = 64\n",
            "100% 4/4 [00:00<00:00,  8.49it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     1000.0\n",
            "  eval_loss               =     0.4275\n",
            "  eval_runtime            = 0:00:04.05\n",
            "  eval_samples_per_second =     54.034\n",
            "  eval_steps_per_second   =      0.987\n",
            "[INFO|trainer.py:2345] 2022-05-19 01:28:11,181 >> Saving model checkpoint to orchid219_pretrain_data2vec-vision-base-mae\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 01:28:11,183 >> Configuration saved in orchid219_pretrain_data2vec-vision-base-mae/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 01:28:13,483 >> Model weights saved in orchid219_pretrain_data2vec-vision-base-mae/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 01:28:13,484 >> Feature extractor saved in orchid219_pretrain_data2vec-vision-base-mae/preprocessor_config.json\n",
            "Upload file pytorch_model.bin:   0% 3.34k/427M [00:00<?, ?B/s]\n",
            "Upload file pytorch_model.bin: 100% 427M/427M [06:31<00:00, 1.18MB/s]remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_data2vec-vision-base-mae\n",
            "   3e12ec3..a725e5b  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_data2vec-vision-base-mae\n",
            "   3e12ec3..a725e5b  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 427M/427M [06:33<00:00, 1.14MB/s]\n",
            "\n",
            "Upload file runs/May18_16-10-11_b937b5e1c6ef/events.out.tfevents.1652923691.b937b5e1c6ef.1308.2: 100% 316/316 [06:33<?, ?B/s]\u001b[A\n",
            "Upload file runs/May18_16-10-11_b937b5e1c6ef/events.out.tfevents.1652923691.b937b5e1c6ef.1308.2: 100% 316/316 [06:33<?, ?B/s]\n",
            "[INFO|modelcard.py:460] 2022-05-19 01:35:00,164 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'dataset': {'name': 'gary109/orchid219', 'type': 'orchid219', 'args': 'orchid219'}}\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_data2vec-vision-base-mae\n",
            "   a725e5b..fdac42f  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_pretrain_data2vec-vision-base-mae\n",
            "   a725e5b..fdac42f  main -> main\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▅▄▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▃▅▆▇▄▃▁▄▆▄▄▃▂▂▂▃▅▃▄▅█▃▂▄▂▁▂▃▇▃▄▂▁▃▂▃▂▃▄▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▆▄▃▂▄▅█▅▃▅▅▆▇▇▇▆▄▆▅▃▁▆▇▅▇▇▇▆▂▆▅▇█▆▇▆▇▆▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▆▄▃▂▄▅█▅▃▅▅▆▇▇▇▆▄▆▅▃▁▆▇▅▇▇▇▆▂▆▅▇█▆▇▆▇▆▅▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▂▅███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▆▅▄▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.42749\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 4.053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 54.034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.987\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 1000.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 31000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.4371\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.9921208425316352e+20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.48026\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 32863.333\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 59.976\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.943\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33morchid219_pretrain_data2vec-vision-base-mae\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/gary109/huggingface/runs/3a7tolwr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220518_161335-3a7tolwr/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! OMP_NUM_THREADS=1 accelerate launch run_mae.py \\\n",
        "    --dataset_name=\"gary109/orchid219\" \\\n",
        "    --model_name_or_path=\"facebook/data2vec-vision-base\" \\\n",
        "    --output_dir=\"orchid219_pretrain_data2vec-vision-base-mae\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --label_names pixel_values \\\n",
        "    --mask_ratio 0.75 \\\n",
        "    --norm_pix_loss \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --base_learning_rate 1.5e-4 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --num_train_epochs 1000 \\\n",
        "    --save_steps=\"1000\" \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --push_to_hub \\\n",
        "    --hub_model_id=\"orchid219_pretrain_data2vec-vision-base-mae\" \\\n",
        "\t--hub_token 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 \\\n",
        "    --seed 1337"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYfPoWy7m_lB"
      },
      "source": [
        "## SimMIM (by Microsoft Research)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdJ6bHkq-X-w"
      },
      "source": [
        "### [openai/clip-vit-base-patch16]  ===> orchid219_pretrain_clip_vit-base-mim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td46YPsSvWYM"
      },
      "source": [
        "### [行不通] [gary109/orchid219_data2vec-vision-base] ===> orchid219_pretrain_data2vec-vision-base-mim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pij54U0Fv4M9",
        "outputId": "f7979701-27cb-4766-b739-2a924676f362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "05/18/2022 15:23:49 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/18/2022 15:23:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=orchid219_pretrain_data2vec-vision-base-mim,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=['pixel_values'],\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=orchid219_pretrain_data2vec-vision-base-mim/runs/May18_15-23-49_d854923d2a56,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1000.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=orchid219_pretrain_data2vec-vision-base-mim,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=64,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=orchid219_pretrain_data2vec-vision-base-mim,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=3,\n",
            "seed=1337,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.05,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Downloading: 100% 3.08k/3.08k [00:00<00:00, 3.04MB/s]\n",
            "05/18/2022 15:23:54 - WARNING - datasets.builder - Using custom data configuration gary109--orchid219-53e55d447bfb4b23\n",
            "Downloading and preparing dataset orchid219/orchid219 (download: 86.69 MiB, generated: 85.33 MiB, post-processed: Unknown size, total: 172.02 MiB) to /root/.cache/huggingface/datasets/gary109___parquet/gary109--orchid219-53e55d447bfb4b23/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n",
            "Downloading data files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/82.1M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   9% 7.25M/82.1M [00:00<00:01, 72.5MB/s]\u001b[A\n",
            "Downloading data:  18% 15.0M/82.1M [00:00<00:00, 75.4MB/s]\u001b[A\n",
            "Downloading data:  28% 22.6M/82.1M [00:00<00:00, 75.8MB/s]\u001b[A\n",
            "Downloading data:  37% 30.2M/82.1M [00:00<00:00, 75.7MB/s]\u001b[A\n",
            "Downloading data:  46% 37.9M/82.1M [00:00<00:00, 76.4MB/s]\u001b[A\n",
            "Downloading data:  56% 45.8M/82.1M [00:00<00:00, 77.3MB/s]\u001b[A\n",
            "Downloading data:  65% 53.6M/82.1M [00:00<00:00, 77.2MB/s]\u001b[A\n",
            "Downloading data:  75% 61.4M/82.1M [00:00<00:00, 77.4MB/s]\u001b[A\n",
            "Downloading data:  84% 69.1M/82.1M [00:00<00:00, 77.4MB/s]\u001b[A\n",
            "Downloading data: 100% 82.1M/82.1M [00:01<00:00, 77.2MB/s]\n",
            "Downloading data files:  50% 1/2 [00:02<00:02,  2.93s/it]\n",
            "Downloading data:   0% 0.00/8.79M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 8.79M/8.79M [00:00<00:00, 68.4MB/s]\n",
            "Downloading data files: 100% 2/2 [00:04<00:00,  2.49s/it]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1302.17it/s]\n",
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/gary109___parquet/gary109--orchid219-53e55d447bfb4b23/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 198.56it/s]\n",
            "[INFO|hub.py:583] 2022-05-18 15:24:01,244 >> https://huggingface.co/gary109/orchid219_data2vec-vision-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4gui8uma\n",
            "Downloading: 100% 1.08k/1.08k [00:00<00:00, 976kB/s]\n",
            "[INFO|hub.py:587] 2022-05-18 15:24:02,145 >> storing https://huggingface.co/gary109/orchid219_data2vec-vision-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/5b366963c824a62e253a59a8c16513e36146a6624fd6a6c934f040e93706e292.641509c9720536b3d59190682d6d2bd86f8489ee609d9987695950d2adedff03\n",
            "[INFO|hub.py:595] 2022-05-18 15:24:02,145 >> creating metadata file for /root/.cache/huggingface/transformers/5b366963c824a62e253a59a8c16513e36146a6624fd6a6c934f040e93706e292.641509c9720536b3d59190682d6d2bd86f8489ee609d9987695950d2adedff03\n",
            "[INFO|configuration_utils.py:659] 2022-05-18 15:24:02,145 >> loading configuration file https://huggingface.co/gary109/orchid219_data2vec-vision-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5b366963c824a62e253a59a8c16513e36146a6624fd6a6c934f040e93706e292.641509c9720536b3d59190682d6d2bd86f8489ee609d9987695950d2adedff03\n",
            "[INFO|configuration_utils.py:708] 2022-05-18 15:24:02,146 >> Model config Data2VecVisionConfig {\n",
            "  \"_name_or_path\": \"gary109/orchid219_data2vec-vision-base\",\n",
            "  \"architectures\": [\n",
            "    \"Data2VecVisionForImageClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"auxiliary_channels\": 256,\n",
            "  \"auxiliary_concat_input\": false,\n",
            "  \"auxiliary_loss_weight\": 0.4,\n",
            "  \"auxiliary_num_convs\": 1,\n",
            "  \"drop_path_rate\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layer_scale_init_value\": 0.1,\n",
            "  \"model_type\": \"data2vec-vision\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"out_indices\": [\n",
            "    3,\n",
            "    5,\n",
            "    7,\n",
            "    11\n",
            "  ],\n",
            "  \"patch_size\": 16,\n",
            "  \"pool_scales\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    6\n",
            "  ],\n",
            "  \"semantic_loss_ignore_index\": 255,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"use_absolute_position_embeddings\": false,\n",
            "  \"use_auxiliary_head\": true,\n",
            "  \"use_mask_token\": false,\n",
            "  \"use_mean_pooling\": false,\n",
            "  \"use_relative_position_bias\": false,\n",
            "  \"use_shared_relative_position_bias\": true,\n",
            "  \"vocab_size\": 8192\n",
            "}\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"run_mim.py\", line 449, in <module>\n",
            "    main()\n",
            "  File \"run_mim.py\", line 308, in main\n",
            "    model_args.encoder_stride if model_args.encoder_stride is not None else config.encoder_stride\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 253, in __getattribute__\n",
            "    return super().__getattribute__(key)\n",
            "AttributeError: 'Data2VecVisionConfig' object has no attribute 'encoder_stride'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/accelerate/commands/accelerate_cli.py\", line 43, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/accelerate/commands/launch.py\", line 520, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/accelerate/commands/launch.py\", line 210, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', 'run_mim.py', '--dataset_name=gary109/orchid219', '--model_name_or_path=gary109/orchid219_data2vec-vision-base', '--model_type', 'vit', '--output_dir=orchid219_pretrain_data2vec-vision-base-mim', '--overwrite_output_dir', '--remove_unused_columns', 'False', '--label_names=pixel_values', '--do_train', '--do_eval', '--learning_rate', '2e-5', '--weight_decay', '0.05', '--num_train_epochs', '1000', '--per_device_train_batch_size', '64', '--per_device_eval_batch_size', '64', '--logging_strategy', 'steps', '--logging_steps', '10', '--evaluation_strategy', 'epoch', '--save_strategy', 'epoch', '--load_best_model_at_end', 'True', '--save_total_limit', '3', '--push_to_hub', '--hub_model_id=orchid219_pretrain_data2vec-vision-base-mim', '--hub_token=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', '--seed=1337', '--use_auth_token=True']' returned non-zero exit status 1.\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch run_mim.py \\\n",
        "    --dataset_name=\"gary109/orchid219\" \\\n",
        "    --model_name_or_path=\"gary109/orchid219_data2vec-vision-base\" \\\n",
        "    --model_type vit \\\n",
        "    --output_dir=\"orchid219_pretrain_data2vec-vision-base-mim\" \\\n",
        "    --overwrite_output_dir \\\n",
        "    --remove_unused_columns False \\\n",
        "    --label_names=\"pixel_values\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --num_train_epochs 1000 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --push_to_hub \\\n",
        "    --hub_model_id=\"orchid219_pretrain_data2vec-vision-base-mim\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --seed=\"1337\" \\\n",
        "    --use_auth_token=\"True\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E2bj-wHBpbf"
      },
      "source": [
        "### [google/vit-base-patch16-224-in21k] ===> orchid219_pretrain_vit-base-mim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4zCymMkncEF"
      },
      "outputs": [],
      "source": [
        "!accelerate launch run_mim.py \\\n",
        "    --dataset_name=\"gary109/orchid219\" \\\n",
        "    --model_name_or_path=\"google/vit-base-patch16-224-in21k\" \\\n",
        "    --model_type vit \\\n",
        "    --output_dir=\"orchid219_pretrain_vit-base-mim\" \\\n",
        "    --overwrite_output_dir \\\n",
        "    --remove_unused_columns False \\\n",
        "    --label_names=\"pixel_values\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --num_train_epochs 1000 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --push_to_hub \\\n",
        "    --hub_model_id=\"orchid219_pretrain_vit-base-mim\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --seed=\"1337\" \\\n",
        "    --use_auth_token=\"True\"\n",
        "\n",
        "# --label_names bool_masked_pos \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nS0lQQyCUC8"
      },
      "source": [
        "### [google/vit-large-patch16-224-in21k] ===> orchid219_pretrain_vit-large-mim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkwgQ3ZJd1Ig"
      },
      "source": [
        "#### 產出 feature_extractor FOR Orchid129\n",
        "- google/vit-large-patch16-224-in21k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "gsLjU4Y1Xzuc",
        "outputId": "28412b12-6771-4c82-bad7-5ec396028b0e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dd2fd0a7b9b4a93998a3a7ad9fbf24c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/vit-large-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-large-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ViTFeatureExtractor {\n",
              "  \"do_normalize\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
              "  \"image_mean\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"image_std\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"resample\": 2,\n",
              "  \"size\": 224\n",
              "}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "from transformers import AutoFeatureExtractor,AutoModelForImageClassification\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "        pretrained_model_name_or_path=\"google/vit-large-patch16-224-in21k\",\n",
        "        feature_extractor_name=\"google/vit-large-patch16-224-in21k\",\n",
        "        use_auth_token=True,\n",
        "    )\n",
        "model = AutoModelForImageClassification.from_pretrained('google/vit-large-patch16-224-in21k',use_auth_token=True,)\n",
        "feature_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pvzOiFwZP2P"
      },
      "outputs": [],
      "source": [
        "feature_extractor.image_mean = [0.48058045,0.42326896,0.36735169]\n",
        "feature_extractor.image_std = [0.21403854, 0.21571221, 0.21655118] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlHYAchlYovt",
        "outputId": "88b0391b-26db-44e6-f8cc-cb91c59d67d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ViTFeatureExtractor {\n",
              "  \"do_normalize\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
              "  \"image_mean\": [\n",
              "    0.48058045,\n",
              "    0.42326896,\n",
              "    0.36735169\n",
              "  ],\n",
              "  \"image_std\": [\n",
              "    0.21403854,\n",
              "    0.21571221,\n",
              "    0.21655118\n",
              "  ],\n",
              "  \"resample\": 2,\n",
              "  \"size\": 224\n",
              "}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "hE5MteYohtS0",
        "outputId": "420b8270-dd1b-4a94-b86b-b8df7cd5b10f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc69f8e68a0740259e06f0afa6498f9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file pytorch_model.bin:   0%|          | 3.33k/1.13G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_vit-large-patch16-224-in21k\n",
            "   b7b7745..1b26175  main -> main\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://huggingface.co/gary109/orchid219_vit-large-patch16-224-in21k/commit/1b2617525240c2239655b9b5ba34340447f763e2'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.push_to_hub('gary109/orchid219_vit-large-patch16-224-in21k',use_auth_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgZHG132ZkRm"
      },
      "outputs": [],
      "source": [
        "feature_extractor.push_to_hub('gary109/orchid219_vit-large-patch16-224-in21k',use_auth_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjZNnwOrCUYw"
      },
      "outputs": [],
      "source": [
        "!accelerate launch run_mim.py \\\n",
        "    --dataset_name=\"gary109/orchid219\" \\\n",
        "    --model_name_or_path=\"google/orchid219_vit-large-patch16-224-in21k\" \\\n",
        "    --model_type vit \\\n",
        "    --output_dir=\"orchid219_pretrain_vit-large-mim\" \\\n",
        "    --overwrite_output_dir \\\n",
        "    --remove_unused_columns False \\\n",
        "    --label_names=\"pixel_values\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --num_train_epochs 1000 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --push_to_hub \\\n",
        "    --hub_model_id=\"orchid219_pretrain_vit-large-mim\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --seed=\"1337\" \\\n",
        "    --use_auth_token=\"True\"\n",
        "\n",
        "    # --gradient_checkpointing \\\n",
        "    # --gradient_accumulation_steps 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB5MPNKOeVoS"
      },
      "source": [
        "### [facebook/data2vec-vision-base] ===> orchid219_pretrain_vit-base-mim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnwXk6rKeVoS"
      },
      "outputs": [],
      "source": [
        "!accelerate launch run_mim.py \\\n",
        "    --dataset_name=\"gary109/orchid219\" \\\n",
        "    --model_name_or_path=\"facebook/data2vec-vision-base\" \\\n",
        "    --model_type vit \\\n",
        "    --output_dir=\"orchid219_pretrain_data2vec-vision-base-mim\" \\\n",
        "    --overwrite_output_dir \\\n",
        "    --remove_unused_columns False \\\n",
        "    --label_names=\"pixel_values\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --num_train_epochs 800 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --push_to_hub \\\n",
        "    --hub_model_id=\"orchid219_pretrain_data2vec-vision-base-mim\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --seed=\"1337\" \\\n",
        "    --use_auth_token=\"True\"\n",
        "\n",
        "# --label_names bool_masked_pos \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eJ01wn1eRss"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYRmaQJjkBmT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "kYFkur3sEb6f",
        "_aVUBWempD48",
        "n3eXyafxpNkp",
        "5lYnSRfsvhd_",
        "zXutErFKr48K",
        "PntiQvYf92jN",
        "td46YPsSvWYM",
        "4E2bj-wHBpbf",
        "2nS0lQQyCUC8",
        "jB5MPNKOeVoS"
      ],
      "machine_shape": "hm",
      "name": "Pre-Train Orchid219 Image classification with ViT",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}