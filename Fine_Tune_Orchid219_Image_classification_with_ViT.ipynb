{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune Orchid219 Image classification with ViT",
      "provenance": [],
      "collapsed_sections": [
        "5lYnSRfsvhd_",
        "2kKsDcHu0hlX",
        "NhO8MiJ41HLI",
        "RcO8DSXZz7W5",
        "NNvuAbjI4PWH",
        "WJsYvA7u4dLr"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FOR TPU needs\n",
        "---"
      ],
      "metadata": {
        "id": "5lYnSRfsvhd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip uninstall -y torch\n",
        "!pip install torch==1.8.2+cpu torchvision==0.9.2+cpu -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "94vK4zQPvfGQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ÊòØÂê¶Ë¶ÅÊéõËºâ Google Drive\n",
        "---"
      ],
      "metadata": {
        "id": "2kKsDcHu0hlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4aEAQyCz1950"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Á¢∫Ë™ç GPU È°ûÂûã\n",
        "---"
      ],
      "metadata": {
        "id": "2ozr9xyv0ZMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  raise Exception(\"GPU not availalbe. CPU training will be too slow.\")\n",
        "print(\"device name\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "aDtXj7uU119F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ÂÆâË£ù transformers,datastes,... Áõ∏‰æùÂ•ó‰ª∂\n",
        "---"
      ],
      "metadata": {
        "id": "1O9n-3Ak0rik"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9T850192NHC"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install soundfile\n",
        "!pip install jiwer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngh37Hdl7cWi",
        "outputId": "124f3d13-85e3-43bf-a8d5-7ff46cbd0b8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 94943, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 94943 (delta 8), reused 12 (delta 3), pack-reused 94924\u001b[K\n",
            "Receiving objects: 100% (94943/94943), 85.39 MiB | 17.50 MiB/s, done.\n",
            "Resolving deltas: 100% (69667/69667), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! apt install git-lfs\n",
        "! git config --global user.email \"gary109@gmail.com\"\n",
        "! git config --global user.name \"GARY\"\n",
        "! git config --global credential.helper store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuGCGjXT7fIK",
        "outputId": "cedb7aec-6418-424e-be41-bbb892887159"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ÁôªÂÖ• huggingface \n",
        "---"
      ],
      "metadata": {
        "id": "A1JcSRcJ0_uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f400e75-5303-4319-d69f-cc4395323b3f",
        "id": "yC1Cp6_e2U77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens.\n",
            "        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n",
            "        \n",
            "Token: \n",
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ÂÆâË£ùÂä†ÈÄüÂô®\n",
        "---"
      ],
      "metadata": {
        "id": "F3AZ8EIyzzir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install accelerate deepspeed"
      ],
      "metadata": {
        "id": "WsDxQ0683W6T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJJEgLEi7OBF",
        "outputId": "5ece9c52-af25-4772-9d4d-fec74c8845a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.8\n",
            "[2022-05-01 04:39:27,013] [WARNING] [partition_parameters.py:54:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.\n",
            "In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0\n",
            "Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 3\n",
            "What is the name of the function in your script that should be launched in all parallel scripts? [main]: \n",
            "How many processes in total will you use? [1]: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1EszQTy7JTh",
        "outputId": "63cec423-ba92-4def-d68d-750295f69446"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.8\n",
            "[2022-05-01 04:39:49,327] [WARNING] [partition_parameters.py:54:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.\n",
            "\n",
            "Running:  accelerate-launch --config_file=None /usr/local/lib/python3.7/dist-packages/accelerate/test_utils/test_script.py\n",
            "stdout: [2022-05-01 04:39:51,334] [WARNING] [partition_parameters.py:54:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.\n",
            "stdout: **Initialization**\n",
            "stdout: Testing, testing. 1, 2, 3.\n",
            "stdout: Distributed environment: TPU\n",
            "stdout: Num processes: 1\n",
            "stdout: Process index: 0\n",
            "stdout: Local process index: 0\n",
            "stdout: Device: xla:1\n",
            "stdout: Mixed precision type: no\n",
            "stdout: \n",
            "stdout: \n",
            "stdout: **Test random number generator synchronization**\n",
            "stdout: All rng are properly synched.\n",
            "stdout: \n",
            "stdout: **DataLoader integration test**\n",
            "stdout: 0 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) <class 'accelerate.data_loader.DataLoaderShard'>\n",
            "stdout: Non-shuffled dataloader passing.\n",
            "stdout: Shuffled dataloader passing.\n",
            "stdout: Non-shuffled central dataloader passing.\n",
            "stdout: Shuffled central dataloader passing.\n",
            "stdout: \n",
            "stdout: **Training integration test**\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
            "stdout: FP16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: Legacy FP16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "stdout: BF16 training check.\n",
            "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
            "Test is a success! You are ready for your distributed training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‰∏ãËºâ orchid219_classification Á®ãÂºèÁ¢º\n",
        "--- "
      ],
      "metadata": {
        "id": "NhO8MiJ41HLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://gary109:Gygy844109109@gitlab.com/gary109/orchid219_classification.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfPGSNnqqdLE",
        "outputId": "4346bcf5-effc-4882-f366-f3109d2f7e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'orchid219_classification'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 16 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ËºâÂÖ• orchid219 Ë®ìÁ∑¥Ë≥áÊñôÈõÜ\n",
        "---"
      ],
      "metadata": {
        "id": "RcO8DSXZz7W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"./orchid219_classification/datasets/orchid219.py\", use_auth_token=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "21e29e71a14849a590345644bbe77c12",
            "92ca2a9769f64f7d9136941126af9de5",
            "e9836e9361b24fa499d9dbfdf167d62c",
            "e735b6e2d50b4ff0bc9e068ec9f8b281",
            "791a6d92ba8a4184ae7de36623cdce0b",
            "80b448779bc84556897d5fe1f3085a58",
            "9b7c67bb6af14a97bbabab0d3d85b4ba",
            "84ed12e3496347b4a0474eb5272ca6b4",
            "6772501c9cc342e099402c6a54abd2b2",
            "212f4978f88245a58cf5295cf98ebce9",
            "77515809ffcb4a3e93c3bbee6ff00293",
            "65b2bddcc6f54aaaac41ec3f571a1256",
            "78854db8781d4c5d803be185dde1add6",
            "59b10a26b44a41018bca1501e843c67b",
            "b2c6dd13978f433cbcbf80d181b1ab71",
            "09af7a2b26e74cefb6710271618e4afe",
            "52fec1ebe01148dbb021e10f09604b30",
            "a9721625069e47d8ab353789dc0ad271",
            "181175ee577d43adb97ff38ae124b45b",
            "9da2f2471d54430585f79edd12ff8b9d",
            "411c2cb2d273499ea12aa4359f94a0ff",
            "34d6eb1c9bd347969b70d0b44c903005",
            "aa52d499b79342d6b55b1d347f93b377",
            "a68c652fa64e47f496b51d302f993a65",
            "b1f6d853fd1f4599b76e612b20564bd1",
            "85f547c8b558450085ab8d4cf3f7fea6",
            "583cecceec114c61bb852a78fc85ba3f",
            "3acb0a6652904fe18bef56e26b72c1f3",
            "2be77b553bd045bf9889f6a9ca470c0e",
            "dd35a450779d402e9156a90087d40d60",
            "108f9f7e212a4aac8d1956228f492333",
            "c724687aaef04034a48df3e389194a1d",
            "768e988bb8934475850cdf12e31314db",
            "cd522ecafef04c05abdcd3eaf58b99ee",
            "23b4a6b3cddf43eaa572b8119d7f7054",
            "6e3b05e8aa7b436bad215df0f6a61314",
            "100422bb34f446ecb657fffd52a9f2b7",
            "8de92688689145b39b05e2a125cae85f",
            "45a3002bfeb047a8b5f6a0f7670bff5a",
            "5b1a3e5b4ec44a648a52d7642c0a8cee",
            "47255aecde6545b5a1c7dc73f4dd9c8c",
            "8f1d942292864a5b9c9fd153b06c23eb",
            "73ecb91dc75e4fb1a0204d58279b1d56",
            "b11bcb507c384d7f8a43549c5481221c"
          ]
        },
        "outputId": "387bde46-099d-4f9a-cad0-2283e758a115",
        "id": "U_idZeBF2zie"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset crop14/crop14-small to /root/.cache/huggingface/datasets/crop14/crop14-small/0.0.3/84d7648f2ea0b147531986b7c0c46914c93dc9da31daac0a5ec3c57590c4e99a...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/2.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21e29e71a14849a590345644bbe77c12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65b2bddcc6f54aaaac41ec3f571a1256"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa52d499b79342d6b55b1d347f93b377"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset crop14 downloaded and prepared to /root/.cache/huggingface/datasets/crop14/crop14-small/0.0.3/84d7648f2ea0b147531986b7c0c46914c93dc9da31daac0a5ec3c57590c4e99a. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd522ecafef04c05abdcd3eaf58b99ee"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils.dummy_vision_objects import ImageGPTFeatureExtractor\n",
        "import random\n",
        "from PIL import ImageDraw, ImageFont, Image\n",
        "\n",
        "def show_examples(ds, seed: int = 1234, examples_per_class: int = 3, size=(100, 100)):\n",
        "\n",
        "    w, h = size\n",
        "    labels = ds['train'].features['category'].names\n",
        "    labels = labels[:9]\n",
        "    grid = Image.new('RGB', size=(examples_per_class * w, len(labels) * h))\n",
        "    draw = ImageDraw.Draw(grid)\n",
        "    font = ImageFont.truetype(\"./fonts/LiberationMono-Bold.ttf\", 24)\n",
        "    for label_id, label in enumerate(labels):\n",
        "\n",
        "        # Filter the dataset by a single label, shuffle it, and grab a few samples\n",
        "        ds_slice = ds['train'].filter(lambda ex: ex['category'] == label_id).shuffle(seed).select(range(examples_per_class))\n",
        "\n",
        "        # Plot this label's examples along a row\n",
        "        for i, example in enumerate(ds_slice):\n",
        "            image = example['image']\n",
        "            idx = examples_per_class * label_id + i\n",
        "            box = (idx % examples_per_class * w, idx // examples_per_class * h)\n",
        "            grid.paste(image.resize(size), box=box)\n",
        "            draw.text(box, str(label), (255, 255, 255), font=font)\n",
        "\n",
        "    return grid\n",
        "\n",
        "show_examples(dataset, seed=random.randint(0, 1337), examples_per_class=3)\n",
        "# dataset['train'][0]['image']"
      ],
      "metadata": {
        "id": "7iNPEbNz2zie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"gary109/orchid219\")"
      ],
      "metadata": {
        "id": "1gymd4g12zif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ÈñãÂßãË®ìÁ∑¥\n",
        "---"
      ],
      "metadata": {
        "id": "GBQ4LLFm2CJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/transformers/examples/pytorch/image-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E5Kms3u0Gx6",
        "outputId": "983fc0ab-d3de-46ee-9988-c56c60718fc8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers/examples/pytorch/image-classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-base-patch16-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "MzvQ9zfD34bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"google/vit-base-patch16-224-in21k\" \\\n",
        "    --output_dir=\"./orchid219_vit-base-patch16-224-in21k/\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id orchid219_vit-base-patch16-224-in21k \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \\\n",
        "    --cache_dir=\"./cache_orchid219_vit-base-patch16-224-in21k\"\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "tNMZWv7q0G02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-base-patch32-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "p9t54nRI4B5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"orchid219\" \\\n",
        "    --model_name_or_path \"google/vit-base-patch32-224-in21k\" \\\n",
        "    --output_dir ./orchid219_vit-base-patch32-224-in21k/ \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id orchid219_vit-base-patch32-224-in21k \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --per_device_train_batch_size 80 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "XTWxM4p30G3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-large-patch16-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "NNvuAbjI4PWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"orchid219\" \\\n",
        "    --model_name_or_path \"google/vit-large-patch16-224-in21k\" \\\n",
        "    --output_dir \"./orchid219_vit-large-patch16-224-in21k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id \"orchid219_vit-large-patch16-224-in21k\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "Xo5w2JgH0G6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-large-patch32-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "WJsYvA7u4dLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"google/vit-large-patch32-224-in21k\" \\\n",
        "    --output_dir \"./orchid219_vit-large-patch32-224-in21k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id \"orchid219_vit-large-patch32-224-in21k\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "c13vuvEg4dlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-huge-patch14-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "qaNA2JF34UPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"google/vit-huge-patch14-224-in21k\" \\\n",
        "    --output_dir \"./orchid219_vit-huge-patch14-224-in21k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id \"orchid219_vit-huge-patch14-224-in21k\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 5 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --gradient_checkpointing"
      ],
      "metadata": {
        "id": "Gd5cMkQ9kBjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Wnmnj8KB9Whn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zsJxSbz8YnX",
        "outputId": "435e82b8-7155-4025-c7d8-05ef0c9ad88c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification_ViT-MAE.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae\" \\\n",
        "    --output_dir=\"./orchid219_ft_vit-base-mae/\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_ft_vit-base-mae\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 100 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \n",
        "    # --cache_dir=\"./cache_test/\"\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing\n",
        "\n",
        "# --model_name_or_path \"gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae\" \\\n",
        "    #orchid219_pretrain_vit-base-patch16-224-in21k-mae\n",
        "    # --model_name_or_path \"gary109/orchid219_vit-base-patch16-224-in21k\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q26wxpY9aic",
        "outputId": "cb89c0ec-f612-45c3-9f92-d455d3435ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.8\n",
            "[2022-05-01 04:47:54,135] [WARNING] [partition_parameters.py:54:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:978: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case gary109/orchid219_ft_vit-base-mae).\n",
            "  FutureWarning,\n",
            "WARNING:run_image_classification_ViT-MAE:Process rank: -1, device: xla:1, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "INFO:run_image_classification_ViT-MAE:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=gary109/orchid219_ft_vit-base-mae,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./orchid219_ft_vit-base-mae/runs/May01_04-47-58_db3a3922d648,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=100.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=./orchid219_ft_vit-base-mae/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=orchid219_ft_vit-base-mae,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./orchid219_ft_vit-base-mae/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=3,\n",
            "seed=1337,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration gary109--orchid219-53e55d447bfb4b23\n",
            "WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/gary109--orchid219-53e55d447bfb4b23/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
            "100% 2/2 [00:00<00:00, 166.79it/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/gary109--orchid219-53e55d447bfb4b23/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-a589d500576625c6.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/gary109--orchid219-53e55d447bfb4b23/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-7390eeefe5a21145.arrow\n",
            "[INFO|configuration_utils.py:659] 2022-05-01 04:48:08,545 >> loading configuration file https://huggingface.co/gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1295778178d320b4c5095e6186d2910f252f7247fa02359242a660f7acad0d86.5202c97a6fd095d1e793d8e4db4378b56c107e10361ebd7f8f777b8e509308cc\n",
            "[INFO|configuration_utils.py:704] 2022-05-01 04:48:08,547 >> Model config ViTMAEConfig {\n",
            "  \"_name_or_path\": \"gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae\",\n",
            "  \"architectures\": [\n",
            "    \"ViTMAEForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"decoder_hidden_size\": 512,\n",
            "  \"decoder_intermediate_size\": 2048,\n",
            "  \"decoder_num_attention_heads\": 16,\n",
            "  \"decoder_num_hidden_layers\": 8,\n",
            "  \"finetuning_task\": \"image-classification\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"10\": 10,\n",
            "    \"100\": 100,\n",
            "    \"101\": 101,\n",
            "    \"102\": 102,\n",
            "    \"103\": 103,\n",
            "    \"104\": 104,\n",
            "    \"105\": 105,\n",
            "    \"106\": 106,\n",
            "    \"107\": 107,\n",
            "    \"108\": 108,\n",
            "    \"109\": 109,\n",
            "    \"11\": 11,\n",
            "    \"110\": 110,\n",
            "    \"111\": 111,\n",
            "    \"112\": 112,\n",
            "    \"113\": 113,\n",
            "    \"114\": 114,\n",
            "    \"115\": 115,\n",
            "    \"116\": 116,\n",
            "    \"117\": 117,\n",
            "    \"118\": 118,\n",
            "    \"119\": 119,\n",
            "    \"12\": 12,\n",
            "    \"120\": 120,\n",
            "    \"121\": 121,\n",
            "    \"122\": 122,\n",
            "    \"123\": 123,\n",
            "    \"124\": 124,\n",
            "    \"125\": 125,\n",
            "    \"126\": 126,\n",
            "    \"127\": 127,\n",
            "    \"128\": 128,\n",
            "    \"129\": 129,\n",
            "    \"13\": 13,\n",
            "    \"130\": 130,\n",
            "    \"131\": 131,\n",
            "    \"132\": 132,\n",
            "    \"133\": 133,\n",
            "    \"134\": 134,\n",
            "    \"135\": 135,\n",
            "    \"136\": 136,\n",
            "    \"137\": 137,\n",
            "    \"138\": 138,\n",
            "    \"139\": 139,\n",
            "    \"14\": 14,\n",
            "    \"140\": 140,\n",
            "    \"141\": 141,\n",
            "    \"142\": 142,\n",
            "    \"143\": 143,\n",
            "    \"144\": 144,\n",
            "    \"145\": 145,\n",
            "    \"146\": 146,\n",
            "    \"147\": 147,\n",
            "    \"148\": 148,\n",
            "    \"149\": 149,\n",
            "    \"15\": 15,\n",
            "    \"150\": 150,\n",
            "    \"151\": 151,\n",
            "    \"152\": 152,\n",
            "    \"153\": 153,\n",
            "    \"154\": 154,\n",
            "    \"155\": 155,\n",
            "    \"156\": 156,\n",
            "    \"157\": 157,\n",
            "    \"158\": 158,\n",
            "    \"159\": 159,\n",
            "    \"16\": 16,\n",
            "    \"160\": 160,\n",
            "    \"161\": 161,\n",
            "    \"162\": 162,\n",
            "    \"163\": 163,\n",
            "    \"164\": 164,\n",
            "    \"165\": 165,\n",
            "    \"166\": 166,\n",
            "    \"167\": 167,\n",
            "    \"168\": 168,\n",
            "    \"169\": 169,\n",
            "    \"17\": 17,\n",
            "    \"170\": 170,\n",
            "    \"171\": 171,\n",
            "    \"172\": 172,\n",
            "    \"173\": 173,\n",
            "    \"174\": 174,\n",
            "    \"175\": 175,\n",
            "    \"176\": 176,\n",
            "    \"177\": 177,\n",
            "    \"178\": 178,\n",
            "    \"179\": 179,\n",
            "    \"18\": 18,\n",
            "    \"180\": 180,\n",
            "    \"181\": 181,\n",
            "    \"182\": 182,\n",
            "    \"183\": 183,\n",
            "    \"184\": 184,\n",
            "    \"185\": 185,\n",
            "    \"186\": 186,\n",
            "    \"187\": 187,\n",
            "    \"188\": 188,\n",
            "    \"189\": 189,\n",
            "    \"19\": 19,\n",
            "    \"190\": 190,\n",
            "    \"191\": 191,\n",
            "    \"192\": 192,\n",
            "    \"193\": 193,\n",
            "    \"194\": 194,\n",
            "    \"195\": 195,\n",
            "    \"196\": 196,\n",
            "    \"197\": 197,\n",
            "    \"198\": 198,\n",
            "    \"199\": 199,\n",
            "    \"2\": 2,\n",
            "    \"20\": 20,\n",
            "    \"200\": 200,\n",
            "    \"201\": 201,\n",
            "    \"202\": 202,\n",
            "    \"203\": 203,\n",
            "    \"204\": 204,\n",
            "    \"205\": 205,\n",
            "    \"206\": 206,\n",
            "    \"207\": 207,\n",
            "    \"208\": 208,\n",
            "    \"209\": 209,\n",
            "    \"21\": 21,\n",
            "    \"210\": 210,\n",
            "    \"211\": 211,\n",
            "    \"212\": 212,\n",
            "    \"213\": 213,\n",
            "    \"214\": 214,\n",
            "    \"215\": 215,\n",
            "    \"216\": 216,\n",
            "    \"217\": 217,\n",
            "    \"218\": 218,\n",
            "    \"22\": 22,\n",
            "    \"23\": 23,\n",
            "    \"24\": 24,\n",
            "    \"25\": 25,\n",
            "    \"26\": 26,\n",
            "    \"27\": 27,\n",
            "    \"28\": 28,\n",
            "    \"29\": 29,\n",
            "    \"3\": 3,\n",
            "    \"30\": 30,\n",
            "    \"31\": 31,\n",
            "    \"32\": 32,\n",
            "    \"33\": 33,\n",
            "    \"34\": 34,\n",
            "    \"35\": 35,\n",
            "    \"36\": 36,\n",
            "    \"37\": 37,\n",
            "    \"38\": 38,\n",
            "    \"39\": 39,\n",
            "    \"4\": 4,\n",
            "    \"40\": 40,\n",
            "    \"41\": 41,\n",
            "    \"42\": 42,\n",
            "    \"43\": 43,\n",
            "    \"44\": 44,\n",
            "    \"45\": 45,\n",
            "    \"46\": 46,\n",
            "    \"47\": 47,\n",
            "    \"48\": 48,\n",
            "    \"49\": 49,\n",
            "    \"5\": 5,\n",
            "    \"50\": 50,\n",
            "    \"51\": 51,\n",
            "    \"52\": 52,\n",
            "    \"53\": 53,\n",
            "    \"54\": 54,\n",
            "    \"55\": 55,\n",
            "    \"56\": 56,\n",
            "    \"57\": 57,\n",
            "    \"58\": 58,\n",
            "    \"59\": 59,\n",
            "    \"6\": 6,\n",
            "    \"60\": 60,\n",
            "    \"61\": 61,\n",
            "    \"62\": 62,\n",
            "    \"63\": 63,\n",
            "    \"64\": 64,\n",
            "    \"65\": 65,\n",
            "    \"66\": 66,\n",
            "    \"67\": 67,\n",
            "    \"68\": 68,\n",
            "    \"69\": 69,\n",
            "    \"7\": 7,\n",
            "    \"70\": 70,\n",
            "    \"71\": 71,\n",
            "    \"72\": 72,\n",
            "    \"73\": 73,\n",
            "    \"74\": 74,\n",
            "    \"75\": 75,\n",
            "    \"76\": 76,\n",
            "    \"77\": 77,\n",
            "    \"78\": 78,\n",
            "    \"79\": 79,\n",
            "    \"8\": 8,\n",
            "    \"80\": 80,\n",
            "    \"81\": 81,\n",
            "    \"82\": 82,\n",
            "    \"83\": 83,\n",
            "    \"84\": 84,\n",
            "    \"85\": 85,\n",
            "    \"86\": 86,\n",
            "    \"87\": 87,\n",
            "    \"88\": 88,\n",
            "    \"89\": 89,\n",
            "    \"9\": 9,\n",
            "    \"90\": 90,\n",
            "    \"91\": 91,\n",
            "    \"92\": 92,\n",
            "    \"93\": 93,\n",
            "    \"94\": 94,\n",
            "    \"95\": 95,\n",
            "    \"96\": 96,\n",
            "    \"97\": 97,\n",
            "    \"98\": 98,\n",
            "    \"99\": 99\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"0\": \"0\",\n",
            "    \"1\": \"1\",\n",
            "    \"2\": \"2\",\n",
            "    \"3\": \"3\",\n",
            "    \"4\": \"4\",\n",
            "    \"5\": \"5\",\n",
            "    \"6\": \"6\",\n",
            "    \"7\": \"7\",\n",
            "    \"8\": \"8\",\n",
            "    \"9\": \"9\",\n",
            "    \"10\": \"10\",\n",
            "    \"11\": \"11\",\n",
            "    \"12\": \"12\",\n",
            "    \"13\": \"13\",\n",
            "    \"14\": \"14\",\n",
            "    \"15\": \"15\",\n",
            "    \"16\": \"16\",\n",
            "    \"17\": \"17\",\n",
            "    \"18\": \"18\",\n",
            "    \"19\": \"19\",\n",
            "    \"20\": \"20\",\n",
            "    \"21\": \"21\",\n",
            "    \"22\": \"22\",\n",
            "    \"23\": \"23\",\n",
            "    \"24\": \"24\",\n",
            "    \"25\": \"25\",\n",
            "    \"26\": \"26\",\n",
            "    \"27\": \"27\",\n",
            "    \"28\": \"28\",\n",
            "    \"29\": \"29\",\n",
            "    \"30\": \"30\",\n",
            "    \"31\": \"31\",\n",
            "    \"32\": \"32\",\n",
            "    \"33\": \"33\",\n",
            "    \"34\": \"34\",\n",
            "    \"35\": \"35\",\n",
            "    \"36\": \"36\",\n",
            "    \"37\": \"37\",\n",
            "    \"38\": \"38\",\n",
            "    \"39\": \"39\",\n",
            "    \"40\": \"40\",\n",
            "    \"41\": \"41\",\n",
            "    \"42\": \"42\",\n",
            "    \"43\": \"43\",\n",
            "    \"44\": \"44\",\n",
            "    \"45\": \"45\",\n",
            "    \"46\": \"46\",\n",
            "    \"47\": \"47\",\n",
            "    \"48\": \"48\",\n",
            "    \"49\": \"49\",\n",
            "    \"50\": \"50\",\n",
            "    \"51\": \"51\",\n",
            "    \"52\": \"52\",\n",
            "    \"53\": \"53\",\n",
            "    \"54\": \"54\",\n",
            "    \"55\": \"55\",\n",
            "    \"56\": \"56\",\n",
            "    \"57\": \"57\",\n",
            "    \"58\": \"58\",\n",
            "    \"59\": \"59\",\n",
            "    \"60\": \"60\",\n",
            "    \"61\": \"61\",\n",
            "    \"62\": \"62\",\n",
            "    \"63\": \"63\",\n",
            "    \"64\": \"64\",\n",
            "    \"65\": \"65\",\n",
            "    \"66\": \"66\",\n",
            "    \"67\": \"67\",\n",
            "    \"68\": \"68\",\n",
            "    \"69\": \"69\",\n",
            "    \"70\": \"70\",\n",
            "    \"71\": \"71\",\n",
            "    \"72\": \"72\",\n",
            "    \"73\": \"73\",\n",
            "    \"74\": \"74\",\n",
            "    \"75\": \"75\",\n",
            "    \"76\": \"76\",\n",
            "    \"77\": \"77\",\n",
            "    \"78\": \"78\",\n",
            "    \"79\": \"79\",\n",
            "    \"80\": \"80\",\n",
            "    \"81\": \"81\",\n",
            "    \"82\": \"82\",\n",
            "    \"83\": \"83\",\n",
            "    \"84\": \"84\",\n",
            "    \"85\": \"85\",\n",
            "    \"86\": \"86\",\n",
            "    \"87\": \"87\",\n",
            "    \"88\": \"88\",\n",
            "    \"89\": \"89\",\n",
            "    \"90\": \"90\",\n",
            "    \"91\": \"91\",\n",
            "    \"92\": \"92\",\n",
            "    \"93\": \"93\",\n",
            "    \"94\": \"94\",\n",
            "    \"95\": \"95\",\n",
            "    \"96\": \"96\",\n",
            "    \"97\": \"97\",\n",
            "    \"98\": \"98\",\n",
            "    \"99\": \"99\",\n",
            "    \"100\": \"100\",\n",
            "    \"101\": \"101\",\n",
            "    \"102\": \"102\",\n",
            "    \"103\": \"103\",\n",
            "    \"104\": \"104\",\n",
            "    \"105\": \"105\",\n",
            "    \"106\": \"106\",\n",
            "    \"107\": \"107\",\n",
            "    \"108\": \"108\",\n",
            "    \"109\": \"109\",\n",
            "    \"110\": \"110\",\n",
            "    \"111\": \"111\",\n",
            "    \"112\": \"112\",\n",
            "    \"113\": \"113\",\n",
            "    \"114\": \"114\",\n",
            "    \"115\": \"115\",\n",
            "    \"116\": \"116\",\n",
            "    \"117\": \"117\",\n",
            "    \"118\": \"118\",\n",
            "    \"119\": \"119\",\n",
            "    \"120\": \"120\",\n",
            "    \"121\": \"121\",\n",
            "    \"122\": \"122\",\n",
            "    \"123\": \"123\",\n",
            "    \"124\": \"124\",\n",
            "    \"125\": \"125\",\n",
            "    \"126\": \"126\",\n",
            "    \"127\": \"127\",\n",
            "    \"128\": \"128\",\n",
            "    \"129\": \"129\",\n",
            "    \"130\": \"130\",\n",
            "    \"131\": \"131\",\n",
            "    \"132\": \"132\",\n",
            "    \"133\": \"133\",\n",
            "    \"134\": \"134\",\n",
            "    \"135\": \"135\",\n",
            "    \"136\": \"136\",\n",
            "    \"137\": \"137\",\n",
            "    \"138\": \"138\",\n",
            "    \"139\": \"139\",\n",
            "    \"140\": \"140\",\n",
            "    \"141\": \"141\",\n",
            "    \"142\": \"142\",\n",
            "    \"143\": \"143\",\n",
            "    \"144\": \"144\",\n",
            "    \"145\": \"145\",\n",
            "    \"146\": \"146\",\n",
            "    \"147\": \"147\",\n",
            "    \"148\": \"148\",\n",
            "    \"149\": \"149\",\n",
            "    \"150\": \"150\",\n",
            "    \"151\": \"151\",\n",
            "    \"152\": \"152\",\n",
            "    \"153\": \"153\",\n",
            "    \"154\": \"154\",\n",
            "    \"155\": \"155\",\n",
            "    \"156\": \"156\",\n",
            "    \"157\": \"157\",\n",
            "    \"158\": \"158\",\n",
            "    \"159\": \"159\",\n",
            "    \"160\": \"160\",\n",
            "    \"161\": \"161\",\n",
            "    \"162\": \"162\",\n",
            "    \"163\": \"163\",\n",
            "    \"164\": \"164\",\n",
            "    \"165\": \"165\",\n",
            "    \"166\": \"166\",\n",
            "    \"167\": \"167\",\n",
            "    \"168\": \"168\",\n",
            "    \"169\": \"169\",\n",
            "    \"170\": \"170\",\n",
            "    \"171\": \"171\",\n",
            "    \"172\": \"172\",\n",
            "    \"173\": \"173\",\n",
            "    \"174\": \"174\",\n",
            "    \"175\": \"175\",\n",
            "    \"176\": \"176\",\n",
            "    \"177\": \"177\",\n",
            "    \"178\": \"178\",\n",
            "    \"179\": \"179\",\n",
            "    \"180\": \"180\",\n",
            "    \"181\": \"181\",\n",
            "    \"182\": \"182\",\n",
            "    \"183\": \"183\",\n",
            "    \"184\": \"184\",\n",
            "    \"185\": \"185\",\n",
            "    \"186\": \"186\",\n",
            "    \"187\": \"187\",\n",
            "    \"188\": \"188\",\n",
            "    \"189\": \"189\",\n",
            "    \"190\": \"190\",\n",
            "    \"191\": \"191\",\n",
            "    \"192\": \"192\",\n",
            "    \"193\": \"193\",\n",
            "    \"194\": \"194\",\n",
            "    \"195\": \"195\",\n",
            "    \"196\": \"196\",\n",
            "    \"197\": \"197\",\n",
            "    \"198\": \"198\",\n",
            "    \"199\": \"199\",\n",
            "    \"200\": \"200\",\n",
            "    \"201\": \"201\",\n",
            "    \"202\": \"202\",\n",
            "    \"203\": \"203\",\n",
            "    \"204\": \"204\",\n",
            "    \"205\": \"205\",\n",
            "    \"206\": \"206\",\n",
            "    \"207\": \"207\",\n",
            "    \"208\": \"208\",\n",
            "    \"209\": \"209\",\n",
            "    \"210\": \"210\",\n",
            "    \"211\": \"211\",\n",
            "    \"212\": \"212\",\n",
            "    \"213\": \"213\",\n",
            "    \"214\": \"214\",\n",
            "    \"215\": \"215\",\n",
            "    \"216\": \"216\",\n",
            "    \"217\": \"217\",\n",
            "    \"218\": \"218\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mask_ratio\": 0.75,\n",
            "  \"model_type\": \"vit_mae\",\n",
            "  \"norm_pix_loss\": true,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.19.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-05-01 04:48:08,696 >> https://huggingface.co/gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp56kvb5k4\n",
            "Downloading: 100% 427M/427M [00:09<00:00, 44.8MB/s]\n",
            "[INFO|hub.py:587] 2022-05-01 04:48:18,810 >> storing https://huggingface.co/gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6fcf3eebd4728c3eae1e2393f988eca97c65e71dc6807a886ed556771920db91.06dff2f0c9f436d20f179eefb1358baf902397c7d627a6acd0b304dbe461df4f\n",
            "[INFO|hub.py:595] 2022-05-01 04:48:18,811 >> creating metadata file for /root/.cache/huggingface/transformers/6fcf3eebd4728c3eae1e2393f988eca97c65e71dc6807a886ed556771920db91.06dff2f0c9f436d20f179eefb1358baf902397c7d627a6acd0b304dbe461df4f\n",
            "[INFO|modeling_utils.py:1880] 2022-05-01 04:48:18,811 >> loading weights file https://huggingface.co/gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6fcf3eebd4728c3eae1e2393f988eca97c65e71dc6807a886ed556771920db91.06dff2f0c9f436d20f179eefb1358baf902397c7d627a6acd0b304dbe461df4f\n",
            "[WARNING|modeling_utils.py:2182] 2022-05-01 04:48:20,739 >> Some weights of the model checkpoint at gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae were not used when initializing ViTForImageClassification: ['decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.mask_token', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.query.bias']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2193] 2022-05-01 04:48:20,739 >> Some weights of ViTForImageClassification were not initialized from the model checkpoint at gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|hub.py:583] 2022-05-01 04:48:20,883 >> https://huggingface.co/gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae/resolve/main/preprocessor_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpegm14_l9\n",
            "Downloading: 100% 228/228 [00:00<00:00, 164kB/s]\n",
            "[INFO|hub.py:587] 2022-05-01 04:48:21,037 >> storing https://huggingface.co/gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae/resolve/main/preprocessor_config.json in cache at /root/.cache/huggingface/transformers/52f394058e38a990b78050ae7c1abddf74d678354ef26fa36a0afcdab23a8685.08655ed7bb323a517686dca7a2716a9fa479de0fa6b11dcf6906fa61e45c4490\n",
            "[INFO|hub.py:595] 2022-05-01 04:48:21,038 >> creating metadata file for /root/.cache/huggingface/transformers/52f394058e38a990b78050ae7c1abddf74d678354ef26fa36a0afcdab23a8685.08655ed7bb323a517686dca7a2716a9fa479de0fa6b11dcf6906fa61e45c4490\n",
            "[INFO|feature_extraction_utils.py:465] 2022-05-01 04:48:21,039 >> loading feature extractor configuration file https://huggingface.co/gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/52f394058e38a990b78050ae7c1abddf74d678354ef26fa36a0afcdab23a8685.08655ed7bb323a517686dca7a2716a9fa479de0fa6b11dcf6906fa61e45c4490\n",
            "[INFO|feature_extraction_utils.py:501] 2022-05-01 04:48:21,041 >> Feature extractor ViTFeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"resample\": 2,\n",
            "  \"size\": 224\n",
            "}\n",
            "\n",
            "Cloning https://huggingface.co/gary109/orchid219_ft_vit-base-mae into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/gary109/orchid219_ft_vit-base-mae into local empty directory.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1317] 2022-05-01 04:48:31,258 >> ***** Running training *****\n",
            "[INFO|trainer.py:1318] 2022-05-01 04:48:31,258 >>   Num examples = 1971\n",
            "[INFO|trainer.py:1319] 2022-05-01 04:48:31,258 >>   Num Epochs = 100\n",
            "[INFO|trainer.py:1320] 2022-05-01 04:48:31,259 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1321] 2022-05-01 04:48:31,259 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1322] 2022-05-01 04:48:31,259 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1323] 2022-05-01 04:48:31,259 >>   Total optimization steps = 12400\n",
            "{'loss': 5.4017, 'learning_rate': 1.9983870967741937e-05, 'epoch': 0.08}\n",
            "{'loss': 5.4146, 'learning_rate': 1.9967741935483872e-05, 'epoch': 0.16}\n",
            "{'loss': 5.4103, 'learning_rate': 1.9951612903225808e-05, 'epoch': 0.24}\n",
            "{'loss': 5.3876, 'learning_rate': 1.9935483870967743e-05, 'epoch': 0.32}\n",
            "{'loss': 5.3891, 'learning_rate': 1.9919354838709678e-05, 'epoch': 0.4}\n",
            "{'loss': 5.395, 'learning_rate': 1.9903225806451613e-05, 'epoch': 0.48}\n",
            "{'loss': 5.3994, 'learning_rate': 1.9887096774193552e-05, 'epoch': 0.56}\n",
            "{'loss': 5.3823, 'learning_rate': 1.9870967741935484e-05, 'epoch': 0.65}\n",
            "{'loss': 5.3674, 'learning_rate': 1.9854838709677423e-05, 'epoch': 0.73}\n",
            "{'loss': 5.3543, 'learning_rate': 1.9838709677419358e-05, 'epoch': 0.81}\n",
            "{'loss': 5.3425, 'learning_rate': 1.982258064516129e-05, 'epoch': 0.89}\n",
            "{'loss': 5.3185, 'learning_rate': 1.980645161290323e-05, 'epoch': 0.97}\n",
            "  1% 124/12400 [01:03<41:37,  4.92it/s][INFO|trainer.py:2443] 2022-05-01 04:49:49,508 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-05-01 04:49:49,508 >>   Num examples = 219\n",
            "[INFO|trainer.py:2448] 2022-05-01 04:49:49,508 >>   Batch size = 16\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 21% 3/14 [00:00<00:00, 20.13it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:00,  8.57it/s]\u001b[A\n",
            " 57% 8/14 [00:00<00:00,  8.29it/s]\u001b[A\n",
            " 71% 10/14 [00:01<00:00,  9.45it/s]\u001b[A\n",
            " 86% 12/14 [00:01<00:00, 10.31it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 5.310067653656006, 'eval_accuracy': 0.0273972602739726, 'eval_runtime': 10.0947, 'eval_samples_per_second': 21.694, 'eval_steps_per_second': 1.387, 'epoch': 1.0}\n",
            "  1% 124/12400 [01:28<41:37,  4.92it/s]\n",
            "100% 14/14 [00:04<00:00,  1.62it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2163] 2022-05-01 04:49:59,604 >> Saving model checkpoint to ./orchid219_ft_vit-base-mae/checkpoint-124\n",
            "[INFO|configuration_utils.py:446] 2022-05-01 04:49:59,607 >> Configuration saved in ./orchid219_ft_vit-base-mae/checkpoint-124/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-05-01 04:50:02,508 >> Model weights saved in ./orchid219_ft_vit-base-mae/checkpoint-124/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-01 04:50:02,509 >> Feature extractor saved in ./orchid219_ft_vit-base-mae/checkpoint-124/preprocessor_config.json\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-01 04:50:09,746 >> Feature extractor saved in ./orchid219_ft_vit-base-mae/preprocessor_config.json\n",
            "{'loss': 5.2928, 'learning_rate': 1.9790322580645164e-05, 'epoch': 1.05}\n",
            "{'loss': 5.2816, 'learning_rate': 1.97741935483871e-05, 'epoch': 1.13}\n",
            "{'loss': 5.2417, 'learning_rate': 1.9758064516129035e-05, 'epoch': 1.21}\n",
            "{'loss': 5.227, 'learning_rate': 1.974193548387097e-05, 'epoch': 1.29}\n",
            "{'loss': 5.2586, 'learning_rate': 1.9725806451612905e-05, 'epoch': 1.37}\n",
            "{'loss': 5.211, 'learning_rate': 1.970967741935484e-05, 'epoch': 1.45}\n",
            "{'loss': 5.1878, 'learning_rate': 1.9693548387096776e-05, 'epoch': 1.53}\n",
            "{'loss': 5.2182, 'learning_rate': 1.967741935483871e-05, 'epoch': 1.61}\n",
            "{'loss': 5.1732, 'learning_rate': 1.9661290322580647e-05, 'epoch': 1.69}\n",
            "{'loss': 5.177, 'learning_rate': 1.9645161290322582e-05, 'epoch': 1.77}\n",
            "{'loss': 5.1496, 'learning_rate': 1.9629032258064517e-05, 'epoch': 1.85}\n",
            "{'loss': 5.1398, 'learning_rate': 1.9612903225806452e-05, 'epoch': 1.94}\n",
            "  2% 248/12400 [02:16<42:01,  4.82it/s][INFO|trainer.py:2443] 2022-05-01 04:50:47,968 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-05-01 04:50:47,968 >>   Num examples = 219\n",
            "[INFO|trainer.py:2448] 2022-05-01 04:50:47,968 >>   Batch size = 16\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 21% 3/14 [00:00<00:00, 15.06it/s]\u001b[A\n",
            " 36% 5/14 [00:00<00:01,  5.39it/s]\u001b[A\n",
            " 50% 7/14 [00:01<00:01,  6.67it/s]\u001b[A\n",
            " 57% 8/14 [00:01<00:00,  7.02it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:01,  3.84it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  5.31it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 5.137734413146973, 'eval_accuracy': 0.0639269406392694, 'eval_runtime': 3.3968, 'eval_samples_per_second': 64.472, 'eval_steps_per_second': 4.121, 'epoch': 2.0}\n",
            "  2% 248/12400 [02:20<42:01,  4.82it/s]\n",
            "100% 14/14 [00:02<00:00,  6.67it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2163] 2022-05-01 04:50:51,366 >> Saving model checkpoint to ./orchid219_ft_vit-base-mae/checkpoint-248\n",
            "[INFO|configuration_utils.py:446] 2022-05-01 04:50:51,370 >> Configuration saved in ./orchid219_ft_vit-base-mae/checkpoint-248/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-05-01 04:50:54,218 >> Model weights saved in ./orchid219_ft_vit-base-mae/checkpoint-248/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-01 04:50:54,220 >> Feature extractor saved in ./orchid219_ft_vit-base-mae/checkpoint-248/preprocessor_config.json\n",
            "{'loss': 5.0976, 'learning_rate': 1.9596774193548388e-05, 'epoch': 2.02}\n",
            "{'loss': 5.0737, 'learning_rate': 1.9580645161290323e-05, 'epoch': 2.1}\n",
            "{'loss': 5.0617, 'learning_rate': 1.9564516129032262e-05, 'epoch': 2.18}\n",
            "{'loss': 5.0629, 'learning_rate': 1.9548387096774194e-05, 'epoch': 2.26}\n",
            "{'loss': 5.0428, 'learning_rate': 1.953225806451613e-05, 'epoch': 2.34}\n",
            "{'loss': 5.0304, 'learning_rate': 1.9516129032258068e-05, 'epoch': 2.42}\n",
            "{'loss': 5.0278, 'learning_rate': 1.95e-05, 'epoch': 2.5}\n",
            "{'loss': 5.0266, 'learning_rate': 1.948387096774194e-05, 'epoch': 2.58}\n",
            "{'loss': 5.0199, 'learning_rate': 1.9467741935483874e-05, 'epoch': 2.66}\n",
            "{'loss': 4.9877, 'learning_rate': 1.9451612903225806e-05, 'epoch': 2.74}\n",
            "{'loss': 4.9842, 'learning_rate': 1.9435483870967744e-05, 'epoch': 2.82}\n",
            "{'loss': 4.9684, 'learning_rate': 1.941935483870968e-05, 'epoch': 2.9}\n",
            "{'loss': 4.9618, 'learning_rate': 1.9403225806451615e-05, 'epoch': 2.98}\n",
            "  3% 372/12400 [03:00<44:18,  4.52it/s][INFO|trainer.py:2443] 2022-05-01 04:51:32,131 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-05-01 04:51:32,131 >>   Num examples = 219\n",
            "[INFO|trainer.py:2448] 2022-05-01 04:51:32,131 >>   Batch size = 16\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 19.75it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:00, 12.72it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:01,  5.15it/s]\u001b[A\n",
            " 50% 7/14 [00:01<00:01,  5.82it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:01,  4.00it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  5.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 4.996541976928711, 'eval_accuracy': 0.1278538812785388, 'eval_runtime': 3.4443, 'eval_samples_per_second': 63.584, 'eval_steps_per_second': 4.065, 'epoch': 3.0}\n",
            "  3% 372/12400 [03:04<44:18,  4.52it/s]\n",
            "100% 14/14 [00:02<00:00,  6.55it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2163] 2022-05-01 04:51:35,577 >> Saving model checkpoint to ./orchid219_ft_vit-base-mae/checkpoint-372\n",
            "[INFO|configuration_utils.py:446] 2022-05-01 04:51:35,581 >> Configuration saved in ./orchid219_ft_vit-base-mae/checkpoint-372/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-05-01 04:51:37,943 >> Model weights saved in ./orchid219_ft_vit-base-mae/checkpoint-372/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-01 04:51:37,944 >> Feature extractor saved in ./orchid219_ft_vit-base-mae/checkpoint-372/preprocessor_config.json\n",
            "{'loss': 4.9136, 'learning_rate': 1.938709677419355e-05, 'epoch': 3.06}\n",
            "{'loss': 4.9025, 'learning_rate': 1.9370967741935486e-05, 'epoch': 3.15}\n",
            "{'loss': 4.9055, 'learning_rate': 1.935483870967742e-05, 'epoch': 3.23}\n",
            "{'loss': 4.8821, 'learning_rate': 1.9338709677419356e-05, 'epoch': 3.31}\n",
            "{'loss': 4.8874, 'learning_rate': 1.932258064516129e-05, 'epoch': 3.39}\n",
            "{'loss': 4.8574, 'learning_rate': 1.9306451612903227e-05, 'epoch': 3.47}\n",
            "{'loss': 4.8869, 'learning_rate': 1.9290322580645162e-05, 'epoch': 3.55}\n",
            "{'loss': 4.8358, 'learning_rate': 1.9274193548387097e-05, 'epoch': 3.63}\n",
            "{'loss': 4.8663, 'learning_rate': 1.9258064516129033e-05, 'epoch': 3.71}\n",
            "{'loss': 4.8188, 'learning_rate': 1.9241935483870968e-05, 'epoch': 3.79}\n",
            "{'loss': 4.818, 'learning_rate': 1.9225806451612907e-05, 'epoch': 3.87}\n",
            "{'loss': 4.8231, 'learning_rate': 1.920967741935484e-05, 'epoch': 3.95}\n",
            "  4% 496/12400 [03:45<41:48,  4.75it/s][INFO|trainer.py:2443] 2022-05-01 04:52:17,093 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-05-01 04:52:17,093 >>   Num examples = 219\n",
            "[INFO|trainer.py:2448] 2022-05-01 04:52:17,093 >>   Batch size = 16\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 21% 3/14 [00:00<00:00, 15.67it/s]\u001b[A\n",
            " 36% 5/14 [00:00<00:01,  5.14it/s]\u001b[A\n",
            " 50% 7/14 [00:01<00:01,  6.48it/s]\u001b[A\n",
            " 57% 8/14 [00:01<00:00,  7.04it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:01,  3.72it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  5.22it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 4.856049060821533, 'eval_accuracy': 0.1963470319634703, 'eval_runtime': 3.4944, 'eval_samples_per_second': 62.672, 'eval_steps_per_second': 4.006, 'epoch': 4.0}\n",
            "  4% 496/12400 [03:49<41:48,  4.75it/s]\n",
            "100% 14/14 [00:02<00:00,  6.57it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2163] 2022-05-01 04:52:20,589 >> Saving model checkpoint to ./orchid219_ft_vit-base-mae/checkpoint-496\n",
            "[INFO|configuration_utils.py:446] 2022-05-01 04:52:20,593 >> Configuration saved in ./orchid219_ft_vit-base-mae/checkpoint-496/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-05-01 04:52:23,111 >> Model weights saved in ./orchid219_ft_vit-base-mae/checkpoint-496/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-01 04:52:23,112 >> Feature extractor saved in ./orchid219_ft_vit-base-mae/checkpoint-496/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-05-01 04:52:29,009 >> Deleting older checkpoint [orchid219_ft_vit-base-mae/checkpoint-124] due to args.save_total_limit\n",
            "{'loss': 4.7831, 'learning_rate': 1.9193548387096777e-05, 'epoch': 4.03}\n",
            "{'loss': 4.806, 'learning_rate': 1.9177419354838713e-05, 'epoch': 4.11}\n",
            "{'loss': 4.7722, 'learning_rate': 1.9161290322580645e-05, 'epoch': 4.19}\n",
            "{'loss': 4.7472, 'learning_rate': 1.9145161290322583e-05, 'epoch': 4.27}\n",
            "{'loss': 4.7307, 'learning_rate': 1.912903225806452e-05, 'epoch': 4.35}\n",
            "{'loss': 4.7283, 'learning_rate': 1.9112903225806454e-05, 'epoch': 4.44}\n",
            "{'loss': 4.7502, 'learning_rate': 1.909677419354839e-05, 'epoch': 4.52}\n",
            "{'loss': 4.7031, 'learning_rate': 1.9080645161290324e-05, 'epoch': 4.6}\n",
            "{'loss': 4.7451, 'learning_rate': 1.906451612903226e-05, 'epoch': 4.68}\n",
            "{'loss': 4.6902, 'learning_rate': 1.9048387096774195e-05, 'epoch': 4.76}\n",
            "{'loss': 4.6769, 'learning_rate': 1.903225806451613e-05, 'epoch': 4.84}\n",
            "{'loss': 4.6579, 'learning_rate': 1.9016129032258066e-05, 'epoch': 4.92}\n",
            "{'loss': 4.6906, 'learning_rate': 1.9e-05, 'epoch': 5.0}\n",
            "  5% 620/12400 [04:31<41:04,  4.78it/s][INFO|trainer.py:2443] 2022-05-01 04:53:02,520 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-05-01 04:53:02,520 >>   Num examples = 219\n",
            "[INFO|trainer.py:2448] 2022-05-01 04:53:02,520 >>   Batch size = 16\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 21% 3/14 [00:00<00:00, 15.42it/s]\u001b[A\n",
            " 36% 5/14 [00:00<00:01,  5.22it/s]\u001b[A\n",
            " 50% 7/14 [00:01<00:01,  6.60it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:01,  4.18it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  5.39it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 4.748806476593018, 'eval_accuracy': 0.228310502283105, 'eval_runtime': 3.5006, 'eval_samples_per_second': 62.561, 'eval_steps_per_second': 3.999, 'epoch': 5.0}\n",
            "  5% 620/12400 [04:34<41:04,  4.78it/s]\n",
            "100% 14/14 [00:02<00:00,  6.61it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2163] 2022-05-01 04:53:06,022 >> Saving model checkpoint to ./orchid219_ft_vit-base-mae/checkpoint-620\n",
            "[INFO|configuration_utils.py:446] 2022-05-01 04:53:06,026 >> Configuration saved in ./orchid219_ft_vit-base-mae/checkpoint-620/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-05-01 04:53:08,577 >> Model weights saved in ./orchid219_ft_vit-base-mae/checkpoint-620/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-01 04:53:08,610 >> Feature extractor saved in ./orchid219_ft_vit-base-mae/checkpoint-620/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-05-01 04:53:14,492 >> Deleting older checkpoint [orchid219_ft_vit-base-mae/checkpoint-248] due to args.save_total_limit\n",
            "{'loss': 4.6283, 'learning_rate': 1.8983870967741936e-05, 'epoch': 5.08}\n",
            "{'loss': 4.617, 'learning_rate': 1.896774193548387e-05, 'epoch': 5.16}\n",
            "{'loss': 4.5783, 'learning_rate': 1.895161290322581e-05, 'epoch': 5.24}\n",
            "{'loss': 4.5647, 'learning_rate': 1.8935483870967742e-05, 'epoch': 5.32}\n",
            "{'loss': 4.6143, 'learning_rate': 1.8919354838709678e-05, 'epoch': 5.4}\n",
            "{'loss': 4.6258, 'learning_rate': 1.8903225806451616e-05, 'epoch': 5.48}\n",
            "{'loss': 4.6114, 'learning_rate': 1.8887096774193548e-05, 'epoch': 5.56}\n",
            "{'loss': 4.6235, 'learning_rate': 1.8870967741935487e-05, 'epoch': 5.65}\n",
            "{'loss': 4.6166, 'learning_rate': 1.8854838709677422e-05, 'epoch': 5.73}\n",
            "{'loss': 4.5528, 'learning_rate': 1.8838709677419354e-05, 'epoch': 5.81}\n",
            "{'loss': 4.5413, 'learning_rate': 1.8822580645161293e-05, 'epoch': 5.89}\n",
            "{'loss': 4.5372, 'learning_rate': 1.8806451612903228e-05, 'epoch': 5.97}\n",
            "  6% 744/12400 [05:16<41:50,  4.64it/s][INFO|trainer.py:2443] 2022-05-01 04:53:48,083 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-05-01 04:53:48,084 >>   Num examples = 219\n",
            "[INFO|trainer.py:2448] 2022-05-01 04:53:48,084 >>   Batch size = 16\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 21% 3/14 [00:00<00:00, 15.97it/s]\u001b[A\n",
            " 36% 5/14 [00:00<00:01,  4.96it/s]\u001b[A\n",
            " 50% 7/14 [00:01<00:01,  6.30it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:01,  4.20it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  5.41it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 4.628534317016602, 'eval_accuracy': 0.3059360730593607, 'eval_runtime': 3.4817, 'eval_samples_per_second': 62.9, 'eval_steps_per_second': 4.021, 'epoch': 6.0}\n",
            "  6% 744/12400 [05:20<41:50,  4.64it/s]\n",
            "100% 14/14 [00:02<00:00,  6.66it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2163] 2022-05-01 04:53:51,567 >> Saving model checkpoint to ./orchid219_ft_vit-base-mae/checkpoint-744\n",
            "[INFO|configuration_utils.py:446] 2022-05-01 04:53:51,571 >> Configuration saved in ./orchid219_ft_vit-base-mae/checkpoint-744/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-05-01 04:53:53,986 >> Model weights saved in ./orchid219_ft_vit-base-mae/checkpoint-744/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-01 04:53:53,987 >> Feature extractor saved in ./orchid219_ft_vit-base-mae/checkpoint-744/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-05-01 04:53:59,502 >> Deleting older checkpoint [orchid219_ft_vit-base-mae/checkpoint-372] due to args.save_total_limit\n",
            "{'loss': 4.5445, 'learning_rate': 1.8790322580645163e-05, 'epoch': 6.05}\n",
            "{'loss': 4.4674, 'learning_rate': 1.87741935483871e-05, 'epoch': 6.13}\n",
            "{'loss': 4.4564, 'learning_rate': 1.8758064516129034e-05, 'epoch': 6.21}\n",
            "{'loss': 4.4882, 'learning_rate': 1.874193548387097e-05, 'epoch': 6.29}\n",
            "{'loss': 4.4852, 'learning_rate': 1.8725806451612905e-05, 'epoch': 6.37}\n",
            "{'loss': 4.5018, 'learning_rate': 1.870967741935484e-05, 'epoch': 6.45}\n",
            "{'loss': 4.4585, 'learning_rate': 1.8693548387096775e-05, 'epoch': 6.53}\n",
            "{'loss': 4.4587, 'learning_rate': 1.867741935483871e-05, 'epoch': 6.61}\n",
            "{'loss': 4.4661, 'learning_rate': 1.8661290322580646e-05, 'epoch': 6.69}\n",
            "{'loss': 4.4614, 'learning_rate': 1.864516129032258e-05, 'epoch': 6.77}\n",
            "{'loss': 4.4643, 'learning_rate': 1.8629032258064517e-05, 'epoch': 6.85}\n",
            "{'loss': 4.4008, 'learning_rate': 1.8612903225806452e-05, 'epoch': 6.94}\n",
            "  7% 868/12400 [06:00<39:11,  4.90it/s][INFO|trainer.py:2443] 2022-05-01 04:54:31,693 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-05-01 04:54:31,693 >>   Num examples = 219\n",
            "[INFO|trainer.py:2448] 2022-05-01 04:54:31,693 >>   Batch size = 16\n",
            "\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:00<00:00, 20.00it/s]\u001b[A\n",
            " 29% 4/14 [00:00<00:00, 12.70it/s]\u001b[A\n",
            " 43% 6/14 [00:00<00:01,  5.58it/s]\u001b[A\n",
            " 57% 8/14 [00:01<00:00,  6.81it/s]\u001b[A\n",
            " 64% 9/14 [00:01<00:01,  3.98it/s]\u001b[A\n",
            " 79% 11/14 [00:01<00:00,  5.41it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 4.518697261810303, 'eval_accuracy': 0.3515981735159817, 'eval_runtime': 3.3237, 'eval_samples_per_second': 65.889, 'eval_steps_per_second': 4.212, 'epoch': 7.0}\n",
            "  7% 868/12400 [06:03<39:11,  4.90it/s]\n",
            "100% 14/14 [00:02<00:00,  6.76it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2163] 2022-05-01 04:54:35,018 >> Saving model checkpoint to ./orchid219_ft_vit-base-mae/checkpoint-868\n",
            "[INFO|configuration_utils.py:446] 2022-05-01 04:54:35,021 >> Configuration saved in ./orchid219_ft_vit-base-mae/checkpoint-868/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-05-01 04:54:37,162 >> Model weights saved in ./orchid219_ft_vit-base-mae/checkpoint-868/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-01 04:54:37,167 >> Feature extractor saved in ./orchid219_ft_vit-base-mae/checkpoint-868/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-05-01 04:54:48,709 >> Deleting older checkpoint [orchid219_ft_vit-base-mae/checkpoint-496] due to args.save_total_limit\n",
            "{'loss': 4.475, 'learning_rate': 1.8596774193548387e-05, 'epoch': 7.02}\n",
            "{'loss': 4.413, 'learning_rate': 1.8580645161290326e-05, 'epoch': 7.1}\n",
            "{'loss': 4.3258, 'learning_rate': 1.856451612903226e-05, 'epoch': 7.18}\n",
            "{'loss': 4.3324, 'learning_rate': 1.8548387096774193e-05, 'epoch': 7.26}\n",
            "{'loss': 4.3746, 'learning_rate': 1.8532258064516132e-05, 'epoch': 7.34}\n",
            "{'loss': 4.3971, 'learning_rate': 1.8516129032258067e-05, 'epoch': 7.42}\n",
            "{'loss': 4.347, 'learning_rate': 1.8500000000000002e-05, 'epoch': 7.5}\n",
            "{'loss': 4.3415, 'learning_rate': 1.8483870967741938e-05, 'epoch': 7.58}\n",
            "  8% 943/12400 [06:37<47:39,  4.01it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## facebook/data2vec-vision-base-ft1k\n",
        "---"
      ],
      "metadata": {
        "id": "Zl3ycpATNA-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"facebook/data2vec-vision-base\" \\\n",
        "    --output_dir=\"./orchid219_data2vec-vision-base\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_data2vec-vision-base\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \n",
        "    # --cache_dir=\"./cache_data2vec-vision-base-ft1k\"\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing\n",
        "\n",
        "# --model_name_or_path \"gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae\" \\\n",
        "    #orchid219_pretrain_vit-base-patch16-224-in21k-mae\n",
        "    # --model_name_or_path \"gary109/orchid219_vit-base-patch16-224-in21k\" "
      ],
      "metadata": {
        "id": "HYRn6_6PNAaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Others\n",
        "---\n",
        "- google/vit-base-patch16-224\n",
        "- google/vit-base-patch16-384\n",
        "- google/vit-base-patch32-384\n",
        "\n",
        "- google/vit-large-patch16-384\n",
        "- google/vit-large-patch16-224\n",
        "- google/vit-large-patch32-384"
      ],
      "metadata": {
        "id": "wVPViAfE4vc6"
      }
    }
  ]
}