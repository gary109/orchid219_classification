{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune Orchid219 Image classification with ViT",
      "provenance": [],
      "collapsed_sections": [
        "2kKsDcHu0hlX",
        "kYFkur3sEb6f",
        "NhO8MiJ41HLI",
        "RcO8DSXZz7W5",
        "4qU_1hTLWQn4",
        "WkwgQ3ZJd1Ig",
        "6hAF3zBoFjX6",
        "Q7MV70DBHbIq",
        "MzvQ9zfD34bE",
        "p9t54nRI4B5q",
        "NNvuAbjI4PWH",
        "WJsYvA7u4dLr",
        "qaNA2JF34UPN",
        "Wnmnj8KB9Whn",
        "Xo9M0Z8J6_8z",
        "ANg5qKaDeNKJ",
        "Zl3ycpATNA-w",
        "aogw2ll1JOZg",
        "5l6FGTXVYA6j"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fab48cb629a64f208ddd24a8c474ec5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44f50a1d2a004a6bb902c14d81fbd8a0",
              "IPY_MODEL_93e8a6547d734774a36cd913b0f6fdb0",
              "IPY_MODEL_cccbb8b7ebf34225ab9a755ea6a7a03a"
            ],
            "layout": "IPY_MODEL_1083296862674595b8df6fdc8935d6e7"
          }
        },
        "44f50a1d2a004a6bb902c14d81fbd8a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbe6cb98578c4063925dcd24ca324780",
            "placeholder": "​",
            "style": "IPY_MODEL_6734fa5fc09b4b3fb33a7a4aa1932eb9",
            "value": "100%"
          }
        },
        "93e8a6547d734774a36cd913b0f6fdb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deaa1644f6cc48608a2aa5c8b8afa042",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1061773125c40ac9e96b91cf7f57a4e",
            "value": 1
          }
        },
        "cccbb8b7ebf34225ab9a755ea6a7a03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35ff918111534bec94451232df24d865",
            "placeholder": "​",
            "style": "IPY_MODEL_93b1f128307b41cdb8521604fee47749",
            "value": " 1/1 [00:00&lt;00:00, 27.34it/s]"
          }
        },
        "1083296862674595b8df6fdc8935d6e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe6cb98578c4063925dcd24ca324780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6734fa5fc09b4b3fb33a7a4aa1932eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "deaa1644f6cc48608a2aa5c8b8afa042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1061773125c40ac9e96b91cf7f57a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35ff918111534bec94451232df24d865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b1f128307b41cdb8521604fee47749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4700aa07706413d83aa8df13b595fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85b37051b722460fb7383bba9742eca8",
              "IPY_MODEL_731922d9715e41ce91c138e3d807e597",
              "IPY_MODEL_440937b6a4ed48c9afa80e6f6e2986e4"
            ],
            "layout": "IPY_MODEL_71634fbe99ff482dbd047e29f0a29e5e"
          }
        },
        "85b37051b722460fb7383bba9742eca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f46d8305ba243f6a244fd20ab968689",
            "placeholder": "​",
            "style": "IPY_MODEL_92f9f4f265a0433da167d3cda5d05472",
            "value": "Pushing dataset shards to the dataset hub: 100%"
          }
        },
        "731922d9715e41ce91c138e3d807e597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8128bfc884e4d5ab91410d89e3bff65",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_059c6d26c3374d50ac30fd82083d2f7d",
            "value": 1
          }
        },
        "440937b6a4ed48c9afa80e6f6e2986e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_458c6977e6bf40b6ba5560637e53d3ce",
            "placeholder": "​",
            "style": "IPY_MODEL_80762b77e4834fb7946329cd1d2d2d7c",
            "value": " 1/1 [00:00&lt;00:00,  1.22it/s]"
          }
        },
        "71634fbe99ff482dbd047e29f0a29e5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f46d8305ba243f6a244fd20ab968689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f9f4f265a0433da167d3cda5d05472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8128bfc884e4d5ab91410d89e3bff65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "059c6d26c3374d50ac30fd82083d2f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "458c6977e6bf40b6ba5560637e53d3ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80762b77e4834fb7946329cd1d2d2d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8322dfac04394d27ba63467f8a523bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3b723bcb0004d56be9f6aea022c086c",
              "IPY_MODEL_be7080e2feff46bf8aeaf0445ab182c6",
              "IPY_MODEL_fd610c20fe35496cbd36f92d653fbff9"
            ],
            "layout": "IPY_MODEL_5931c1d8c6ed463f83f71554d81942ca"
          }
        },
        "c3b723bcb0004d56be9f6aea022c086c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc82eac98ac949d7957181cf23bff4e0",
            "placeholder": "​",
            "style": "IPY_MODEL_374237e61278446188a0a3a7d0edb43e",
            "value": "100%"
          }
        },
        "be7080e2feff46bf8aeaf0445ab182c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45ec73e18be142a18aa2bc51a3aa5889",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_711d3a19242644ce8116a951c8027840",
            "value": 1
          }
        },
        "fd610c20fe35496cbd36f92d653fbff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43cf40feb1af4e46adb68bb1fb8547d8",
            "placeholder": "​",
            "style": "IPY_MODEL_ee059f6125074996b5ac1facdd447f8b",
            "value": " 1/1 [00:00&lt;00:00, 10.73ba/s]"
          }
        },
        "5931c1d8c6ed463f83f71554d81942ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc82eac98ac949d7957181cf23bff4e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "374237e61278446188a0a3a7d0edb43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45ec73e18be142a18aa2bc51a3aa5889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711d3a19242644ce8116a951c8027840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43cf40feb1af4e46adb68bb1fb8547d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee059f6125074996b5ac1facdd447f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3ece1c021ee4045acdbbdd26caf0eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b477b885f26a44fc9ff0c8a5d7bb3206",
              "IPY_MODEL_b32f4272f1f04d76b99d4ca7dbacfba2",
              "IPY_MODEL_2027317bc21d49968c816c2c4f909533"
            ],
            "layout": "IPY_MODEL_daccf9a3441d4d2d8c17a0e7f3f4d1a8"
          }
        },
        "b477b885f26a44fc9ff0c8a5d7bb3206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cda33a487c0b4ed89460c880e53a8a24",
            "placeholder": "​",
            "style": "IPY_MODEL_c899aa6007b04e66b3aa4a2b49916670",
            "value": "Downloading: 100%"
          }
        },
        "b32f4272f1f04d76b99d4ca7dbacfba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3365dfe4c154249b72bfc615c31f3ba",
            "max": 302,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13acb81c371a412291490ca73d7f640b",
            "value": 302
          }
        },
        "2027317bc21d49968c816c2c4f909533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ca173fadc2b4f84ae55d360e4de607e",
            "placeholder": "​",
            "style": "IPY_MODEL_a1478aa9dc1148369e2b2457856159f2",
            "value": " 302/302 [00:00&lt;00:00, 7.07kB/s]"
          }
        },
        "daccf9a3441d4d2d8c17a0e7f3f4d1a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cda33a487c0b4ed89460c880e53a8a24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c899aa6007b04e66b3aa4a2b49916670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3365dfe4c154249b72bfc615c31f3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13acb81c371a412291490ca73d7f640b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ca173fadc2b4f84ae55d360e4de607e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1478aa9dc1148369e2b2457856159f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dac47f2197b948ae9f1426bb7df19a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be02ea367c3540c6bc0ff27244aedb60",
              "IPY_MODEL_6aa3b7e8557d43a5b35f988addfa229d",
              "IPY_MODEL_aef697538da441948968508353f85e72"
            ],
            "layout": "IPY_MODEL_d27b42c5ed344829a46b8a55eb2b1dd3"
          }
        },
        "be02ea367c3540c6bc0ff27244aedb60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc01fd7a1b754e73a9d28ce4bd34ff0b",
            "placeholder": "​",
            "style": "IPY_MODEL_3272340a2de740a6b80dc70bc2c6f645",
            "value": "Downloading: 100%"
          }
        },
        "6aa3b7e8557d43a5b35f988addfa229d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8923b23b2f59411cb5df57f011aed7d1",
            "max": 8245,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94e56179352444eb9a757fba5ee7c8de",
            "value": 8245
          }
        },
        "aef697538da441948968508353f85e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c90dc962ec1f4be4b2475395e1fb874d",
            "placeholder": "​",
            "style": "IPY_MODEL_d6987f0f33824b60865495cf653e7804",
            "value": " 8.05k/8.05k [00:00&lt;00:00, 233kB/s]"
          }
        },
        "d27b42c5ed344829a46b8a55eb2b1dd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc01fd7a1b754e73a9d28ce4bd34ff0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3272340a2de740a6b80dc70bc2c6f645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8923b23b2f59411cb5df57f011aed7d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94e56179352444eb9a757fba5ee7c8de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c90dc962ec1f4be4b2475395e1fb874d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6987f0f33824b60865495cf653e7804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cab36b8cc28248fba430fde50f7f13fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b30a3a31e05416bb43ceb38e9520b7d",
              "IPY_MODEL_d6d6bf67d4ae4c0cbadb4b1bbb5a4624",
              "IPY_MODEL_c5927cbe3e3c4ac9ad1d0bf77feacf09"
            ],
            "layout": "IPY_MODEL_2c82a67d16d740efba36cf967ba6c736"
          }
        },
        "3b30a3a31e05416bb43ceb38e9520b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237644cfc20141708406aefc32cb9e64",
            "placeholder": "​",
            "style": "IPY_MODEL_0513eddefae941ea99a3f0d42cf4ac99",
            "value": "Downloading: 100%"
          }
        },
        "d6d6bf67d4ae4c0cbadb4b1bbb5a4624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7987a41e0f3742389b6ae77570094b80",
            "max": 343718167,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ddbb449c7e441efb3122e3db8698906",
            "value": 343718167
          }
        },
        "c5927cbe3e3c4ac9ad1d0bf77feacf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a6d1b864f184162a62c6ec59a97ea79",
            "placeholder": "​",
            "style": "IPY_MODEL_c92176ba1ee142cca9d3ff2d5eb3f0c7",
            "value": " 328M/328M [00:07&lt;00:00, 43.6MB/s]"
          }
        },
        "2c82a67d16d740efba36cf967ba6c736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "237644cfc20141708406aefc32cb9e64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0513eddefae941ea99a3f0d42cf4ac99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7987a41e0f3742389b6ae77570094b80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ddbb449c7e441efb3122e3db8698906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a6d1b864f184162a62c6ec59a97ea79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c92176ba1ee142cca9d3ff2d5eb3f0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b59fa663bdf542d3ab7444d174c130c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da42b1b09ad7485abfe6a74032177097",
              "IPY_MODEL_bbd4b2e9a96846be8b5b6c6ab8f148b1",
              "IPY_MODEL_37e0f188836b4ccead445f559289dd1c"
            ],
            "layout": "IPY_MODEL_aabfeb44239349eca112e237d0127aec"
          }
        },
        "da42b1b09ad7485abfe6a74032177097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_123763be6ef244f4af84a7d49a72d7f2",
            "placeholder": "​",
            "style": "IPY_MODEL_cc7b3d15ee5146139ff47cf821c0a334",
            "value": "Downloading: 100%"
          }
        },
        "bbd4b2e9a96846be8b5b6c6ab8f148b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4e7f729be8c485791c96596cd4f55d0",
            "max": 228,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06fea7c460614a86969568ad087a2100",
            "value": 228
          }
        },
        "37e0f188836b4ccead445f559289dd1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb1344571cd647488c419d750e8b0b66",
            "placeholder": "​",
            "style": "IPY_MODEL_da724f5356fe4990871c79d6c95b7c6a",
            "value": " 228/228 [00:00&lt;00:00, 5.94kB/s]"
          }
        },
        "aabfeb44239349eca112e237d0127aec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "123763be6ef244f4af84a7d49a72d7f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7b3d15ee5146139ff47cf821c0a334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4e7f729be8c485791c96596cd4f55d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06fea7c460614a86969568ad087a2100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb1344571cd647488c419d750e8b0b66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da724f5356fe4990871c79d6c95b7c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02bd2ab7886b495b99fa1919452be503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e23c150ac0894b2eb4f6878aef099069",
              "IPY_MODEL_e709c0aeb7c44f389384d043c6c04324",
              "IPY_MODEL_b18f324f0c4145bd9296b5513052f03a"
            ],
            "layout": "IPY_MODEL_69097a583c884e95801d4a2b4a235d08"
          }
        },
        "e23c150ac0894b2eb4f6878aef099069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbde0163c3cf4bea9b0793ff741a6e4b",
            "placeholder": "​",
            "style": "IPY_MODEL_25aadd10c3584c1cb89f554f7a351381",
            "value": "Downloading: 100%"
          }
        },
        "e709c0aeb7c44f389384d043c6c04324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5859a0c8b93d4b1c916720c246e0bbdd",
            "max": 7900,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89371b7e8c8a42f5b56848a15c072733",
            "value": 7900
          }
        },
        "b18f324f0c4145bd9296b5513052f03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e7e5754037e485ca134cb9afd973629",
            "placeholder": "​",
            "style": "IPY_MODEL_0906d8334594404a8ca897140569599f",
            "value": " 7.71k/7.71k [00:00&lt;00:00, 197kB/s]"
          }
        },
        "69097a583c884e95801d4a2b4a235d08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbde0163c3cf4bea9b0793ff741a6e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25aadd10c3584c1cb89f554f7a351381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5859a0c8b93d4b1c916720c246e0bbdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89371b7e8c8a42f5b56848a15c072733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e7e5754037e485ca134cb9afd973629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0906d8334594404a8ca897140569599f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d7219d9c4b44de78cb62af940ac6192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_457090bdf30b4ccf8528225947d512e6",
              "IPY_MODEL_aeb00f75f9cc493da565eb4e204b9e06",
              "IPY_MODEL_71ab873c5753406bb69445b01105c882"
            ],
            "layout": "IPY_MODEL_b3a9288db84441fcbe4ab14bc38647e1"
          }
        },
        "457090bdf30b4ccf8528225947d512e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6673fb5450bf43168bdcddbab70bd8ae",
            "placeholder": "​",
            "style": "IPY_MODEL_b1b678e912084dc58970cb4e7fa912f8",
            "value": "Downloading: 100%"
          }
        },
        "aeb00f75f9cc493da565eb4e204b9e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22649f76d0c248f691a184425623039a",
            "max": 343934823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93ce7a1bbeba489cbcf76f03d1ceb563",
            "value": 343934823
          }
        },
        "71ab873c5753406bb69445b01105c882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_796ed23619af43dfb57c231ee06469e3",
            "placeholder": "​",
            "style": "IPY_MODEL_5d68aa66b3604c0ba5ef89feba16e4b8",
            "value": " 328M/328M [00:07&lt;00:00, 44.3MB/s]"
          }
        },
        "b3a9288db84441fcbe4ab14bc38647e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6673fb5450bf43168bdcddbab70bd8ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1b678e912084dc58970cb4e7fa912f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22649f76d0c248f691a184425623039a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93ce7a1bbeba489cbcf76f03d1ceb563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "796ed23619af43dfb57c231ee06469e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d68aa66b3604c0ba5ef89feba16e4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab172ea6d85f429ab907ab07015f04b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52768167b90d49998787469e578fb73f",
              "IPY_MODEL_cb87ca8a12c24aaeb37fc69394f8bd25",
              "IPY_MODEL_9b1eb15f1f444df5888631d0d78e82b4"
            ],
            "layout": "IPY_MODEL_dcc9405d3d644b72810b9460b7e5bffe"
          }
        },
        "52768167b90d49998787469e578fb73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_648d4d3da9434732a7a77f96c5caaa60",
            "placeholder": "​",
            "style": "IPY_MODEL_0a6deb829a2548d09453afee9cf73524",
            "value": "Downloading data: 100%"
          }
        },
        "cb87ca8a12c24aaeb37fc69394f8bd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67d8df12242b4e2db31dd943cca4498f",
            "max": 90862213,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d387361e11fd471ba4d0bd8c21d50aeb",
            "value": 90862213
          }
        },
        "9b1eb15f1f444df5888631d0d78e82b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5234078a06ce4b45b0a87367cffef658",
            "placeholder": "​",
            "style": "IPY_MODEL_29f93ba6af5a41279c4ed06a6c5a85b1",
            "value": " 90.9M/90.9M [00:02&lt;00:00, 57.3MB/s]"
          }
        },
        "dcc9405d3d644b72810b9460b7e5bffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "648d4d3da9434732a7a77f96c5caaa60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a6deb829a2548d09453afee9cf73524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67d8df12242b4e2db31dd943cca4498f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d387361e11fd471ba4d0bd8c21d50aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5234078a06ce4b45b0a87367cffef658": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29f93ba6af5a41279c4ed06a6c5a85b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "220593df1121490a9d985524a7817688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48acc97e36ca499b8d0c531868b1991b",
              "IPY_MODEL_d25eaab6ba07457c95b9a3be7ae7f3fd",
              "IPY_MODEL_f289fd6f1bf14325b1750c5e2f181602"
            ],
            "layout": "IPY_MODEL_7d9aef587ff74571b8fca8bdd12e1943"
          }
        },
        "48acc97e36ca499b8d0c531868b1991b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07de66f00cb8455d9ad6fc8072b8c0b5",
            "placeholder": "​",
            "style": "IPY_MODEL_f0f43722d8f54ac7a988e6f572fc1048",
            "value": "Generating train split: "
          }
        },
        "d25eaab6ba07457c95b9a3be7ae7f3fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d2c4fab239d4d1bb1564e4f19329035",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bef834d139ec496d9177c8c4d8010c8d",
            "value": 1
          }
        },
        "f289fd6f1bf14325b1750c5e2f181602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_770f4fabf5cc40c292741246ade3f814",
            "placeholder": "​",
            "style": "IPY_MODEL_cef1ae9f3e9f41c698127ce15ec87f7d",
            "value": " 1677/0 [00:00&lt;00:00, 3570.68 examples/s]"
          }
        },
        "7d9aef587ff74571b8fca8bdd12e1943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07de66f00cb8455d9ad6fc8072b8c0b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0f43722d8f54ac7a988e6f572fc1048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d2c4fab239d4d1bb1564e4f19329035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bef834d139ec496d9177c8c4d8010c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "770f4fabf5cc40c292741246ade3f814": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef1ae9f3e9f41c698127ce15ec87f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d40554d2e064ef6a4ca586c57d2c86f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f071029ebb6a4fd399af1f69423ffc5d",
              "IPY_MODEL_e06da8a8116246c59b18d7ffb662cdc2",
              "IPY_MODEL_6e053877aca84410988286f5bf777317"
            ],
            "layout": "IPY_MODEL_7a8828ae1f27436e8cca9023dc403d34"
          }
        },
        "f071029ebb6a4fd399af1f69423ffc5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a071fa1f1dc84fe4a1a48885aabd65ee",
            "placeholder": "​",
            "style": "IPY_MODEL_df0734f49334478b9876fcae8e65784b",
            "value": "Generating validation split: "
          }
        },
        "e06da8a8116246c59b18d7ffb662cdc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d937c6be56c14c1abe58d5416daaed2d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3bef52cdfc84003bce10ffb354fb001",
            "value": 1
          }
        },
        "6e053877aca84410988286f5bf777317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_346ec69d07cc45a2832dd5954940d5ea",
            "placeholder": "​",
            "style": "IPY_MODEL_b35e6c369b2e4e2abb778db68022b3b7",
            "value": " 0/0 [00:00&lt;?, ? examples/s]"
          }
        },
        "7a8828ae1f27436e8cca9023dc403d34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a071fa1f1dc84fe4a1a48885aabd65ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df0734f49334478b9876fcae8e65784b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d937c6be56c14c1abe58d5416daaed2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e3bef52cdfc84003bce10ffb354fb001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "346ec69d07cc45a2832dd5954940d5ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b35e6c369b2e4e2abb778db68022b3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a48e1b89a1e43d3b051d3d57945fbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14c87b4451284e66b1570430de55f192",
              "IPY_MODEL_5fcf3ae99de04c3c9636226f048fa640",
              "IPY_MODEL_4f9dd6c30dca47c595da772ac2e64014"
            ],
            "layout": "IPY_MODEL_f9c79ff8ae6b4259ac75711f5e10b248"
          }
        },
        "14c87b4451284e66b1570430de55f192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c0de6eb1d2942e0959bb227549651b3",
            "placeholder": "​",
            "style": "IPY_MODEL_540edbd69cdf4154ba3bf545d45b6f7e",
            "value": "100%"
          }
        },
        "5fcf3ae99de04c3c9636226f048fa640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e2a6e9938154c889f208f678de794e5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e7ce89fd8e1431c98f2a3d2783cf55e",
            "value": 2
          }
        },
        "4f9dd6c30dca47c595da772ac2e64014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17099b1c2af241f3b012f9dfad020e66",
            "placeholder": "​",
            "style": "IPY_MODEL_e665001844b745deab57e10f1d9d1006",
            "value": " 2/2 [00:00&lt;00:00, 53.45it/s]"
          }
        },
        "f9c79ff8ae6b4259ac75711f5e10b248": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c0de6eb1d2942e0959bb227549651b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "540edbd69cdf4154ba3bf545d45b6f7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e2a6e9938154c889f208f678de794e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e7ce89fd8e1431c98f2a3d2783cf55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17099b1c2af241f3b012f9dfad020e66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e665001844b745deab57e10f1d9d1006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5107725ab36c40c1bcb17243bb3cccb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b011df545494a7b9d50db19c0b1dd63",
              "IPY_MODEL_b2a596a16d3b422d8445b04fbf21f68d",
              "IPY_MODEL_a4495e118357455689569d2071d32c83"
            ],
            "layout": "IPY_MODEL_1f338259d63a4d50a01ce56de3689470"
          }
        },
        "2b011df545494a7b9d50db19c0b1dd63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64b1c13de1bd4cf6a124015030d9e8a2",
            "placeholder": "​",
            "style": "IPY_MODEL_7ec16532fb924f2da1762a4e8574a39c",
            "value": "Upload file pytorch_model.bin: 100%"
          }
        },
        "b2a596a16d3b422d8445b04fbf21f68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a8e37c333124813bb9ced06e6f1a572",
            "max": 343047703,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5177c3540f94fed9764e873aedc5f60",
            "value": 343047703
          }
        },
        "a4495e118357455689569d2071d32c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0ffe6ced14342acbc714c0eeb6ae230",
            "placeholder": "​",
            "style": "IPY_MODEL_30c7109ba9ac405eb5b4d71d741b6788",
            "value": " 327M/327M [05:12&lt;00:00, 777kB/s]"
          }
        },
        "1f338259d63a4d50a01ce56de3689470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64b1c13de1bd4cf6a124015030d9e8a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec16532fb924f2da1762a4e8574a39c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a8e37c333124813bb9ced06e6f1a572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5177c3540f94fed9764e873aedc5f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0ffe6ced14342acbc714c0eeb6ae230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c7109ba9ac405eb5b4d71d741b6788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29d95bc242354b6ca00788873e430242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e44b46b733347a5bc1179ab0a632fde",
              "IPY_MODEL_517a22b5f9d64494b2e29cc386e144f9",
              "IPY_MODEL_33bff7ddc3bb48aca9f18c201388e156"
            ],
            "layout": "IPY_MODEL_00ec248b03b54a80a8425c530b58ee5d"
          }
        },
        "7e44b46b733347a5bc1179ab0a632fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32476c4f2c9d4c4ea581cdf4f56a364c",
            "placeholder": "​",
            "style": "IPY_MODEL_b3ac500c6de243e2a9ef7544e18035de",
            "value": "Downloading: 100%"
          }
        },
        "517a22b5f9d64494b2e29cc386e144f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a024cbb858de490e930f0f4578ebc10f",
            "max": 1037,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43c5df61f3b94a75adaa236708cee726",
            "value": 1037
          }
        },
        "33bff7ddc3bb48aca9f18c201388e156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72f77bd4fc9245f994b9017ef91bf33c",
            "placeholder": "​",
            "style": "IPY_MODEL_a8578551940343bca212fb26491a9f9a",
            "value": " 1.01k/1.01k [00:00&lt;00:00, 34.9kB/s]"
          }
        },
        "00ec248b03b54a80a8425c530b58ee5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32476c4f2c9d4c4ea581cdf4f56a364c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ac500c6de243e2a9ef7544e18035de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a024cbb858de490e930f0f4578ebc10f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c5df61f3b94a75adaa236708cee726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72f77bd4fc9245f994b9017ef91bf33c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8578551940343bca212fb26491a9f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad323c5a2df6482eaff38f9f36920d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d91870991864d8798ee23a183425643",
              "IPY_MODEL_f6dfe841cda949a2bbee05ea7bd89544",
              "IPY_MODEL_30118304785f477185cb9d39c9c88f55"
            ],
            "layout": "IPY_MODEL_74cade9238294e649f46e042f0bfe914"
          }
        },
        "8d91870991864d8798ee23a183425643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9004464da654c0c9f0aaad3b3ce9407",
            "placeholder": "​",
            "style": "IPY_MODEL_513ec0625b41443b9d1beed0632c6aa8",
            "value": "Downloading: 100%"
          }
        },
        "f6dfe841cda949a2bbee05ea7bd89544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bc5e4e6f78e46a79858934686828005",
            "max": 302,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3b1f2b481a84c6f8ff66a8c12531d57",
            "value": 302
          }
        },
        "30118304785f477185cb9d39c9c88f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c9996f4ab414e6f8e15f0ea9f70545c",
            "placeholder": "​",
            "style": "IPY_MODEL_6b6136775070466eaac5cf12c50269f5",
            "value": " 302/302 [00:00&lt;00:00, 11.3kB/s]"
          }
        },
        "74cade9238294e649f46e042f0bfe914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9004464da654c0c9f0aaad3b3ce9407": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "513ec0625b41443b9d1beed0632c6aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bc5e4e6f78e46a79858934686828005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3b1f2b481a84c6f8ff66a8c12531d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c9996f4ab414e6f8e15f0ea9f70545c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b6136775070466eaac5cf12c50269f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1797a68603dd40c38097021569c0820d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8eee2f9618494f80aa86904c6fb57bf4",
              "IPY_MODEL_9f666b87dac1429ea00e93c0a6501031",
              "IPY_MODEL_c6f74c9e4fb44264a497a0d4395b3f95"
            ],
            "layout": "IPY_MODEL_de36e4ebbadd4e348c30169f5d239155"
          }
        },
        "8eee2f9618494f80aa86904c6fb57bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d68f22ada7df44a5871d6bb868c5649f",
            "placeholder": "​",
            "style": "IPY_MODEL_352cf01bb70a4e9185fd9c8586278abe",
            "value": "Downloading: 100%"
          }
        },
        "9f666b87dac1429ea00e93c0a6501031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e116e63ac1743cd8d417e4f8d0ff1a5",
            "max": 1212985603,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70be5e3e29a444fa954843466d5e118d",
            "value": 1212985603
          }
        },
        "c6f74c9e4fb44264a497a0d4395b3f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd807ca5eda940ad923aad4b0db26225",
            "placeholder": "​",
            "style": "IPY_MODEL_5326b71eb91b4f90bd18745da111eda6",
            "value": " 1.13G/1.13G [00:22&lt;00:00, 31.4MB/s]"
          }
        },
        "de36e4ebbadd4e348c30169f5d239155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d68f22ada7df44a5871d6bb868c5649f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "352cf01bb70a4e9185fd9c8586278abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e116e63ac1743cd8d417e4f8d0ff1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70be5e3e29a444fa954843466d5e118d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd807ca5eda940ad923aad4b0db26225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5326b71eb91b4f90bd18745da111eda6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8690ccda9018402bab9449db4d7f8c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95314d8fe0f043f587797d95ba53f612",
              "IPY_MODEL_14e1bbd731834736a1986a5dc74f4cd2",
              "IPY_MODEL_1229f1ece8ba42b69dc11d192ca015e5"
            ],
            "layout": "IPY_MODEL_7d51cac01c1945099109dd28db48921d"
          }
        },
        "95314d8fe0f043f587797d95ba53f612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_193f4dbdf1154c4aa21b279e8e378447",
            "placeholder": "​",
            "style": "IPY_MODEL_f34aa2ed9bd44b3683edcb292648c106",
            "value": "Download file pytorch_model.bin: 100%"
          }
        },
        "14e1bbd731834736a1986a5dc74f4cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca16367f9d8c486c8bc83c7d00476dd1",
            "max": 1213001023,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf7716f1b15a4cb0b421e82dcc8f4f46",
            "value": 1213001023
          }
        },
        "1229f1ece8ba42b69dc11d192ca015e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_075fbf09e39d4db2a400053525ae64ff",
            "placeholder": "​",
            "style": "IPY_MODEL_965408d02b8d4bb196347af84c49a560",
            "value": " 1.13G/1.13G [09:12&lt;00:00, 50.9kB/s]"
          }
        },
        "7d51cac01c1945099109dd28db48921d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "193f4dbdf1154c4aa21b279e8e378447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f34aa2ed9bd44b3683edcb292648c106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca16367f9d8c486c8bc83c7d00476dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf7716f1b15a4cb0b421e82dcc8f4f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "075fbf09e39d4db2a400053525ae64ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965408d02b8d4bb196347af84c49a560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52a3b5fa2a294d3b89cc1508ecb2a685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60532175cf6c45d8aeb8f250e3666607",
              "IPY_MODEL_f7dfd93e216441399e185d3faae40c91",
              "IPY_MODEL_4542d00dd14b4901bae6dc4c061d234d"
            ],
            "layout": "IPY_MODEL_6b5719806235443bbb77d19e9a364131"
          }
        },
        "60532175cf6c45d8aeb8f250e3666607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49857ce6bc2f48c6a05f7e87328f1f56",
            "placeholder": "​",
            "style": "IPY_MODEL_b76b757f3c33487aa732878762cfe902",
            "value": "Clean file pytorch_model.bin: 100%"
          }
        },
        "f7dfd93e216441399e185d3faae40c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_229502b251694f64bf2231da9772d532",
            "max": 1213001023,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b639e8579c5f4171a75e710e387f2419",
            "value": 1213001023
          }
        },
        "4542d00dd14b4901bae6dc4c061d234d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c1b943e8e8d428e98b1b33b45386a46",
            "placeholder": "​",
            "style": "IPY_MODEL_0206ae3f9c124aa4b86ca7f3ed7dce57",
            "value": " 1.13G/1.13G [02:36&lt;00:00, 7.46MB/s]"
          }
        },
        "6b5719806235443bbb77d19e9a364131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49857ce6bc2f48c6a05f7e87328f1f56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b76b757f3c33487aa732878762cfe902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "229502b251694f64bf2231da9772d532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b639e8579c5f4171a75e710e387f2419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c1b943e8e8d428e98b1b33b45386a46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0206ae3f9c124aa4b86ca7f3ed7dce57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b86a48bcfa3a4e3c962e80be83ffacdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5a8d36d8530481fa9c8b8d312ab03d3",
              "IPY_MODEL_c72740b9f8904bb1ad3b9cb9106fe0fc",
              "IPY_MODEL_6448384bfac742bba224a5fdc51d06f4"
            ],
            "layout": "IPY_MODEL_a457093836614f799fef1358859bdf52"
          }
        },
        "f5a8d36d8530481fa9c8b8d312ab03d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_964755e9afeb4c749bf5c748dedf7a4c",
            "placeholder": "​",
            "style": "IPY_MODEL_4e819f6b36894d2fa6b21a6f3dac95d6",
            "value": "Upload file pytorch_model.bin: 100%"
          }
        },
        "c72740b9f8904bb1ad3b9cb9106fe0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b218a731e8564103b8cec998c95fb878",
            "max": 1213001023,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32b808865cc64e5c984b5e554916e2ef",
            "value": 1213001023
          }
        },
        "6448384bfac742bba224a5fdc51d06f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_727a6b91402c4aa3bb92af62b976a49b",
            "placeholder": "​",
            "style": "IPY_MODEL_f1aaeac0045d45b2a32049e287558923",
            "value": " 1.13G/1.13G [19:03&lt;00:00, 752kB/s]"
          }
        },
        "a457093836614f799fef1358859bdf52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "964755e9afeb4c749bf5c748dedf7a4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e819f6b36894d2fa6b21a6f3dac95d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b218a731e8564103b8cec998c95fb878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32b808865cc64e5c984b5e554916e2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "727a6b91402c4aa3bb92af62b976a49b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1aaeac0045d45b2a32049e287558923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 確認 GPU 類型\n",
        "---"
      ],
      "metadata": {
        "id": "2ozr9xyv0ZMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  raise Exception(\"GPU not availalbe. CPU training will be too slow.\")\n",
        "print(\"device name\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "aDtXj7uU119F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355bd426-3983-492e-846b-9de6adcbb311"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device name Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 是否要掛載 Google Drive\n",
        "---"
      ],
      "metadata": {
        "id": "2kKsDcHu0hlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4aEAQyCz1950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fdc257-4344-4bd9-f70b-449fd9ff0feb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#確認 ＴＰＵ規格"
      ],
      "metadata": {
        "id": "kYFkur3sEb6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "ZCfuY-RMEctt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安裝 transformers,datastes,... 相依套件\n",
        "---"
      ],
      "metadata": {
        "id": "1O9n-3Ak0rik"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9T850192NHC"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install soundfile\n",
        "!pip install jiwer\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!apt install git-lfs\n",
        "!git config --global user.email \"gary109@gmail.com\"\n",
        "!git config --global user.name \"GARY\"\n",
        "!git config --global credential.helper store\n",
        "!pip install wandb\n",
        "!wandb login 2cf656515a3b158f4f603aeba63181236de2fc1b"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lsy1dPZl8ab",
        "outputId": "838471f5-49e5-45e1-b432-67f871842868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-bndwutfm\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-bndwutfm\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.12.0+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.25.11)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369387 sha256=7b4c8bcca55b466e88a74d0807f6b51958ee9bca67f072f3f09113a6f37d9595\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-umhfy376/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 登入 huggingface \n",
        "---"
      ],
      "metadata": {
        "id": "A1JcSRcJ0_uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605baf83-777b-4c84-de3e-c260b08d1f9b",
        "id": "yC1Cp6_e2U77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens.\n",
            "        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n",
            "        \n",
            "Token: \n",
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安裝加速器\n",
        "---"
      ],
      "metadata": {
        "id": "F3AZ8EIyzzir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install accelerate deepspeed"
      ],
      "metadata": {
        "id": "WsDxQ0683W6T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config"
      ],
      "metadata": {
        "id": "JJJEgLEi7OBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7997b5-e2e2-4366-dc0e-13d3eb892985"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0\n",
            "Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 3\n",
            "What is the name of the function in your script that should be launched in all parallel scripts? [main]: \n",
            "How many TPU cores should be used for distributed training? [1]:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate test"
      ],
      "metadata": {
        "id": "J1EszQTy7JTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 下載 orchid219_classification 程式碼\n",
        "--- "
      ],
      "metadata": {
        "id": "NhO8MiJ41HLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://gary109:Gygy844109109@gitlab.com/gary109/orchid219_classification.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfPGSNnqqdLE",
        "outputId": "008c7c29-8c85-4903-b2e6-80506e1a08a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'orchid219_classification'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 87 (delta 42), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (87/87), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd orchid219_classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYJiI9-u7qYs",
        "outputId": "4f94c4d6-1bd6-496e-95d8-e76ce23a53aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/orchid219_classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 載入 orchid219 訓練資料集\n",
        "---"
      ],
      "metadata": {
        "id": "RcO8DSXZz7W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"orchid219.py\", use_auth_token=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "ab172ea6d85f429ab907ab07015f04b6",
            "52768167b90d49998787469e578fb73f",
            "cb87ca8a12c24aaeb37fc69394f8bd25",
            "9b1eb15f1f444df5888631d0d78e82b4",
            "dcc9405d3d644b72810b9460b7e5bffe",
            "648d4d3da9434732a7a77f96c5caaa60",
            "0a6deb829a2548d09453afee9cf73524",
            "67d8df12242b4e2db31dd943cca4498f",
            "d387361e11fd471ba4d0bd8c21d50aeb",
            "5234078a06ce4b45b0a87367cffef658",
            "29f93ba6af5a41279c4ed06a6c5a85b1",
            "220593df1121490a9d985524a7817688",
            "48acc97e36ca499b8d0c531868b1991b",
            "d25eaab6ba07457c95b9a3be7ae7f3fd",
            "f289fd6f1bf14325b1750c5e2f181602",
            "7d9aef587ff74571b8fca8bdd12e1943",
            "07de66f00cb8455d9ad6fc8072b8c0b5",
            "f0f43722d8f54ac7a988e6f572fc1048",
            "2d2c4fab239d4d1bb1564e4f19329035",
            "bef834d139ec496d9177c8c4d8010c8d",
            "770f4fabf5cc40c292741246ade3f814",
            "cef1ae9f3e9f41c698127ce15ec87f7d",
            "2d40554d2e064ef6a4ca586c57d2c86f",
            "f071029ebb6a4fd399af1f69423ffc5d",
            "e06da8a8116246c59b18d7ffb662cdc2",
            "6e053877aca84410988286f5bf777317",
            "7a8828ae1f27436e8cca9023dc403d34",
            "a071fa1f1dc84fe4a1a48885aabd65ee",
            "df0734f49334478b9876fcae8e65784b",
            "d937c6be56c14c1abe58d5416daaed2d",
            "e3bef52cdfc84003bce10ffb354fb001",
            "346ec69d07cc45a2832dd5954940d5ea",
            "b35e6c369b2e4e2abb778db68022b3b7",
            "1a48e1b89a1e43d3b051d3d57945fbf1",
            "14c87b4451284e66b1570430de55f192",
            "5fcf3ae99de04c3c9636226f048fa640",
            "4f9dd6c30dca47c595da772ac2e64014",
            "f9c79ff8ae6b4259ac75711f5e10b248",
            "7c0de6eb1d2942e0959bb227549651b3",
            "540edbd69cdf4154ba3bf545d45b6f7e",
            "0e2a6e9938154c889f208f678de794e5",
            "3e7ce89fd8e1431c98f2a3d2783cf55e",
            "17099b1c2af241f3b012f9dfad020e66",
            "e665001844b745deab57e10f1d9d1006"
          ]
        },
        "outputId": "46d0e767-ec43-4833-b2fc-88d6f5581641",
        "id": "U_idZeBF2zie"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No config specified, defaulting to: orchid219/orchid219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset orchid219/orchid219 to /root/.cache/huggingface/datasets/orchid219/orchid219/1.0.0/8f8444a00f455cca182e267fafef70db843b3dd0d3ddb264f27c2accbf34d75e...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab172ea6d85f429ab907ab07015f04b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "220593df1121490a9d985524a7817688"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d40554d2e064ef6a4ca586c57d2c86f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset orchid219 downloaded and prepared to /root/.cache/huggingface/datasets/orchid219/orchid219/1.0.0/8f8444a00f455cca182e267fafef70db843b3dd0d3ddb264f27c2accbf34d75e. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a48e1b89a1e43d3b051d3d57945fbf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['filename', 'image', 'category'],\n",
              "        num_rows: 1971\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['filename', 'image', 'category'],\n",
              "        num_rows: 219\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 計算 ＩＭＡＧＥ's ＳＴＤ ＆ ＭＥＡＮ"
      ],
      "metadata": {
        "id": "4qU_1hTLWQn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os, argparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "orchid219_std = []\n",
        "orchid219_mean = []\n",
        "def delete_ipynb_checkpoints():\n",
        "    # delete all .ipynb_checkpoints dir\n",
        "    for filename in Path(os.getcwd()).glob('**/*.ipynb_checkpoints'):\n",
        "        try:\n",
        "            shutil.rmtree(filename)\n",
        "        except OSError as e:\n",
        "            print(e)\n",
        "        else: \n",
        "            print(\"The %s is deleted successfully\" % (filename))    \n",
        "\n",
        "def process_image_std_mean():\n",
        "    \n",
        "    delete_ipynb_checkpoints()\n",
        "    m_list, s_list = [], []\n",
        "    for c in dataset:\n",
        "        for ds in tqdm(dataset[c]):\n",
        "            img = cv2.imread(ds['filename'])\n",
        "            img = img / 255.0\n",
        "            m, s = cv2.meanStdDev(img)\n",
        "            m_list.append(m.reshape((3,)))\n",
        "            s_list.append(s.reshape((3,)))    \n",
        "          \n",
        "    m_array = np.array(m_list)\n",
        "    s_array = np.array(s_list)\n",
        "    m = m_array.mean(axis=0, keepdims=True)\n",
        "    s = s_array.mean(axis=0, keepdims=True)\n",
        "\n",
        "    # print(f\" mean:{m[0][::-1]} std:{s[0][::-1]} \")\n",
        "    return s[0][::-1], m[0][::-1]\n",
        "\n",
        "orchid219_std,orchid219_mean = process_image_std_mean()\n",
        "print(orchid219_std,orchid219_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6EN2IeEN_it",
        "outputId": "64a33822-5a31-45e2-b631-df0ad4cea0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1971/1971 [00:22<00:00, 88.65it/s]\n",
            "100%|██████████| 219/219 [00:02<00:00, 86.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.21403854 0.21571221 0.21655118] [0.48058045 0.42326896 0.36735169]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils.dummy_vision_objects import ImageGPTFeatureExtractor\n",
        "import random\n",
        "from PIL import ImageDraw, ImageFont, Image\n",
        "\n",
        "def show_examples(ds, seed: int = 1234, examples_per_class: int = 3, size=(100, 100)):\n",
        "\n",
        "    w, h = size\n",
        "    labels = ds['train'].features['category'].names\n",
        "    labels = labels[:9]\n",
        "    grid = Image.new('RGB', size=(examples_per_class * w, len(labels) * h))\n",
        "    draw = ImageDraw.Draw(grid)\n",
        "    font = ImageFont.truetype(\"./fonts/LiberationMono-Bold.ttf\", 24)\n",
        "    for label_id, label in enumerate(labels):\n",
        "\n",
        "        # Filter the dataset by a single label, shuffle it, and grab a few samples\n",
        "        ds_slice = ds['train'].filter(lambda ex: ex['category'] == label_id).shuffle(seed).select(range(examples_per_class))\n",
        "\n",
        "        # Plot this label's examples along a row\n",
        "        for i, example in enumerate(ds_slice):\n",
        "            image = example['image']\n",
        "            idx = examples_per_class * label_id + i\n",
        "            box = (idx % examples_per_class * w, idx // examples_per_class * h)\n",
        "            grid.paste(image.resize(size), box=box)\n",
        "            draw.text(box, str(label), (255, 255, 255), font=font)\n",
        "\n",
        "    return grid\n",
        "\n",
        "show_examples(dataset, seed=random.randint(0, 1337), examples_per_class=3)\n",
        "# dataset['train'][0]['image']"
      ],
      "metadata": {
        "id": "7iNPEbNz2zie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"gary109/orchid219\")"
      ],
      "metadata": {
        "id": "1gymd4g12zif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 產出 feature_extractor FOR Orchid129 ===> google/vit-base-patch16-224-in21k\n"
      ],
      "metadata": {
        "id": "WkwgQ3ZJd1Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoFeatureExtractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "        pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\",\n",
        "        feature_extractor_name=\"google/vit-base-patch16-224-in21k\",\n",
        "        use_auth_token=True,\n",
        "    )\n",
        "\n",
        "# mean:[0.48058045 0.42326896 0.36735169] std:[0.21403854 0.21571221 0.21655118] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsLjU4Y1Xzuc",
        "outputId": "97062a2a-8fbc-411e-e1b0-513eade535c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTFeatureExtractor {\n",
              "  \"do_normalize\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
              "  \"image_mean\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"image_std\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"resample\": 2,\n",
              "  \"size\": 224\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.image_mean = [0.48058045,0.42326896,0.36735169]\n",
        "feature_extractor.image_std = [0.21403854, 0.21571221, 0.21655118] "
      ],
      "metadata": {
        "id": "0pvzOiFwZP2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlHYAchlYovt",
        "outputId": "a892182f-c324-4861-cbf6-d85b16a1edc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTFeatureExtractor {\n",
              "  \"do_normalize\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
              "  \"image_mean\": [\n",
              "    0.48058045,\n",
              "    0.42326896,\n",
              "    0.36735169\n",
              "  ],\n",
              "  \"image_std\": [\n",
              "    0.21403854,\n",
              "    0.21571221,\n",
              "    0.21655118\n",
              "  ],\n",
              "  \"resample\": 2,\n",
              "  \"size\": 224\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.push_to_hub('gary109/orchid219_feature_extractor',use_auth_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "tgZHG132ZkRm",
        "outputId": "fa191d1b-a5f3-4b80-809f-c24cb9992f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_deprecation.py:43: FutureWarning: Pass token='orchid219_feature_extractor_vit-base-patch16-224-in21k' as keyword args. From version 0.7 passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py:676: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
            "  FutureWarning,\n",
            "Cloning https://huggingface.co/gary109/orchid219_feature_extractor_vit-base-patch16-224-in21k into local empty directory.\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_feature_extractor_vit-base-patch16-224-in21k\n",
            "   8c1b545..cc8872b  main -> main\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/gary109/orchid219_feature_extractor_vit-base-patch16-224-in21k/commit/cc8872b4873513c51281aa51489a86073d844210'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 產出 feature_extractor FOR Orchid129 ===> facebook/data2vec-vision-base\n"
      ],
      "metadata": {
        "id": "6hAF3zBoFjX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoFeatureExtractor,AutoModelForImageClassification,AutoConfig\n",
        "config = AutoConfig.from_pretrained(\"facebook/data2vec-vision-base\",)\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "        pretrained_model_name_or_path=\"facebook/data2vec-vision-base\",\n",
        "        feature_extractor_name=\"facebook/data2vec-vision-base\",\n",
        "        use_auth_token=True)\n",
        "model = AutoModelForImageClassification.from_pretrained(\"facebook/data2vec-vision-base\",use_auth_token=True)\n",
        "feature_extractor\n",
        "# mean:[0.48058045 0.42326896 0.36735169] std:[0.21403854 0.21571221 0.21655118] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2fcc83c-2678-4da4-b95a-739ff0ace081",
        "id": "6LwLVmWHFjYG"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Data2VecVisionForImageClassification were not initialized from the model checkpoint at facebook/data2vec-vision-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BeitFeatureExtractor {\n",
              "  \"crop_size\": 224,\n",
              "  \"do_center_crop\": false,\n",
              "  \"do_normalize\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"feature_extractor_type\": \"BeitFeatureExtractor\",\n",
              "  \"image_mean\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"image_std\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"reduce_labels\": false,\n",
              "  \"resample\": 3,\n",
              "  \"size\": 224\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.image_mean = [0.48058045,0.42326896,0.36735169]\n",
        "feature_extractor.image_std = [0.21403854, 0.21571221, 0.21655118] "
      ],
      "metadata": {
        "id": "T5IkkP6HFjYH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaec2156-76b6-4538-9911-6cef6197685b",
        "id": "6GK9qJjNFjYH"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BeitFeatureExtractor {\n",
              "  \"crop_size\": 224,\n",
              "  \"do_center_crop\": false,\n",
              "  \"do_normalize\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"feature_extractor_type\": \"BeitFeatureExtractor\",\n",
              "  \"image_mean\": [\n",
              "    0.48058045,\n",
              "    0.42326896,\n",
              "    0.36735169\n",
              "  ],\n",
              "  \"image_std\": [\n",
              "    0.21403854,\n",
              "    0.21571221,\n",
              "    0.21655118\n",
              "  ],\n",
              "  \"reduce_labels\": false,\n",
              "  \"resample\": 3,\n",
              "  \"size\": 224\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.push_to_hub('gary109/orchid219_data2vec-vision-base',use_auth_token=True)\n",
        "model.push_to_hub('gary109/orchid219_data2vec-vision-base',use_auth_token=True)\n",
        "feature_extractor.push_to_hub('gary109/orchid219_data2vec-vision-base',use_auth_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "5107725ab36c40c1bcb17243bb3cccb5",
            "2b011df545494a7b9d50db19c0b1dd63",
            "b2a596a16d3b422d8445b04fbf21f68d",
            "a4495e118357455689569d2071d32c83",
            "1f338259d63a4d50a01ce56de3689470",
            "64b1c13de1bd4cf6a124015030d9e8a2",
            "7ec16532fb924f2da1762a4e8574a39c",
            "8a8e37c333124813bb9ced06e6f1a572",
            "f5177c3540f94fed9764e873aedc5f60",
            "e0ffe6ced14342acbc714c0eeb6ae230",
            "30c7109ba9ac405eb5b4d71d741b6788"
          ]
        },
        "outputId": "5a313ae0-8f19-4627-f8f0-5efe83202d7f",
        "id": "VFxf51Y9FjYH"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload file pytorch_model.bin:   0%|          | 3.34k/327M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5107725ab36c40c1bcb17243bb3cccb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_data2vec-vision-base\n",
            "   eb27f63..a0eccdd  main -> main\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 產出 feature_extractor FOR Orchid129 ===> facebook/data2vec-vision-large\n"
      ],
      "metadata": {
        "id": "Q7MV70DBHbIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoFeatureExtractor,AutoModelForImageClassification,AutoConfig\n",
        "config = AutoConfig.from_pretrained(\"facebook/data2vec-vision-large\",)\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "        pretrained_model_name_or_path=\"facebook/data2vec-vision-large\",\n",
        "        feature_extractor_name=\"facebook/data2vec-vision-large\",\n",
        "        use_auth_token=True)\n",
        "model = AutoModelForImageClassification.from_pretrained(\"facebook/data2vec-vision-large\",use_auth_token=True)\n",
        "feature_extractor\n",
        "# mean:[0.48058045 0.42326896 0.36735169] std:[0.21403854 0.21571221 0.21655118] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "29d95bc242354b6ca00788873e430242",
            "7e44b46b733347a5bc1179ab0a632fde",
            "517a22b5f9d64494b2e29cc386e144f9",
            "33bff7ddc3bb48aca9f18c201388e156",
            "00ec248b03b54a80a8425c530b58ee5d",
            "32476c4f2c9d4c4ea581cdf4f56a364c",
            "b3ac500c6de243e2a9ef7544e18035de",
            "a024cbb858de490e930f0f4578ebc10f",
            "43c5df61f3b94a75adaa236708cee726",
            "72f77bd4fc9245f994b9017ef91bf33c",
            "a8578551940343bca212fb26491a9f9a",
            "ad323c5a2df6482eaff38f9f36920d83",
            "8d91870991864d8798ee23a183425643",
            "f6dfe841cda949a2bbee05ea7bd89544",
            "30118304785f477185cb9d39c9c88f55",
            "74cade9238294e649f46e042f0bfe914",
            "d9004464da654c0c9f0aaad3b3ce9407",
            "513ec0625b41443b9d1beed0632c6aa8",
            "2bc5e4e6f78e46a79858934686828005",
            "a3b1f2b481a84c6f8ff66a8c12531d57",
            "5c9996f4ab414e6f8e15f0ea9f70545c",
            "6b6136775070466eaac5cf12c50269f5",
            "1797a68603dd40c38097021569c0820d",
            "8eee2f9618494f80aa86904c6fb57bf4",
            "9f666b87dac1429ea00e93c0a6501031",
            "c6f74c9e4fb44264a497a0d4395b3f95",
            "de36e4ebbadd4e348c30169f5d239155",
            "d68f22ada7df44a5871d6bb868c5649f",
            "352cf01bb70a4e9185fd9c8586278abe",
            "7e116e63ac1743cd8d417e4f8d0ff1a5",
            "70be5e3e29a444fa954843466d5e118d",
            "cd807ca5eda940ad923aad4b0db26225",
            "5326b71eb91b4f90bd18745da111eda6"
          ]
        },
        "outputId": "ad60f527-5d56-44ec-ebaf-9355fa3e80c6",
        "id": "YGafJNUDHbI7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29d95bc242354b6ca00788873e430242"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/302 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad323c5a2df6482eaff38f9f36920d83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1797a68603dd40c38097021569c0820d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Data2VecVisionForImageClassification were not initialized from the model checkpoint at facebook/data2vec-vision-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BeitFeatureExtractor {\n",
              "  \"crop_size\": 224,\n",
              "  \"do_center_crop\": false,\n",
              "  \"do_normalize\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"feature_extractor_type\": \"BeitFeatureExtractor\",\n",
              "  \"image_mean\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"image_std\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"reduce_labels\": false,\n",
              "  \"resample\": 3,\n",
              "  \"size\": 224\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor.image_mean = [0.48058045,0.42326896,0.36735169]\n",
        "feature_extractor.image_std = [0.21403854, 0.21571221, 0.21655118] "
      ],
      "metadata": {
        "id": "KfQE7sVmHbI7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53b21eb-b6dc-4d3f-f989-ebca9f6e3ca2",
        "id": "C4JhmG6zHbI7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BeitFeatureExtractor {\n",
              "  \"crop_size\": 224,\n",
              "  \"do_center_crop\": false,\n",
              "  \"do_normalize\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"feature_extractor_type\": \"BeitFeatureExtractor\",\n",
              "  \"image_mean\": [\n",
              "    0.48058045,\n",
              "    0.42326896,\n",
              "    0.36735169\n",
              "  ],\n",
              "  \"image_std\": [\n",
              "    0.21403854,\n",
              "    0.21571221,\n",
              "    0.21655118\n",
              "  ],\n",
              "  \"reduce_labels\": false,\n",
              "  \"resample\": 3,\n",
              "  \"size\": 224\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.push_to_hub('gary109/orchid219_data2vec-vision-large',use_auth_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "8690ccda9018402bab9449db4d7f8c09",
            "95314d8fe0f043f587797d95ba53f612",
            "14e1bbd731834736a1986a5dc74f4cd2",
            "1229f1ece8ba42b69dc11d192ca015e5",
            "7d51cac01c1945099109dd28db48921d",
            "193f4dbdf1154c4aa21b279e8e378447",
            "f34aa2ed9bd44b3683edcb292648c106",
            "ca16367f9d8c486c8bc83c7d00476dd1",
            "bf7716f1b15a4cb0b421e82dcc8f4f46",
            "075fbf09e39d4db2a400053525ae64ff",
            "965408d02b8d4bb196347af84c49a560",
            "52a3b5fa2a294d3b89cc1508ecb2a685",
            "60532175cf6c45d8aeb8f250e3666607",
            "f7dfd93e216441399e185d3faae40c91",
            "4542d00dd14b4901bae6dc4c061d234d",
            "6b5719806235443bbb77d19e9a364131",
            "49857ce6bc2f48c6a05f7e87328f1f56",
            "b76b757f3c33487aa732878762cfe902",
            "229502b251694f64bf2231da9772d532",
            "b639e8579c5f4171a75e710e387f2419",
            "5c1b943e8e8d428e98b1b33b45386a46",
            "0206ae3f9c124aa4b86ca7f3ed7dce57"
          ]
        },
        "id": "xkaSfhzN6KLn",
        "outputId": "165585e6-09bb-44fa-80ec-377a2f793ae1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_deprecation.py:43: FutureWarning: Pass token='orchid219_data2vec-vision-large' as keyword args. From version 0.7 passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py:676: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
            "  FutureWarning,\n",
            "Cloning https://huggingface.co/gary109/orchid219_data2vec-vision-large into local empty directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Download file pytorch_model.bin:   0%|          | 2.01k/1.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8690ccda9018402bab9449db4d7f8c09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Clean file pytorch_model.bin:   0%|          | 1.00k/1.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52a3b5fa2a294d3b89cc1508ecb2a685"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_data2vec-vision-large\n",
            "   a9e2135..72862eb  main -> main\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/gary109/orchid219_data2vec-vision-large/commit/72862ebd5dc6c3419cd91a598617b9229a0beb5f'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub('gary109/orchid219_data2vec-vision-large',use_auth_token=True)\n",
        "feature_extractor.push_to_hub('gary109/orchid219_data2vec-vision-large',use_auth_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "b86a48bcfa3a4e3c962e80be83ffacdb",
            "f5a8d36d8530481fa9c8b8d312ab03d3",
            "c72740b9f8904bb1ad3b9cb9106fe0fc",
            "6448384bfac742bba224a5fdc51d06f4",
            "a457093836614f799fef1358859bdf52",
            "964755e9afeb4c749bf5c748dedf7a4c",
            "4e819f6b36894d2fa6b21a6f3dac95d6",
            "b218a731e8564103b8cec998c95fb878",
            "32b808865cc64e5c984b5e554916e2ef",
            "727a6b91402c4aa3bb92af62b976a49b",
            "f1aaeac0045d45b2a32049e287558923"
          ]
        },
        "outputId": "8a3b3776-564d-46ff-80f6-67b37e4f2054",
        "id": "WoWPHLgSHbI7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_deprecation.py:43: FutureWarning: Pass token='orchid219_data2vec-vision-large' as keyword args. From version 0.7 passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py:676: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
            "  FutureWarning,\n",
            "Cloning https://huggingface.co/gary109/orchid219_data2vec-vision-large into local empty directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload file pytorch_model.bin:   0%|          | 3.33k/1.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b86a48bcfa3a4e3c962e80be83ffacdb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_data2vec-vision-large\n",
            "   7029e10..46708a8  main -> main\n",
            "\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_data2vec-vision-large\n",
            "   46708a8..a9e2135  main -> main\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/gary109/orchid219_data2vec-vision-large/commit/a9e2135dd5adeccba2657b4a32a073b336e5fb77'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FOR TPU needs\n",
        "---"
      ],
      "metadata": {
        "id": "5lYnSRfsvhd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip uninstall -y torch\n",
        "!pip install torch==1.8.2+cpu torchvision==0.9.2+cpu -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "94vK4zQPvfGQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 開始訓練\n",
        "---"
      ],
      "metadata": {
        "id": "GBQ4LLFm2CJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tune Pre-Trained Model\n",
        "---\n",
        "- google\n",
        "    - google/vit-base-patch16-224-in21k\n",
        "    - google/vit-base-patch32-224-in21k\n",
        "    - google/vit-large-patch16-224-in21k\n",
        "    - google/vit-large-patch32-224-in21k\n",
        "    - google/vit-huge-patch14-224-in21k\n",
        "---\n",
        "- gary109\n",
        "    - gary109/orchid219_pretrain_vit-mae-base\n",
        "    - gary109/orchid219_pretrain_vit-mae-large\n",
        "    - gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae\n",
        "    - gary109/orchid219_pretrain_data2vec-vision-base-mae\n",
        "    - gary109/orchid219_pretrain_vit-mae-large\n",
        "    - gary109/orchid219_pretrain_vit-base-mim\n",
        "---\n"
      ],
      "metadata": {
        "id": "msyHAoNs_gbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-base-patch16-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "MzvQ9zfD34bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path=\"google/vit-base-patch16-224-in21k\" \\\n",
        "    --feature_extractor_name='gary109/orchid219_feature_extractor' \\\n",
        "    --output_dir=\"./orchid219_ft_vit-base-patch16-224-in21k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id orchid219_vit-base-patch16-224-in21k \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \\\n",
        "    --cache_dir=\"./cache_orchid219_vit-base-patch16-224-in21k\"\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "tNMZWv7q0G02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-base-patch32-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "p9t54nRI4B5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"orchid219\" \\\n",
        "    --model_name_or_path \"google/vit-base-patch32-224-in21k\" \\\n",
        "    --output_dir ./orchid219_vit-base-patch32-224-in21k/ \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id orchid219_vit-base-patch32-224-in21k \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --per_device_train_batch_size 80 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "XTWxM4p30G3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-large-patch16-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "NNvuAbjI4PWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"orchid219\" \\\n",
        "    --model_name_or_path \"google/vit-large-patch16-224-in21k\" \\\n",
        "    --output_dir \"./orchid219_vit-large-patch16-224-in21k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id \"orchid219_vit-large-patch16-224-in21k\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "Xo5w2JgH0G6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-large-patch32-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "WJsYvA7u4dLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"google/vit-large-patch32-224-in21k\" \\\n",
        "    --output_dir \"./orchid219_vit-large-patch32-224-in21k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id \"orchid219_vit-large-patch32-224-in21k\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "c13vuvEg4dlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/vit-huge-patch14-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "qaNA2JF34UPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"google/vit-huge-patch14-224-in21k\" \\\n",
        "    --output_dir \"./orchid219_vit-huge-patch14-224-in21k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id \"orchid219_vit-huge-patch14-224-in21k\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 5 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --gradient_checkpointing"
      ],
      "metadata": {
        "id": "Gd5cMkQ9kBjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gary109/orchid219_pretrain_vit-mae-large\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Wnmnj8KB9Whn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification_ViT-MAE.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"gary109/orchid219_pretrain_vit-mae-large\" \\\n",
        "    --output_dir=\"./orchid219_vit-mae-large_ft/\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_vit-mae-large\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 100 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \n",
        "    # --cache_dir=\"./cache_test/\"\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing\n",
        "\n",
        "# --model_name_or_path \"gary109/orchid219_pretrain_vit-base-patch16-224-in21k-mae\" \\\n",
        "    #orchid219_pretrain_vit-base-patch16-224-in21k-mae\n",
        "    # --model_name_or_path \"gary109/orchid219_vit-base-patch16-224-in21k\" "
      ],
      "metadata": {
        "id": "9Q26wxpY9aic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gary109/orchid219_ft_pretrain_vit-base-mim\n",
        "---"
      ],
      "metadata": {
        "id": "Xo9M0Z8J6_8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification.py \\\n",
        "    --model_name_or_path \"gary109/orchid219_pretrain_vit-base-mim\" \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --output_dir=\"orchid219_ft_pretrain_vit-base-mim/\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_ft_pretrain_vit-base-mim\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 400 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token=\"True\" \\\n",
        "    --seed 1337 \n",
        "\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "hNIW2P2B6-9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [暫時未成功] openai/clip-vit-base-patch16\n",
        "---"
      ],
      "metadata": {
        "id": "ANg5qKaDeNKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification.py \\\n",
        "    --model_name_or_path \"google/vit-base-patch16-224-in21k\" \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --output_dir=\"orchid219_ft_clip-vit-base-patch16-224-in21k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_ft_clip-vit-base-patch16-224-in21k\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 400 \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token=\"True\" \\\n",
        "    --seed 1337 \n",
        "\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "puWYXXTXegoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gary109/orchid219_ft_data2vec-vision-base ===> facebook/data2vec-vision-base\n",
        "---"
      ],
      "metadata": {
        "id": "Zl3ycpATNA-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"gary109/orchid219_data2vec-vision-base\" \\\n",
        "    --output_dir=\"orchid219_ft_data2vec-vision-base\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train --do_eval --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_ft_data2vec-vision-base\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 50 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --use_auth_token=\"True\" \\\n",
        "    --seed 1337 \n",
        "\n",
        "# --gradient_accumulation_steps 8 \\\n",
        "# --gradient_checkpointing"
      ],
      "metadata": {
        "id": "HYRn6_6PNAaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gary109/orchid219_ft_pretrain_data2vec-vision-base-mae \n",
        "---"
      ],
      "metadata": {
        "id": "vZORz99kJYSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification_Data2Vec-MAE.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"gary109/orchid219_pretrain_data2vec-vision-base-mae\" \\\n",
        "    --output_dir=\"orchid219_ft_pretrain_data2vec-vision-base-mae\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train --do_eval --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_ft_pretrain_data2vec-vision-base-mae\" \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --num_train_epochs 200 \\\n",
        "    --per_device_train_batch_size 32 \\\n",
        "    --per_device_eval_batch_size 32 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token=\"True\" \\\n",
        "    --seed 1337 \n",
        "\n",
        "# --gradient_accumulation_steps 8 \\\n",
        "# --gradient_checkpointing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEIAf4sXJYS2",
        "outputId": "ed0c9841-c3c6-471a-aba6-f060bcf82a65"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m串流輸出內容已截斷至最後 5000 行。\u001b[0m\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:19:48,969 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1302\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:19:48,973 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1302/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:19:50,647 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1302/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:19:50,648 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1302/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:19:54,268 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1116] due to args.save_total_limit\n",
            " 11% 1310/12400 [12:47<2:06:01,  1.47it/s]{'loss': 1.1568, 'learning_rate': 0.0001788709677419355, 'epoch': 21.13}\n",
            "                                          {'loss': 1.2107, 'learning_rate': 0.00017870967741935484, 'epoch': 21.29}\n",
            " 11% 1330/12400 [12:56<1:15:16,  2.45it/s]{'loss': 1.2084, 'learning_rate': 0.0001785483870967742, 'epoch': 21.45}\n",
            " 11% 1340/12400 [13:00<1:16:07,  2.42it/s]{'loss': 1.3308, 'learning_rate': 0.00017838709677419357, 'epoch': 21.61}\n",
            "{'loss': 1.3346, 'learning_rate': 0.0001782258064516129, 'epoch': 21.77}\n",
            " 11% 1360/12400 [13:07<1:09:04,  2.66it/s]{'loss': 1.0957, 'learning_rate': 0.00017806451612903228, 'epoch': 21.94}\n",
            " 11% 1364/12400 [13:09<1:07:32,  2.72it/s][INFO|trainer.py:2625] 2022-05-19 04:20:21,326 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:20:21,326 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:20:21,326 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.67it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.40it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.33it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.37it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.38it/s]\u001b[A\n",
            "                                          \n",
            " 11% 1364/12400 [13:12<1:07:32,  2.72it/s]\n",
            "100% 7/7 [00:00<00:00,  8.68it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:20:24,332 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1364\n",
            "{'eval_loss': 2.2302768230438232, 'eval_accuracy': 0.3972602739726027, 'eval_runtime': 3.0041, 'eval_samples_per_second': 72.9, 'eval_steps_per_second': 2.33, 'epoch': 22.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:20:24,336 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1364/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:20:26,099 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1364/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:20:26,100 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1364/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:20:30,055 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1240] due to args.save_total_limit\n",
            " 11% 1370/12400 [13:22<2:57:14,  1.04it/s]{'loss': 1.183, 'learning_rate': 0.00017790322580645162, 'epoch': 22.1}\n",
            " 11% 1380/12400 [13:27<1:20:35,  2.28it/s]{'loss': 1.1227, 'learning_rate': 0.00017774193548387098, 'epoch': 22.26}\n",
            " 11% 1390/12400 [13:31<1:12:21,  2.54it/s]{'loss': 1.0866, 'learning_rate': 0.00017758064516129035, 'epoch': 22.42}\n",
            "{'loss': 1.1278, 'learning_rate': 0.0001774193548387097, 'epoch': 22.58}\n",
            " 11% 1410/12400 [13:38<1:14:29,  2.46it/s]{'loss': 1.3818, 'learning_rate': 0.00017725806451612903, 'epoch': 22.74}\n",
            "{'loss': 1.288, 'learning_rate': 0.0001770967741935484, 'epoch': 22.9}\n",
            " 12% 1426/12400 [13:44<1:04:16,  2.85it/s][INFO|trainer.py:2625] 2022-05-19 04:20:56,872 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:20:56,873 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:20:56,873 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.71it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.53it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.36it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.217120409011841, 'eval_accuracy': 0.4200913242009132, 'eval_runtime': 3.0128, 'eval_samples_per_second': 72.689, 'eval_steps_per_second': 2.323, 'epoch': 23.0}\n",
            " 12% 1426/12400 [13:47<1:04:16,  2.85it/s]\n",
            "100% 7/7 [00:00<00:00,  8.66it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:20:59,888 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1426\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:20:59,891 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1426/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:21:01,567 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1426/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:21:01,568 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1426/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:21:05,369 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1302] due to args.save_total_limit\n",
            "{'loss': 1.2053, 'learning_rate': 0.00017693548387096773, 'epoch': 23.06}\n",
            " 12% 1440/12400 [14:01<1:24:39,  2.16it/s]{'loss': 0.9652, 'learning_rate': 0.0001767741935483871, 'epoch': 23.23}\n",
            " 12% 1450/12400 [14:05<1:13:05,  2.50it/s]{'loss': 1.1585, 'learning_rate': 0.00017661290322580647, 'epoch': 23.39}\n",
            "{'loss': 1.0897, 'learning_rate': 0.0001764516129032258, 'epoch': 23.55}\n",
            " 12% 1470/12400 [14:13<1:13:07,  2.49it/s]{'loss': 1.2674, 'learning_rate': 0.00017629032258064517, 'epoch': 23.71}\n",
            "{'loss': 1.0633, 'learning_rate': 0.0001761290322580645, 'epoch': 23.87}\n",
            " 12% 1488/12400 [14:19<1:04:30,  2.82it/s][INFO|trainer.py:2625] 2022-05-19 04:21:32,081 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:21:32,082 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:21:32,082 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.20it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.26it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.32it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            "                                          {'eval_loss': 2.221461772918701, 'eval_accuracy': 0.4657534246575342, 'eval_runtime': 3.0418, 'eval_samples_per_second': 71.997, 'eval_steps_per_second': 2.301, 'epoch': 24.0}\n",
            "\n",
            " 12% 1488/12400 [14:23<1:04:30,  2.82it/s]\n",
            "100% 7/7 [00:00<00:00,  8.57it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:21:35,125 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1488\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:21:35,130 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1488/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:21:36,780 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1488/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:21:36,781 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1488/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:21:40,434 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1364] due to args.save_total_limit\n",
            "{'loss': 1.1617, 'learning_rate': 0.00017596774193548388, 'epoch': 24.03}\n",
            "{'loss': 0.912, 'learning_rate': 0.00017580645161290325, 'epoch': 24.19}\n",
            "                                          {'loss': 1.0777, 'learning_rate': 0.00017564516129032259, 'epoch': 24.35}\n",
            "                                          {'loss': 1.136, 'learning_rate': 0.00017548387096774195, 'epoch': 24.52}\n",
            "{'loss': 1.1416, 'learning_rate': 0.0001753225806451613, 'epoch': 24.68}\n",
            "{'loss': 1.2061, 'learning_rate': 0.00017516129032258066, 'epoch': 24.84}\n",
            "                                          {'loss': 0.9971, 'learning_rate': 0.000175, 'epoch': 25.0}\n",
            " 12% 1550/12400 [14:56<1:06:20,  2.73it/s][INFO|trainer.py:2625] 2022-05-19 04:22:08,036 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:22:08,036 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:22:08,036 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  9.49it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.97it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.81it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.71it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.62it/s]\u001b[A\n",
            "                                          \n",
            " 12% 1550/12400 [14:59<1:06:20,  2.73it/s]\n",
            "100% 7/7 [00:00<00:00,  8.88it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.048746347427368, 'eval_accuracy': 0.4429223744292237, 'eval_runtime': 3.0027, 'eval_samples_per_second': 72.935, 'eval_steps_per_second': 2.331, 'epoch': 25.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 04:22:11,041 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1550\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:22:11,044 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1550/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:22:12,677 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1550/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:22:12,677 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1550/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:22:16,156 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1178] due to args.save_total_limit\n",
            " 13% 1560/12400 [15:10<1:40:15,  1.80it/s]{'loss': 1.0244, 'learning_rate': 0.00017483870967741936, 'epoch': 25.16}\n",
            "{'loss': 0.9576, 'learning_rate': 0.00017467741935483873, 'epoch': 25.32}\n",
            "{'loss': 0.9713, 'learning_rate': 0.00017451612903225807, 'epoch': 25.48}\n",
            " 13% 1590/12400 [15:23<1:13:50,  2.44it/s]{'loss': 1.1033, 'learning_rate': 0.00017435483870967744, 'epoch': 25.65}\n",
            " 13% 1600/12400 [15:26<1:10:00,  2.57it/s]{'loss': 1.0871, 'learning_rate': 0.00017419354838709678, 'epoch': 25.81}\n",
            "                                          {'loss': 1.0852, 'learning_rate': 0.00017403225806451612, 'epoch': 25.97}\n",
            " 13% 1612/12400 [15:31<1:05:41,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 04:22:43,351 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:22:43,351 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:22:43,351 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.83it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.59it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.51it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.42it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.44it/s]\u001b[A\n",
            "                                          \n",
            " 13% 1612/12400 [15:34<1:05:41,  2.74it/s]\n",
            "100% 7/7 [00:00<00:00,  8.79it/s]\u001b[A{'eval_loss': 2.2428436279296875, 'eval_accuracy': 0.45662100456621, 'eval_runtime': 3.0001, 'eval_samples_per_second': 72.998, 'eval_steps_per_second': 2.333, 'epoch': 26.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:22:46,353 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1612\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:22:46,356 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1612/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:22:47,978 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1612/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:22:47,979 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1612/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:22:51,439 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1426] due to args.save_total_limit\n",
            " 13% 1620/12400 [15:44<2:01:06,  1.48it/s]{'loss': 0.8997, 'learning_rate': 0.0001738709677419355, 'epoch': 26.13}\n",
            "{'loss': 0.83, 'learning_rate': 0.00017370967741935485, 'epoch': 26.29}\n",
            "{'loss': 0.9558, 'learning_rate': 0.00017354838709677422, 'epoch': 26.45}\n",
            "                                          {'loss': 1.0175, 'learning_rate': 0.00017338709677419356, 'epoch': 26.61}\n",
            "                                          {'loss': 1.0876, 'learning_rate': 0.0001732258064516129, 'epoch': 26.77}\n",
            "                                          {'loss': 1.1024, 'learning_rate': 0.00017306451612903226, 'epoch': 26.94}\n",
            " 14% 1674/12400 [16:06<1:07:05,  2.66it/s][INFO|trainer.py:2625] 2022-05-19 04:23:18,773 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:23:18,774 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:23:18,774 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  9.00it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.72it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.53it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.53it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.51it/s]\u001b[A\n",
            "                                          \n",
            " 14% 1674/12400 [16:09<1:07:05,  2.66it/s]\n",
            "100% 7/7 [00:00<00:00,  8.83it/s]{'eval_loss': 2.2839748859405518, 'eval_accuracy': 0.4429223744292237, 'eval_runtime': 3.0358, 'eval_samples_per_second': 72.138, 'eval_steps_per_second': 2.306, 'epoch': 27.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:23:21,811 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1674\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:23:21,814 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1674/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:23:23,454 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1674/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:23:23,455 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1674/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:23:27,137 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1488] due to args.save_total_limit\n",
            " 14% 1680/12400 [16:19<2:49:44,  1.05it/s]{'loss': 1.0007, 'learning_rate': 0.00017290322580645163, 'epoch': 27.1}\n",
            "{'loss': 0.8228, 'learning_rate': 0.000172741935483871, 'epoch': 27.26}\n",
            " 14% 1700/12400 [16:28<1:13:48,  2.42it/s]{'loss': 1.0115, 'learning_rate': 0.00017258064516129034, 'epoch': 27.42}\n",
            " 14% 1710/12400 [16:32<1:12:16,  2.47it/s]{'loss': 0.9921, 'learning_rate': 0.00017241935483870967, 'epoch': 27.58}\n",
            " 14% 1720/12400 [16:36<1:13:24,  2.42it/s]{'loss': 0.958, 'learning_rate': 0.00017225806451612904, 'epoch': 27.74}\n",
            "{'loss': 1.012, 'learning_rate': 0.00017209677419354838, 'epoch': 27.9}\n",
            " 14% 1736/12400 [16:42<1:03:09,  2.81it/s][INFO|trainer.py:2625] 2022-05-19 04:23:54,382 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:23:54,383 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:23:54,383 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.70it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.52it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.51it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.38it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.1984341144561768, 'eval_accuracy': 0.4611872146118721, 'eval_runtime': 3.0193, 'eval_samples_per_second': 72.534, 'eval_steps_per_second': 2.318, 'epoch': 28.0}\n",
            " 14% 1736/12400 [16:45<1:03:09,  2.81it/s]\n",
            "100% 7/7 [00:00<00:00,  8.67it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:23:57,404 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1736\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:23:57,407 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1736/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:23:59,045 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1736/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:23:59,045 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1736/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:24:02,696 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1612] due to args.save_total_limit\n",
            " 14% 1740/12400 [16:54<4:20:33,  1.47s/it]{'loss': 0.9617, 'learning_rate': 0.00017193548387096775, 'epoch': 28.06}\n",
            " 14% 1750/12400 [16:58<1:22:00,  2.16it/s]{'loss': 0.9256, 'learning_rate': 0.00017177419354838711, 'epoch': 28.23}\n",
            " 14% 1760/12400 [17:03<1:13:38,  2.41it/s]{'loss': 0.8888, 'learning_rate': 0.00017161290322580645, 'epoch': 28.39}\n",
            "                                          {'loss': 0.8914, 'learning_rate': 0.00017145161290322582, 'epoch': 28.55}\n",
            "{'loss': 0.9037, 'learning_rate': 0.00017129032258064516, 'epoch': 28.71}\n",
            "                                          {'loss': 1.0316, 'learning_rate': 0.00017112903225806453, 'epoch': 28.87}\n",
            " 14% 1798/12400 [17:17<1:01:36,  2.87it/s][INFO|trainer.py:2625] 2022-05-19 04:24:29,872 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:24:29,872 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:24:29,872 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.71it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.55it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.46it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.39it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.36it/s]\u001b[A\n",
            "                                          \n",
            " 14% 1798/12400 [17:20<1:01:36,  2.87it/s]\n",
            "{'eval_loss': 2.277151346206665, 'eval_accuracy': 0.4337899543378995, 'eval_runtime': 3.0143, 'eval_samples_per_second': 72.655, 'eval_steps_per_second': 2.322, 'epoch': 29.0}\n",
            "100% 7/7 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:24:32,888 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1798\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:24:32,891 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1798/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:24:34,547 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1798/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:24:34,550 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1798/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:24:38,138 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1674] due to args.save_total_limit\n",
            "{'loss': 0.8202, 'learning_rate': 0.0001709677419354839, 'epoch': 29.03}\n",
            "{'loss': 0.7397, 'learning_rate': 0.00017080645161290323, 'epoch': 29.19}\n",
            " 15% 1820/12400 [17:37<1:12:16,  2.44it/s]{'loss': 0.937, 'learning_rate': 0.0001706451612903226, 'epoch': 29.35}\n",
            "{'loss': 0.7802, 'learning_rate': 0.00017048387096774194, 'epoch': 29.52}\n",
            "{'loss': 0.95, 'learning_rate': 0.00017032258064516128, 'epoch': 29.68}\n",
            " 15% 1850/12400 [17:49<1:08:50,  2.55it/s]{'loss': 0.9325, 'learning_rate': 0.00017016129032258065, 'epoch': 29.84}\n",
            " 15% 1860/12400 [17:52<1:03:27,  2.77it/s]{'loss': 1.0084, 'learning_rate': 0.00017, 'epoch': 30.0}\n",
            " 15% 1860/12400 [17:52<1:03:27,  2.77it/s][INFO|trainer.py:2625] 2022-05-19 04:25:04,883 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:25:04,883 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:25:04,883 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.86it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.67it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.58it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.50it/s]\u001b[A\n",
            "                                          \n",
            " 15% 1860/12400 [17:56<1:03:27,  2.77it/s]\n",
            "100% 7/7 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.2640960216522217, 'eval_accuracy': 0.4611872146118721, 'eval_runtime': 3.0911, 'eval_samples_per_second': 70.849, 'eval_steps_per_second': 2.265, 'epoch': 30.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 04:25:07,976 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1860\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:25:07,980 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1860/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:25:09,612 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1860/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:25:09,613 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1860/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:25:13,226 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1736] due to args.save_total_limit\n",
            "{'loss': 0.7131, 'learning_rate': 0.00016983870967741938, 'epoch': 30.16}\n",
            "{'loss': 0.832, 'learning_rate': 0.00016967741935483872, 'epoch': 30.32}\n",
            "{'loss': 0.8533, 'learning_rate': 0.00016951612903225806, 'epoch': 30.48}\n",
            "                                          {'loss': 0.9498, 'learning_rate': 0.00016935483870967742, 'epoch': 30.65}\n",
            "                                          {'loss': 0.8811, 'learning_rate': 0.0001691935483870968, 'epoch': 30.81}\n",
            " 15% 1920/12400 [18:27<1:01:11,  2.85it/s]{'loss': 0.88, 'learning_rate': 0.00016903225806451616, 'epoch': 30.97}\n",
            " 16% 1922/12400 [18:28<1:02:19,  2.80it/s][INFO|trainer.py:2625] 2022-05-19 04:25:40,411 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:25:40,411 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:25:40,411 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.91it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.03it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.15it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.23it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.30it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.24482798576355, 'eval_accuracy': 0.4337899543378995, 'eval_runtime': 3.0729, 'eval_samples_per_second': 71.268, 'eval_steps_per_second': 2.278, 'epoch': 31.0}\n",
            " 16% 1922/12400 [18:31<1:02:19,  2.80it/s]\n",
            "100% 7/7 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:25:43,486 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1922\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:25:43,489 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1922/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:25:45,130 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1922/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:25:45,131 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1922/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:25:48,771 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1798] due to args.save_total_limit\n",
            " 16% 1930/12400 [18:42<1:59:30,  1.46it/s]{'loss': 0.7944, 'learning_rate': 0.0001688709677419355, 'epoch': 31.13}\n",
            " 16% 1940/12400 [18:46<1:15:45,  2.30it/s]{'loss': 0.6553, 'learning_rate': 0.00016870967741935484, 'epoch': 31.29}\n",
            "                                          {'loss': 0.8374, 'learning_rate': 0.0001685483870967742, 'epoch': 31.45}\n",
            " 16% 1960/12400 [18:54<1:11:06,  2.45it/s]{'loss': 0.6967, 'learning_rate': 0.00016838709677419354, 'epoch': 31.61}\n",
            "{'loss': 0.8849, 'learning_rate': 0.0001682258064516129, 'epoch': 31.77}\n",
            "{'loss': 0.9075, 'learning_rate': 0.00016806451612903228, 'epoch': 31.94}\n",
            " 16% 1984/12400 [19:03<1:03:20,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 04:26:16,082 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:26:16,083 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:26:16,083 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.51it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.80it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.00it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.10it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            "                                          \n",
            " 16% 1984/12400 [19:07<1:03:20,  2.74it/s]\n",
            "100% 7/7 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:26:19,203 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1984\n",
            "{'eval_loss': 2.309948682785034, 'eval_accuracy': 0.4657534246575342, 'eval_runtime': 3.1185, 'eval_samples_per_second': 70.226, 'eval_steps_per_second': 2.245, 'epoch': 32.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:26:19,206 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1984/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:26:20,891 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1984/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:26:20,892 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1984/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:26:24,531 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1860] due to args.save_total_limit\n",
            "                                          {'loss': 0.8219, 'learning_rate': 0.00016790322580645162, 'epoch': 32.1}\n",
            "                                          {'loss': 0.7048, 'learning_rate': 0.00016774193548387098, 'epoch': 32.26}\n",
            " 16% 2010/12400 [19:25<1:11:24,  2.43it/s]{'loss': 0.8028, 'learning_rate': 0.00016758064516129032, 'epoch': 32.42}\n",
            "{'loss': 0.6955, 'learning_rate': 0.00016741935483870966, 'epoch': 32.58}\n",
            "{'loss': 0.8235, 'learning_rate': 0.00016725806451612906, 'epoch': 32.74}\n",
            "{'loss': 0.8746, 'learning_rate': 0.0001670967741935484, 'epoch': 32.9}\n",
            " 16% 2046/12400 [19:39<1:05:17,  2.64it/s][INFO|trainer.py:2625] 2022-05-19 04:26:51,702 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:26:51,702 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:26:51,703 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.90it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.02it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.16it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.2678143978118896, 'eval_accuracy': 0.4657534246575342, 'eval_runtime': 3.0732, 'eval_samples_per_second': 71.261, 'eval_steps_per_second': 2.278, 'epoch': 33.0}\n",
            " 16% 2046/12400 [19:42<1:05:17,  2.64it/s]\n",
            "100% 7/7 [00:00<00:00,  8.56it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:26:54,778 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2046\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:26:54,781 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2046/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:26:56,423 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2046/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:26:56,424 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2046/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:27:00,168 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1922] due to args.save_total_limit\n",
            "                                          {'loss': 0.7936, 'learning_rate': 0.00016693548387096776, 'epoch': 33.06}\n",
            " 17% 2060/12400 [19:56<1:19:40,  2.16it/s]{'loss': 0.7695, 'learning_rate': 0.0001667741935483871, 'epoch': 33.23}\n",
            "                                          {'loss': 0.835, 'learning_rate': 0.00016661290322580644, 'epoch': 33.39}\n",
            "                                          {'loss': 0.832, 'learning_rate': 0.0001664516129032258, 'epoch': 33.55}\n",
            " 17% 2090/12400 [20:08<1:10:10,  2.45it/s]{'loss': 0.8546, 'learning_rate': 0.00016629032258064517, 'epoch': 33.71}\n",
            "{'loss': 0.769, 'learning_rate': 0.00016612903225806454, 'epoch': 33.87}\n",
            " 17% 2108/12400 [20:15<1:04:00,  2.68it/s][INFO|trainer.py:2625] 2022-05-19 04:27:27,464 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:27:27,464 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:27:27,464 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.57it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.42it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.39it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.38it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.37it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.2047042846679688, 'eval_accuracy': 0.4520547945205479, 'eval_runtime': 3.0745, 'eval_samples_per_second': 71.231, 'eval_steps_per_second': 2.277, 'epoch': 34.0}\n",
            " 17% 2108/12400 [20:18<1:04:00,  2.68it/s]\n",
            "100% 7/7 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:27:30,540 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2108\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:27:30,544 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2108/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:27:32,193 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2108/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:27:32,193 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2108/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:27:36,083 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1984] due to args.save_total_limit\n",
            " 17% 2110/12400 [20:26<7:25:25,  2.60s/it] {'loss': 0.6695, 'learning_rate': 0.00016596774193548388, 'epoch': 34.03}\n",
            "                                          {'loss': 0.5655, 'learning_rate': 0.00016580645161290322, 'epoch': 34.19}\n",
            " 17% 2130/12400 [20:35<1:09:58,  2.45it/s]{'loss': 0.6843, 'learning_rate': 0.0001656451612903226, 'epoch': 34.35}\n",
            " 17% 2140/12400 [20:39<1:08:09,  2.51it/s]{'loss': 0.9336, 'learning_rate': 0.00016548387096774193, 'epoch': 34.52}\n",
            " 17% 2150/12400 [20:43<1:09:39,  2.45it/s]{'loss': 0.7679, 'learning_rate': 0.00016532258064516132, 'epoch': 34.68}\n",
            "{'loss': 0.8024, 'learning_rate': 0.00016516129032258066, 'epoch': 34.84}\n",
            "                                          {'loss': 0.7043, 'learning_rate': 0.000165, 'epoch': 35.0}\n",
            " 18% 2170/12400 [20:51<1:01:22,  2.78it/s][INFO|trainer.py:2625] 2022-05-19 04:28:03,132 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:28:03,132 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:28:03,132 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.78it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.53it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.42it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.44it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.3034584522247314, 'eval_accuracy': 0.4429223744292237, 'eval_runtime': 3.0331, 'eval_samples_per_second': 72.202, 'eval_steps_per_second': 2.308, 'epoch': 35.0}\n",
            " 18% 2170/12400 [20:54<1:01:22,  2.78it/s]\n",
            "100% 7/7 [00:00<00:00,  8.78it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:28:06,167 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2170\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:28:06,171 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2170/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:28:07,829 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2170/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:28:07,830 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2170/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:28:11,425 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2046] due to args.save_total_limit\n",
            " 18% 2180/12400 [21:05<1:34:58,  1.79it/s]{'loss': 0.8048, 'learning_rate': 0.00016483870967741937, 'epoch': 35.16}\n",
            " 18% 2190/12400 [21:10<1:11:23,  2.38it/s]{'loss': 0.6669, 'learning_rate': 0.0001646774193548387, 'epoch': 35.32}\n",
            "{'loss': 0.6675, 'learning_rate': 0.00016451612903225807, 'epoch': 35.48}\n",
            "{'loss': 0.6965, 'learning_rate': 0.00016435483870967744, 'epoch': 35.65}\n",
            " 18% 2220/12400 [21:21<1:06:54,  2.54it/s]{'loss': 0.6518, 'learning_rate': 0.00016419354838709678, 'epoch': 35.81}\n",
            "                                          {'loss': 0.813, 'learning_rate': 0.00016403225806451614, 'epoch': 35.97}\n",
            " 18% 2232/12400 [21:26<1:03:42,  2.66it/s][INFO|trainer.py:2625] 2022-05-19 04:28:38,680 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:28:38,681 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:28:38,681 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.89it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.97it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.22it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.2991867065429688, 'eval_accuracy': 0.4429223744292237, 'eval_runtime': 3.0352, 'eval_samples_per_second': 72.154, 'eval_steps_per_second': 2.306, 'epoch': 36.0}\n",
            " 18% 2232/12400 [21:29<1:03:42,  2.66it/s]\n",
            "100% 7/7 [00:00<00:00,  8.69it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:28:41,718 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2232\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:28:41,720 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2232/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:28:43,378 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2232/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:28:43,379 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2232/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:28:47,061 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2108] due to args.save_total_limit\n",
            "{'loss': 0.7449, 'learning_rate': 0.00016387096774193548, 'epoch': 36.13}\n",
            "{'loss': 0.7054, 'learning_rate': 0.00016370967741935485, 'epoch': 36.29}\n",
            "{'loss': 0.6523, 'learning_rate': 0.0001635483870967742, 'epoch': 36.45}\n",
            "{'loss': 0.7147, 'learning_rate': 0.00016338709677419356, 'epoch': 36.61}\n",
            " 18% 2280/12400 [21:57<1:07:39,  2.49it/s]{'loss': 0.7325, 'learning_rate': 0.00016322580645161292, 'epoch': 36.77}\n",
            "{'loss': 0.7945, 'learning_rate': 0.00016306451612903226, 'epoch': 36.94}\n",
            " 18% 2294/12400 [22:02<1:00:13,  2.80it/s][INFO|trainer.py:2625] 2022-05-19 04:29:14,163 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:29:14,163 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:29:14,164 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.54it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.66it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.18it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.52it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.2741260528564453, 'eval_accuracy': 0.4611872146118721, 'eval_runtime': 3.0009, 'eval_samples_per_second': 72.978, 'eval_steps_per_second': 2.333, 'epoch': 37.0}\n",
            " 18% 2294/12400 [22:05<1:00:13,  2.80it/s]\n",
            "100% 7/7 [00:00<00:00,  8.01it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:29:17,167 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2294\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:29:17,170 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2294/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:29:18,851 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2294/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:29:18,852 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2294/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:29:22,523 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2170] due to args.save_total_limit\n",
            "{'loss': 0.6199, 'learning_rate': 0.00016290322580645163, 'epoch': 37.1}\n",
            "{'loss': 0.5728, 'learning_rate': 0.00016274193548387097, 'epoch': 37.26}\n",
            "{'loss': 0.5797, 'learning_rate': 0.00016258064516129034, 'epoch': 37.42}\n",
            " 19% 2330/12400 [22:27<1:08:37,  2.45it/s]{'loss': 0.7633, 'learning_rate': 0.0001624193548387097, 'epoch': 37.58}\n",
            "                                          {'loss': 0.8072, 'learning_rate': 0.00016225806451612904, 'epoch': 37.74}\n",
            "{'loss': 0.6188, 'learning_rate': 0.00016209677419354838, 'epoch': 37.9}\n",
            " 19% 2356/12400 [22:37<1:01:01,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 04:29:49,582 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:29:49,582 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:29:49,582 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.02it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.23it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.89it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.33it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.67it/s]\u001b[A\n",
            "                                          \n",
            " 19% 2356/12400 [22:40<1:01:01,  2.74it/s]\n",
            "100% 7/7 [00:00<00:00,  8.19it/s]{'eval_loss': 2.3901216983795166, 'eval_accuracy': 0.4611872146118721, 'eval_runtime': 3.0078, 'eval_samples_per_second': 72.812, 'eval_steps_per_second': 2.327, 'epoch': 38.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:29:52,592 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2356\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:29:52,594 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2356/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:29:54,255 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2356/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:29:54,256 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2356/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:29:57,872 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2232] due to args.save_total_limit\n",
            "{'loss': 0.7643, 'learning_rate': 0.00016193548387096775, 'epoch': 38.06}\n",
            "                                          {'loss': 0.6459, 'learning_rate': 0.0001617741935483871, 'epoch': 38.23}\n",
            " 19% 2380/12400 [22:57<1:07:13,  2.48it/s]{'loss': 0.6984, 'learning_rate': 0.00016161290322580645, 'epoch': 38.39}\n",
            "                                          {'loss': 0.5167, 'learning_rate': 0.00016145161290322582, 'epoch': 38.55}\n",
            "{'loss': 0.7169, 'learning_rate': 0.00016129032258064516, 'epoch': 38.71}\n",
            "                                          {'loss': 0.6586, 'learning_rate': 0.00016112903225806453, 'epoch': 38.87}\n",
            " 20% 2418/12400 [23:12<59:47,  2.78it/s]  [INFO|trainer.py:2625] 2022-05-19 04:30:24,789 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:30:24,789 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:30:24,789 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.63it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.50it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.42it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.41it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.36it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.338545560836792, 'eval_accuracy': 0.4611872146118721, 'eval_runtime': 3.0908, 'eval_samples_per_second': 70.855, 'eval_steps_per_second': 2.265, 'epoch': 39.0}\n",
            " 20% 2418/12400 [23:15<59:47,  2.78it/s]\n",
            "100% 7/7 [00:00<00:00,  8.71it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:30:27,882 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2418\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:30:27,885 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2418/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:30:29,544 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2418/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:30:29,545 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2418/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:30:33,296 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2294] due to args.save_total_limit\n",
            "                                          {'loss': 0.5876, 'learning_rate': 0.00016096774193548387, 'epoch': 39.03}\n",
            "                                          {'loss': 0.5461, 'learning_rate': 0.00016080645161290323, 'epoch': 39.19}\n",
            " 20% 2440/12400 [23:32<1:09:32,  2.39it/s]{'loss': 0.66, 'learning_rate': 0.0001606451612903226, 'epoch': 39.35}\n",
            " 20% 2450/12400 [23:36<1:09:00,  2.40it/s]{'loss': 0.6567, 'learning_rate': 0.00016048387096774194, 'epoch': 39.52}\n",
            "                                          {'loss': 0.6079, 'learning_rate': 0.0001603225806451613, 'epoch': 39.68}\n",
            " 20% 2470/12400 [23:45<1:05:29,  2.53it/s]{'loss': 0.6233, 'learning_rate': 0.00016016129032258065, 'epoch': 39.84}\n",
            "                                        {'loss': 0.705, 'learning_rate': 0.00016, 'epoch': 40.0}\n",
            " 20% 2480/12400 [23:48<59:08,  2.80it/s][INFO|trainer.py:2625] 2022-05-19 04:31:00,855 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:31:00,855 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:31:00,855 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.56it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.51it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.41it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.35it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.282827138900757, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.0961, 'eval_samples_per_second': 70.734, 'eval_steps_per_second': 2.261, 'epoch': 40.0}\n",
            " 20% 2480/12400 [23:52<59:08,  2.80it/s]\n",
            "100% 7/7 [00:00<00:00,  8.68it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:31:03,954 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2480\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:31:03,957 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2480/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:31:05,598 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2480/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:31:05,599 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2480/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:31:09,168 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2356] due to args.save_total_limit\n",
            "{'loss': 0.5796, 'learning_rate': 0.00015983870967741935, 'epoch': 40.16}\n",
            "                                          {'loss': 0.6727, 'learning_rate': 0.00015967741935483872, 'epoch': 40.32}\n",
            "{'loss': 0.5332, 'learning_rate': 0.00015951612903225809, 'epoch': 40.48}\n",
            " 20% 2520/12400 [24:15<1:05:38,  2.51it/s]{'loss': 0.5313, 'learning_rate': 0.00015935483870967743, 'epoch': 40.65}\n",
            " 20% 2530/12400 [24:19<1:03:40,  2.58it/s]{'loss': 0.6496, 'learning_rate': 0.0001591935483870968, 'epoch': 40.81}\n",
            " 20% 2540/12400 [24:23<1:02:57,  2.61it/s]{'loss': 0.5174, 'learning_rate': 0.00015903225806451613, 'epoch': 40.97}\n",
            " 20% 2542/12400 [24:24<1:01:25,  2.67it/s][INFO|trainer.py:2625] 2022-05-19 04:31:36,157 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:31:36,158 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:31:36,158 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.62it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.89it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.11it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            "                                          \n",
            "                                 {'eval_loss': 2.2798190116882324, 'eval_accuracy': 0.45662100456621, 'eval_runtime': 3.1137, 'eval_samples_per_second': 70.333, 'eval_steps_per_second': 2.248, 'epoch': 41.0}\n",
            " 20% 2542/12400 [24:27<1:01:25,  2.67it/s]\n",
            "100% 7/7 [00:00<00:00,  8.58it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:31:39,273 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2542\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:31:39,276 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2542/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:31:40,906 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2542/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:31:40,907 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2542/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:31:44,356 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2418] due to args.save_total_limit\n",
            "{'loss': 0.589, 'learning_rate': 0.00015887096774193547, 'epoch': 41.13}\n",
            "                                          {'loss': 0.4978, 'learning_rate': 0.00015870967741935487, 'epoch': 41.29}\n",
            " 21% 2570/12400 [24:46<1:06:15,  2.47it/s]{'loss': 0.5959, 'learning_rate': 0.0001585483870967742, 'epoch': 41.45}\n",
            "                                          {'loss': 0.5626, 'learning_rate': 0.00015838709677419357, 'epoch': 41.61}\n",
            "{'loss': 0.6655, 'learning_rate': 0.0001582258064516129, 'epoch': 41.77}\n",
            "                                        {'loss': 0.7161, 'learning_rate': 0.00015806451612903225, 'epoch': 41.94}\n",
            " 21% 2604/12400 [24:59<59:03,  2.76it/s]  [INFO|trainer.py:2625] 2022-05-19 04:32:11,520 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:32:11,520 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:32:11,520 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.52it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.92it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.61it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.09it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.48it/s]\u001b[A\n",
            "                                        \n",
            " 21% 2604/12400 [25:02<59:03,  2.76it/s]\n",
            "{'eval_loss': 2.325350522994995, 'eval_accuracy': 0.4703196347031963, 'eval_runtime': 3.0717, 'eval_samples_per_second': 71.295, 'eval_steps_per_second': 2.279, 'epoch': 42.0}\n",
            "100% 7/7 [00:00<00:00,  7.99it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:32:14,594 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2604\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:32:14,597 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2604/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:32:16,270 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2604/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:32:16,271 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2604/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:32:20,859 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2480] due to args.save_total_limit\n",
            " 21% 2610/12400 [25:13<2:43:02,  1.00it/s]{'loss': 0.4478, 'learning_rate': 0.00015790322580645162, 'epoch': 42.1}\n",
            "{'loss': 0.5151, 'learning_rate': 0.00015774193548387098, 'epoch': 42.26}\n",
            " 21% 2630/12400 [25:22<1:06:54,  2.43it/s]{'loss': 0.5843, 'learning_rate': 0.00015758064516129035, 'epoch': 42.42}\n",
            "                                          {'loss': 0.5211, 'learning_rate': 0.0001574193548387097, 'epoch': 42.58}\n",
            "                                          {'loss': 0.5943, 'learning_rate': 0.00015725806451612903, 'epoch': 42.74}\n",
            " 21% 2660/12400 [25:34<1:03:28,  2.56it/s]{'loss': 0.5686, 'learning_rate': 0.0001570967741935484, 'epoch': 42.9}\n",
            " 22% 2666/12400 [25:36<59:41,  2.72it/s][INFO|trainer.py:2625] 2022-05-19 04:32:48,341 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:32:48,341 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:32:48,341 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.06it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.40it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.67it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.39066743850708, 'eval_accuracy': 0.4657534246575342, 'eval_runtime': 3.0322, 'eval_samples_per_second': 72.226, 'eval_steps_per_second': 2.309, 'epoch': 43.0}\n",
            " 22% 2666/12400 [25:39<59:41,  2.72it/s]\n",
            "100% 7/7 [00:00<00:00,  8.15it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:32:51,375 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2666\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:32:51,378 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2666/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:32:53,033 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2666/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:32:53,034 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2666/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:32:56,775 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2542] due to args.save_total_limit\n",
            "{'loss': 0.6456, 'learning_rate': 0.00015693548387096774, 'epoch': 43.06}\n",
            "{'loss': 0.6646, 'learning_rate': 0.0001567741935483871, 'epoch': 43.23}\n",
            "                                          {'loss': 0.4846, 'learning_rate': 0.00015661290322580647, 'epoch': 43.39}\n",
            "                                          {'loss': 0.4512, 'learning_rate': 0.0001564516129032258, 'epoch': 43.55}\n",
            " 22% 2710/12400 [26:05<1:07:34,  2.39it/s]{'loss': 0.5989, 'learning_rate': 0.00015629032258064518, 'epoch': 43.71}\n",
            "                                          {'loss': 0.4879, 'learning_rate': 0.00015612903225806451, 'epoch': 43.87}\n",
            " 22% 2728/12400 [26:12<59:15,  2.72it/s][INFO|trainer.py:2625] 2022-05-19 04:33:24,545 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:33:24,546 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:33:24,546 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.10it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.06it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.73it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.22it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.59it/s]\u001b[A\n",
            "                                        \n",
            "{'eval_loss': 2.28242564201355, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.0433, 'eval_samples_per_second': 71.962, 'eval_steps_per_second': 2.3, 'epoch': 44.0}\n",
            " 22% 2728/12400 [26:15<59:15,  2.72it/s]\n",
            "100% 7/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:33:27,591 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2728\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:33:27,594 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2728/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:33:29,272 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2728/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:33:29,273 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2728/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:33:32,910 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2604] due to args.save_total_limit\n",
            " 22% 2730/12400 [26:23<6:51:21,  2.55s/it]{'loss': 0.4787, 'learning_rate': 0.00015596774193548388, 'epoch': 44.03}\n",
            "{'loss': 0.3449, 'learning_rate': 0.00015580645161290325, 'epoch': 44.19}\n",
            "                                          {'loss': 0.4941, 'learning_rate': 0.0001556451612903226, 'epoch': 44.35}\n",
            "{'loss': 0.5183, 'learning_rate': 0.00015548387096774195, 'epoch': 44.52}\n",
            "                                          {'loss': 0.5231, 'learning_rate': 0.0001553225806451613, 'epoch': 44.68}\n",
            "                                          {'loss': 0.5845, 'learning_rate': 0.00015516129032258063, 'epoch': 44.84}\n",
            " 22% 2790/12400 [26:48<59:36,  2.69it/s]{'loss': 0.5908, 'learning_rate': 0.000155, 'epoch': 45.0}\n",
            " 22% 2790/12400 [26:48<59:36,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 04:34:00,362 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:34:00,362 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:34:00,363 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.77it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.95it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.02it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.20it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.383288860321045, 'eval_accuracy': 0.4748858447488584, 'eval_runtime': 3.184, 'eval_samples_per_second': 68.781, 'eval_steps_per_second': 2.198, 'epoch': 45.0}\n",
            " 22% 2790/12400 [26:51<59:36,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.54it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:34:03,549 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2790\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:34:03,552 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2790/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:34:05,199 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2790/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:34:05,200 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2790/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:34:08,756 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2666] due to args.save_total_limit\n",
            "                                          {'loss': 0.3669, 'learning_rate': 0.00015483870967741937, 'epoch': 45.16}\n",
            "                                          {'loss': 0.501, 'learning_rate': 0.00015467741935483873, 'epoch': 45.32}\n",
            "{'loss': 0.5961, 'learning_rate': 0.00015451612903225807, 'epoch': 45.48}\n",
            "{'loss': 0.668, 'learning_rate': 0.0001543548387096774, 'epoch': 45.65}\n",
            " 23% 2840/12400 [27:19<1:03:53,  2.49it/s]{'loss': 0.5507, 'learning_rate': 0.00015419354838709678, 'epoch': 45.81}\n",
            "{'loss': 0.5333, 'learning_rate': 0.00015403225806451615, 'epoch': 45.97}\n",
            " 23% 2852/12400 [27:24<1:01:51,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 04:34:36,244 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:34:36,244 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:34:36,245 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.86it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.95it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            "                                          \n",
            " 23% 2852/12400 [27:27<1:01:51,  2.57it/s]\n",
            "100% 7/7 [00:00<00:00,  8.52it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:34:39,385 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2852\n",
            "{'eval_loss': 2.2621867656707764, 'eval_accuracy': 0.4611872146118721, 'eval_runtime': 3.1389, 'eval_samples_per_second': 69.77, 'eval_steps_per_second': 2.23, 'epoch': 46.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:34:39,388 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2852/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:34:41,076 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2852/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:34:41,077 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2852/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:34:44,505 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2728] due to args.save_total_limit\n",
            "                                          {'loss': 0.5062, 'learning_rate': 0.0001538709677419355, 'epoch': 46.13}\n",
            "                                          {'loss': 0.5549, 'learning_rate': 0.00015370967741935485, 'epoch': 46.29}\n",
            " 23% 2880/12400 [27:46<1:05:49,  2.41it/s]{'loss': 0.5272, 'learning_rate': 0.0001535483870967742, 'epoch': 46.45}\n",
            "                                          {'loss': 0.4816, 'learning_rate': 0.00015338709677419356, 'epoch': 46.61}\n",
            "{'loss': 0.6236, 'learning_rate': 0.0001532258064516129, 'epoch': 46.77}\n",
            "                                        {'loss': 0.4796, 'learning_rate': 0.00015306451612903226, 'epoch': 46.94}\n",
            " 24% 2914/12400 [27:59<57:34,  2.75it/s][INFO|trainer.py:2625] 2022-05-19 04:35:11,843 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:35:11,843 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:35:11,843 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.63it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.83it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.98it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.03it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.04it/s]\u001b[A\n",
            "                                        \n",
            "{'eval_loss': 2.296900749206543, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.1357, 'eval_samples_per_second': 69.842, 'eval_steps_per_second': 2.232, 'epoch': 47.0}\n",
            " 24% 2914/12400 [28:03<57:34,  2.75it/s]\n",
            "100% 7/7 [00:00<00:00,  8.46it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:35:14,981 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2914\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:35:14,984 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2914/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:35:16,672 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2914/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:35:16,674 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2914/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:35:20,143 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2790] due to args.save_total_limit\n",
            "{'loss': 0.4659, 'learning_rate': 0.00015290322580645163, 'epoch': 47.1}\n",
            " 24% 2930/12400 [28:17<1:11:55,  2.19it/s]{'loss': 0.5037, 'learning_rate': 0.00015274193548387097, 'epoch': 47.26}\n",
            "{'loss': 0.4743, 'learning_rate': 0.00015258064516129034, 'epoch': 47.42}\n",
            "{'loss': 0.5005, 'learning_rate': 0.00015241935483870968, 'epoch': 47.58}\n",
            "{'loss': 0.553, 'learning_rate': 0.00015225806451612902, 'epoch': 47.74}\n",
            "                                          {'loss': 0.544, 'learning_rate': 0.0001520967741935484, 'epoch': 47.9}\n",
            " 24% 2976/12400 [28:35<58:44,  2.67it/s][INFO|trainer.py:2625] 2022-05-19 04:35:47,649 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:35:47,649 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:35:47,649 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.64it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.84it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.92it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.01it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.10it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.3692424297332764, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.1249, 'eval_samples_per_second': 70.082, 'eval_steps_per_second': 2.24, 'epoch': 48.0}\n",
            " 24% 2976/12400 [28:38<58:44,  2.67it/s]\n",
            "100% 7/7 [00:00<00:00,  8.48it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:35:50,776 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2976\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:35:50,779 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2976/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:35:52,473 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2976/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:35:52,475 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2976/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:35:55,926 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2852] due to args.save_total_limit\n",
            "                                          {'loss': 0.5014, 'learning_rate': 0.00015193548387096775, 'epoch': 48.06}\n",
            " 24% 2990/12400 [28:52<1:12:48,  2.15it/s]{'loss': 0.4263, 'learning_rate': 0.00015177419354838712, 'epoch': 48.23}\n",
            "{'loss': 0.4718, 'learning_rate': 0.00015161290322580646, 'epoch': 48.39}\n",
            "{'loss': 0.51, 'learning_rate': 0.0001514516129032258, 'epoch': 48.55}\n",
            "                                          {'loss': 0.6897, 'learning_rate': 0.00015129032258064516, 'epoch': 48.71}\n",
            " 24% 3030/12400 [29:08<1:02:44,  2.49it/s]{'loss': 0.4627, 'learning_rate': 0.00015112903225806453, 'epoch': 48.87}\n",
            " 24% 3038/12400 [29:11<59:24,  2.63it/s][INFO|trainer.py:2625] 2022-05-19 04:36:23,637 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:36:23,637 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:36:23,638 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.45it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.78it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.11it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.4335641860961914, 'eval_accuracy': 0.4611872146118721, 'eval_runtime': 3.1043, 'eval_samples_per_second': 70.547, 'eval_steps_per_second': 2.255, 'epoch': 49.0}\n",
            " 24% 3038/12400 [29:14<59:24,  2.63it/s]\n",
            "100% 7/7 [00:00<00:00,  8.52it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:36:26,744 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3038\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:36:26,747 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3038/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:36:28,429 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3038/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:36:28,430 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3038/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:36:31,920 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2914] due to args.save_total_limit\n",
            "{'loss': 0.5747, 'learning_rate': 0.0001509677419354839, 'epoch': 49.03}\n",
            " 25% 3050/12400 [29:27<1:29:05,  1.75it/s]{'loss': 0.5208, 'learning_rate': 0.00015080645161290323, 'epoch': 49.19}\n",
            " 25% 3060/12400 [29:32<1:07:38,  2.30it/s]{'loss': 0.4616, 'learning_rate': 0.00015064516129032257, 'epoch': 49.35}\n",
            "                                          {'loss': 0.4095, 'learning_rate': 0.00015048387096774194, 'epoch': 49.52}\n",
            "{'loss': 0.5981, 'learning_rate': 0.00015032258064516128, 'epoch': 49.68}\n",
            "                                          {'loss': 0.607, 'learning_rate': 0.00015016129032258067, 'epoch': 49.84}\n",
            "{'loss': 0.4277, 'learning_rate': 0.00015000000000000001, 'epoch': 50.0}\n",
            " 25% 3100/12400 [29:48<54:32,  2.84it/s][INFO|trainer.py:2625] 2022-05-19 04:37:00,097 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:37:00,098 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:37:00,098 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.72it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.93it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.20it/s]\u001b[A\n",
            "                                        \n",
            " 25% 3100/12400 [29:51<54:32,  2.84it/s]\n",
            "100% 7/7 [00:00<00:00,  8.58it/s]\u001b[A\n",
            "                                 {'eval_loss': 2.2420270442962646, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.0851, 'eval_samples_per_second': 70.986, 'eval_steps_per_second': 2.269, 'epoch': 50.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 04:37:03,185 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3100\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:37:03,188 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3100/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:37:04,873 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3100/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:37:04,874 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3100/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:37:08,557 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-2976] due to args.save_total_limit\n",
            "{'loss': 0.4017, 'learning_rate': 0.00014983870967741935, 'epoch': 50.16}\n",
            " 25% 3120/12400 [30:07<1:04:57,  2.38it/s]{'loss': 0.4745, 'learning_rate': 0.00014967741935483872, 'epoch': 50.32}\n",
            "                                          {'loss': 0.5711, 'learning_rate': 0.00014951612903225806, 'epoch': 50.48}\n",
            " 25% 3140/12400 [30:15<1:02:47,  2.46it/s]{'loss': 0.448, 'learning_rate': 0.00014935483870967743, 'epoch': 50.65}\n",
            " 25% 3150/12400 [30:19<1:00:06,  2.56it/s]{'loss': 0.5586, 'learning_rate': 0.0001491935483870968, 'epoch': 50.81}\n",
            "                                        {'loss': 0.3953, 'learning_rate': 0.00014903225806451613, 'epoch': 50.97}\n",
            " 26% 3162/12400 [30:23<58:56,  2.61it/s][INFO|trainer.py:2625] 2022-05-19 04:37:35,966 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:37:35,966 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:37:35,966 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.22it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.4544053077697754, 'eval_accuracy': 0.4200913242009132, 'eval_runtime': 3.1169, 'eval_samples_per_second': 70.263, 'eval_steps_per_second': 2.246, 'epoch': 51.0}\n",
            " 26% 3162/12400 [30:27<58:56,  2.61it/s]\n",
            "100% 7/7 [00:00<00:00,  8.56it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:37:39,085 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3162\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:37:39,089 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3162/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:37:40,767 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3162/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:37:40,768 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3162/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:37:44,234 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3038] due to args.save_total_limit\n",
            "                                          {'loss': 0.3802, 'learning_rate': 0.0001488709677419355, 'epoch': 51.13}\n",
            "                                          {'loss': 0.4551, 'learning_rate': 0.00014870967741935484, 'epoch': 51.29}\n",
            "                                          {'loss': 0.3648, 'learning_rate': 0.0001485483870967742, 'epoch': 51.45}\n",
            "                                          {'loss': 0.4603, 'learning_rate': 0.00014838709677419355, 'epoch': 51.61}\n",
            "{'loss': 0.5487, 'learning_rate': 0.0001482258064516129, 'epoch': 51.77}\n",
            "{'loss': 0.6133, 'learning_rate': 0.00014806451612903228, 'epoch': 51.94}\n",
            " 26% 3224/12400 [30:59<59:30,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 04:38:11,812 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:38:11,813 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:38:11,813 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.73it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.91it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.00it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            "                                        \n",
            " 26% 3224/12400 [31:03<59:30,  2.57it/s]\n",
            "100% 7/7 [00:00<00:00,  8.59it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.303511619567871, 'eval_accuracy': 0.4474885844748858, 'eval_runtime': 3.1463, 'eval_samples_per_second': 69.605, 'eval_steps_per_second': 2.225, 'epoch': 52.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 04:38:14,961 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3224\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:38:14,964 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3224/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:38:16,645 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3224/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:38:16,645 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3224/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:38:20,397 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3100] due to args.save_total_limit\n",
            " 26% 3230/12400 [31:13<2:28:12,  1.03it/s]{'loss': 0.3918, 'learning_rate': 0.00014790322580645162, 'epoch': 52.1}\n",
            "{'loss': 0.485, 'learning_rate': 0.00014774193548387098, 'epoch': 52.26}\n",
            "                                          {'loss': 0.4312, 'learning_rate': 0.00014758064516129032, 'epoch': 52.42}\n",
            "                                          {'loss': 0.4428, 'learning_rate': 0.0001474193548387097, 'epoch': 52.58}\n",
            "{'loss': 0.5256, 'learning_rate': 0.00014725806451612906, 'epoch': 52.74}\n",
            "                                        {'loss': 0.431, 'learning_rate': 0.0001470967741935484, 'epoch': 52.9}\n",
            " 26% 3286/12400 [31:35<52:49,  2.88it/s][INFO|trainer.py:2625] 2022-05-19 04:38:47,744 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:38:47,745 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:38:47,745 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.29it/s]\u001b[A\n",
            "                                        \n",
            " 26% 3286/12400 [31:38<52:49,  2.88it/s]\n",
            "100% 7/7 [00:00<00:00,  8.67it/s]{'eval_loss': 2.475928544998169, 'eval_accuracy': 0.4703196347031963, 'eval_runtime': 3.0738, 'eval_samples_per_second': 71.248, 'eval_steps_per_second': 2.277, 'epoch': 53.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:38:50,820 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3286\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:38:50,823 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3286/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:38:52,480 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3286/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:38:52,481 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3286/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:38:56,116 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3162] due to args.save_total_limit\n",
            "{'loss': 0.484, 'learning_rate': 0.00014693548387096774, 'epoch': 53.06}\n",
            " 27% 3300/12400 [31:52<1:09:31,  2.18it/s]{'loss': 0.4627, 'learning_rate': 0.0001467741935483871, 'epoch': 53.23}\n",
            " 27% 3310/12400 [31:56<1:01:08,  2.48it/s]{'loss': 0.3816, 'learning_rate': 0.00014661290322580644, 'epoch': 53.39}\n",
            "                                          {'loss': 0.4457, 'learning_rate': 0.0001464516129032258, 'epoch': 53.55}\n",
            " 27% 3330/12400 [32:04<59:48,  2.53it/s]{'loss': 0.392, 'learning_rate': 0.00014629032258064518, 'epoch': 53.71}\n",
            "{'loss': 0.4666, 'learning_rate': 0.00014612903225806452, 'epoch': 53.87}\n",
            " 27% 3348/12400 [32:11<55:05,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 04:39:23,148 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:39:23,148 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:39:23,148 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.12it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.29it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.36it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.70it/s]\u001b[A\n",
            "                                        \n",
            " 27% 3348/12400 [32:14<55:05,  2.74it/s]\n",
            "100% 7/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.37141752243042, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 2.9878, 'eval_samples_per_second': 73.299, 'eval_steps_per_second': 2.343, 'epoch': 54.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 04:39:26,138 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3348\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:39:26,141 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3348/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:39:27,787 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3348/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:39:27,788 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3348/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:39:31,722 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3224] due to args.save_total_limit\n",
            "{'loss': 0.4319, 'learning_rate': 0.00014596774193548388, 'epoch': 54.03}\n",
            "{'loss': 0.4918, 'learning_rate': 0.00014580645161290322, 'epoch': 54.19}\n",
            "{'loss': 0.425, 'learning_rate': 0.0001456451612903226, 'epoch': 54.35}\n",
            "{'loss': 0.4204, 'learning_rate': 0.00014548387096774196, 'epoch': 54.52}\n",
            "                                          {'loss': 0.4502, 'learning_rate': 0.0001453225806451613, 'epoch': 54.68}\n",
            "{'loss': 0.4641, 'learning_rate': 0.00014516129032258066, 'epoch': 54.84}\n",
            " 28% 3410/12400 [32:47<56:09,  2.67it/s]{'loss': 0.5171, 'learning_rate': 0.000145, 'epoch': 55.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 04:39:58,918 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:39:58,918 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:39:58,918 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.07it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.47it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.76it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.95it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.07it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.4447622299194336, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.0727, 'eval_samples_per_second': 71.273, 'eval_steps_per_second': 2.278, 'epoch': 55.0}\n",
            " 28% 3410/12400 [32:50<56:09,  2.67it/s]\n",
            "100% 7/7 [00:00<00:00,  8.49it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:40:01,993 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3410\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:40:01,996 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3410/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:40:03,666 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3410/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:40:03,667 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3410/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:40:07,900 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3286] due to args.save_total_limit\n",
            " 28% 3420/12400 [33:01<1:22:56,  1.80it/s]{'loss': 0.3971, 'learning_rate': 0.00014483870967741937, 'epoch': 55.16}\n",
            " 28% 3430/12400 [33:06<1:01:10,  2.44it/s]{'loss': 0.5283, 'learning_rate': 0.0001446774193548387, 'epoch': 55.32}\n",
            "                                          {'loss': 0.4769, 'learning_rate': 0.00014451612903225807, 'epoch': 55.48}\n",
            "{'loss': 0.4566, 'learning_rate': 0.00014435483870967744, 'epoch': 55.65}\n",
            "{'loss': 0.4612, 'learning_rate': 0.00014419354838709678, 'epoch': 55.81}\n",
            "                                        {'loss': 0.4945, 'learning_rate': 0.00014403225806451615, 'epoch': 55.97}\n",
            " 28% 3472/12400 [33:22<54:14,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 04:40:34,967 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:40:34,967 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:40:34,967 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.29it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "                                        \n",
            " 28% 3472/12400 [33:26<54:14,  2.74it/s]{'eval_loss': 2.3640663623809814, 'eval_accuracy': 0.4611872146118721, 'eval_runtime': 3.1187, 'eval_samples_per_second': 70.222, 'eval_steps_per_second': 2.245, 'epoch': 56.0}\n",
            "\n",
            "100% 7/7 [00:00<00:00,  8.71it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:40:38,089 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3472\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:40:38,092 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3472/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:40:39,748 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3472/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:40:39,749 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3472/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:40:43,483 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3348] due to args.save_total_limit\n",
            " 28% 3480/12400 [33:36<1:42:06,  1.46it/s]{'loss': 0.3305, 'learning_rate': 0.00014387096774193549, 'epoch': 56.13}\n",
            " 28% 3490/12400 [33:40<1:03:02,  2.36it/s]{'loss': 0.4277, 'learning_rate': 0.00014370967741935483, 'epoch': 56.29}\n",
            " 28% 3500/12400 [33:44<1:00:28,  2.45it/s]{'loss': 0.4035, 'learning_rate': 0.00014354838709677422, 'epoch': 56.45}\n",
            " 28% 3510/12400 [33:49<1:00:32,  2.45it/s]{'loss': 0.3644, 'learning_rate': 0.00014338709677419356, 'epoch': 56.61}\n",
            " 28% 3520/12400 [33:53<58:19,  2.54it/s]{'loss': 0.4874, 'learning_rate': 0.00014322580645161293, 'epoch': 56.77}\n",
            " 28% 3530/12400 [33:57<56:43,  2.61it/s]{'loss': 0.5206, 'learning_rate': 0.00014306451612903227, 'epoch': 56.94}\n",
            " 28% 3534/12400 [33:58<56:00,  2.64it/s][INFO|trainer.py:2625] 2022-05-19 04:41:10,631 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:41:10,631 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:41:10,632 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.45it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.36it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.33it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            "                                        \n",
            "                                 {'eval_loss': 2.3773488998413086, 'eval_accuracy': 0.4931506849315068, 'eval_runtime': 3.1155, 'eval_samples_per_second': 70.293, 'eval_steps_per_second': 2.247, 'epoch': 57.0}\n",
            " 28% 3534/12400 [34:01<56:00,  2.64it/s]\n",
            "100% 7/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:41:13,749 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3534\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:41:13,752 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3534/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:41:15,411 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3534/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:41:15,412 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3534/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:41:18,883 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3410] due to args.save_total_limit\n",
            " 29% 3540/12400 [34:11<2:21:58,  1.04it/s]{'loss': 0.4418, 'learning_rate': 0.0001429032258064516, 'epoch': 57.1}\n",
            "{'loss': 0.3458, 'learning_rate': 0.00014274193548387097, 'epoch': 57.26}\n",
            "{'loss': 0.3166, 'learning_rate': 0.00014258064516129034, 'epoch': 57.42}\n",
            "{'loss': 0.3806, 'learning_rate': 0.00014241935483870968, 'epoch': 57.58}\n",
            "{'loss': 0.3291, 'learning_rate': 0.00014225806451612904, 'epoch': 57.74}\n",
            " 29% 3590/12400 [34:32<56:01,  2.62it/s]{'loss': 0.4706, 'learning_rate': 0.00014209677419354838, 'epoch': 57.9}\n",
            " 29% 3596/12400 [34:34<53:40,  2.73it/s][INFO|trainer.py:2625] 2022-05-19 04:41:46,356 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:41:46,357 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:41:46,357 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.63it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.84it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.57it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.04it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.40it/s]\u001b[A\n",
            "                                        {'eval_loss': 2.466498613357544, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.0628, 'eval_samples_per_second': 71.503, 'eval_steps_per_second': 2.285, 'epoch': 58.0}\n",
            "\n",
            " 29% 3596/12400 [34:37<53:40,  2.73it/s]\n",
            "100% 7/7 [00:00<00:00,  7.97it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:41:49,422 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3596\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:41:49,425 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3596/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:41:51,106 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3596/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:41:51,107 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3596/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:41:54,714 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3472] due to args.save_total_limit\n",
            " 29% 3600/12400 [34:46<3:37:04,  1.48s/it]{'loss': 0.3856, 'learning_rate': 0.00014193548387096775, 'epoch': 58.06}\n",
            "                                          {'loss': 0.4312, 'learning_rate': 0.0001417741935483871, 'epoch': 58.23}\n",
            "                                        {'loss': 0.3452, 'learning_rate': 0.00014161290322580646, 'epoch': 58.39}\n",
            " 29% 3630/12400 [34:59<59:10,  2.47it/s]{'loss': 0.4396, 'learning_rate': 0.00014145161290322582, 'epoch': 58.55}\n",
            " 29% 3640/12400 [35:02<59:07,  2.47it/s]{'loss': 0.4223, 'learning_rate': 0.00014129032258064516, 'epoch': 58.71}\n",
            "                                        {'loss': 0.342, 'learning_rate': 0.00014112903225806453, 'epoch': 58.87}\n",
            " 30% 3658/12400 [35:09<54:31,  2.67it/s][INFO|trainer.py:2625] 2022-05-19 04:42:21,881 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:42:21,881 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:42:21,881 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.89it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            "                                        \n",
            " 30% 3658/12400 [35:13<54:31,  2.67it/s]\n",
            "100% 7/7 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:42:24,997 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3658\n",
            "{'eval_loss': 2.3991200923919678, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.1145, 'eval_samples_per_second': 70.316, 'eval_steps_per_second': 2.248, 'epoch': 59.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:42:25,000 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3658/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:42:26,678 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3658/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:42:26,680 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3658/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:42:30,424 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3534] due to args.save_total_limit\n",
            " 30% 3660/12400 [35:21<6:15:55,  2.58s/it]{'loss': 0.3315, 'learning_rate': 0.00014096774193548387, 'epoch': 59.03}\n",
            " 30% 3670/12400 [35:25<1:11:06,  2.05it/s]{'loss': 0.4106, 'learning_rate': 0.00014080645161290324, 'epoch': 59.19}\n",
            "{'loss': 0.4358, 'learning_rate': 0.0001406451612903226, 'epoch': 59.35}\n",
            " 30% 3690/12400 [35:33<59:07,  2.46it/s]{'loss': 0.3655, 'learning_rate': 0.00014048387096774194, 'epoch': 59.52}\n",
            "                                        {'loss': 0.4829, 'learning_rate': 0.0001403225806451613, 'epoch': 59.68}\n",
            " 30% 3710/12400 [35:42<57:17,  2.53it/s]{'loss': 0.4175, 'learning_rate': 0.00014016129032258065, 'epoch': 59.84}\n",
            " 30% 3720/12400 [35:45<52:43,  2.74it/s]{'loss': 0.4102, 'learning_rate': 0.00014, 'epoch': 60.0}\n",
            " 30% 3720/12400 [35:45<52:43,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 04:42:57,554 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:42:57,554 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:42:57,554 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.32it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.35it/s]\u001b[A\n",
            "                                        \n",
            " 30% 3720/12400 [35:48<52:43,  2.74it/s]\n",
            "{'eval_loss': 2.475078821182251, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.0676, 'eval_samples_per_second': 71.391, 'eval_steps_per_second': 2.282, 'epoch': 60.0}\n",
            "100% 7/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:43:00,624 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3720\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:43:00,627 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3720/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:43:02,314 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3720/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:43:02,315 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3720/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:43:05,845 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3596] due to args.save_total_limit\n",
            "                                          {'loss': 0.2884, 'learning_rate': 0.00013983870967741935, 'epoch': 60.16}\n",
            "                                          {'loss': 0.4152, 'learning_rate': 0.00013967741935483872, 'epoch': 60.32}\n",
            "{'loss': 0.3996, 'learning_rate': 0.0001395161290322581, 'epoch': 60.48}\n",
            " 30% 3760/12400 [36:12<59:21,  2.43it/s]{'loss': 0.5327, 'learning_rate': 0.00013935483870967743, 'epoch': 60.65}\n",
            " 30% 3770/12400 [36:17<57:31,  2.50it/s]{'loss': 0.3504, 'learning_rate': 0.00013919354838709677, 'epoch': 60.81}\n",
            " 30% 3780/12400 [36:20<55:08,  2.61it/s]{'loss': 0.4558, 'learning_rate': 0.00013903225806451613, 'epoch': 60.97}\n",
            " 30% 3782/12400 [36:21<53:55,  2.66it/s][INFO|trainer.py:2625] 2022-05-19 04:43:33,597 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:43:33,597 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:43:33,597 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.46it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.74it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.50it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.03it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.41it/s]\u001b[A\n",
            "                                        \n",
            " 30% 3782/12400 [36:24<53:55,  2.66it/s]\n",
            "100% 7/7 [00:00<00:00,  7.92it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.3961212635040283, 'eval_accuracy': 0.502283105022831, 'eval_runtime': 3.082, 'eval_samples_per_second': 71.057, 'eval_steps_per_second': 2.271, 'epoch': 61.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 04:43:36,681 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3782\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:43:36,684 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3782/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:43:38,351 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3782/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:43:38,352 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3782/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:43:42,064 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3658] due to args.save_total_limit\n",
            "{'loss': 0.4097, 'learning_rate': 0.0001388709677419355, 'epoch': 61.13}\n",
            " 31% 3800/12400 [36:40<1:02:45,  2.28it/s]{'loss': 0.3208, 'learning_rate': 0.00013870967741935487, 'epoch': 61.29}\n",
            "{'loss': 0.4092, 'learning_rate': 0.0001385483870967742, 'epoch': 61.45}\n",
            " 31% 3820/12400 [36:48<59:23,  2.41it/s]{'loss': 0.4636, 'learning_rate': 0.00013838709677419355, 'epoch': 61.61}\n",
            "{'loss': 0.3699, 'learning_rate': 0.0001382258064516129, 'epoch': 61.77}\n",
            " 31% 3840/12400 [36:56<52:27,  2.72it/s]{'loss': 0.4233, 'learning_rate': 0.00013806451612903225, 'epoch': 61.94}\n",
            " 31% 3844/12400 [36:57<50:23,  2.83it/s][INFO|trainer.py:2625] 2022-05-19 04:44:09,540 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:44:09,540 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:44:09,540 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.56it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.80it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.93it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.5903828144073486, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.0862, 'eval_samples_per_second': 70.962, 'eval_steps_per_second': 2.268, 'epoch': 62.0}\n",
            " 31% 3844/12400 [37:00<50:23,  2.83it/s]\n",
            "100% 7/7 [00:00<00:00,  8.48it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:44:12,628 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3844\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:44:12,631 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3844/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:44:14,287 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3844/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:44:14,288 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3844/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:44:17,892 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3720] due to args.save_total_limit\n",
            "{'loss': 0.347, 'learning_rate': 0.00013790322580645162, 'epoch': 62.1}\n",
            "                                          {'loss': 0.3106, 'learning_rate': 0.00013774193548387099, 'epoch': 62.26}\n",
            " 31% 3870/12400 [37:18<57:41,  2.46it/s]{'loss': 0.3771, 'learning_rate': 0.00013758064516129032, 'epoch': 62.42}\n",
            " 31% 3880/12400 [37:22<57:49,  2.46it/s]{'loss': 0.3921, 'learning_rate': 0.0001374193548387097, 'epoch': 62.58}\n",
            "                                        {'loss': 0.3576, 'learning_rate': 0.00013725806451612903, 'epoch': 62.74}\n",
            "                                        {'loss': 0.4281, 'learning_rate': 0.00013709677419354837, 'epoch': 62.9}\n",
            " 32% 3906/12400 [37:32<51:08,  2.77it/s][INFO|trainer.py:2625] 2022-05-19 04:44:45,091 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:44:45,092 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:44:45,092 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.23it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.34it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.430992841720581, 'eval_accuracy': 0.502283105022831, 'eval_runtime': 3.0629, 'eval_samples_per_second': 71.501, 'eval_steps_per_second': 2.285, 'epoch': 63.0}\n",
            " 32% 3906/12400 [37:36<51:08,  2.77it/s]\n",
            "100% 7/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:44:48,156 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3906\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:44:48,160 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3906/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:44:49,817 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3906/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:44:49,818 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3906/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:44:53,440 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3782] due to args.save_total_limit\n",
            "                                          {'loss': 0.3268, 'learning_rate': 0.00013693548387096776, 'epoch': 63.06}\n",
            "                                          {'loss': 0.3752, 'learning_rate': 0.0001367741935483871, 'epoch': 63.23}\n",
            "{'loss': 0.3992, 'learning_rate': 0.00013661290322580647, 'epoch': 63.39}\n",
            "                                        {'loss': 0.4248, 'learning_rate': 0.0001364516129032258, 'epoch': 63.55}\n",
            " 32% 3950/12400 [38:01<57:05,  2.47it/s]{'loss': 0.4516, 'learning_rate': 0.00013629032258064515, 'epoch': 63.71}\n",
            " 32% 3960/12400 [38:05<54:34,  2.58it/s]{'loss': 0.3688, 'learning_rate': 0.00013612903225806452, 'epoch': 63.87}\n",
            " 32% 3968/12400 [38:08<51:25,  2.73it/s][INFO|trainer.py:2625] 2022-05-19 04:45:20,625 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:45:20,625 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:45:20,625 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.67it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.85it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.00it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.16it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.405967950820923, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.0827, 'eval_samples_per_second': 71.042, 'eval_steps_per_second': 2.271, 'epoch': 64.0}\n",
            " 32% 3968/12400 [38:11<51:25,  2.73it/s]\n",
            "100% 7/7 [00:00<00:00,  8.55it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:45:23,709 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3968\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:45:23,712 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3968/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:45:25,413 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3968/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:45:25,414 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3968/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:45:29,140 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3844] due to args.save_total_limit\n",
            "{'loss': 0.3329, 'learning_rate': 0.00013596774193548388, 'epoch': 64.03}\n",
            " 32% 3980/12400 [38:24<1:10:42,  1.98it/s]{'loss': 0.3275, 'learning_rate': 0.00013580645161290325, 'epoch': 64.19}\n",
            " 32% 3990/12400 [38:28<57:59,  2.42it/s]{'loss': 0.4172, 'learning_rate': 0.0001356451612903226, 'epoch': 64.35}\n",
            "{'loss': 0.3234, 'learning_rate': 0.00013548387096774193, 'epoch': 64.52}\n",
            " 32% 4010/12400 [38:36<57:20,  2.44it/s]{'loss': 0.3322, 'learning_rate': 0.0001353225806451613, 'epoch': 64.68}\n",
            "{'loss': 0.4625, 'learning_rate': 0.00013516129032258064, 'epoch': 64.84}\n",
            "{'loss': 0.4007, 'learning_rate': 0.00013500000000000003, 'epoch': 65.0}\n",
            " 32% 4030/12400 [38:44<50:32,  2.76it/s][INFO|trainer.py:2625] 2022-05-19 04:45:56,353 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:45:56,353 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:45:56,353 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.22it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.03it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.68it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.18it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.54it/s]\u001b[A\n",
            "                                        \n",
            " 32% 4030/12400 [38:47<50:32,  2.76it/s]\n",
            "100% 7/7 [00:00<00:00,  8.06it/s]{'eval_loss': 2.4195985794067383, 'eval_accuracy': 0.5159817351598174, 'eval_runtime': 3.094, 'eval_samples_per_second': 70.783, 'eval_steps_per_second': 2.262, 'epoch': 65.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:45:59,449 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4030\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:45:59,451 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4030/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:46:01,122 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4030/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:46:01,123 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4030/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:46:04,948 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3906] due to args.save_total_limit\n",
            " 33% 4040/12400 [38:59<1:16:20,  1.83it/s]{'loss': 0.2566, 'learning_rate': 0.00013483870967741937, 'epoch': 65.16}\n",
            "{'loss': 0.3323, 'learning_rate': 0.0001346774193548387, 'epoch': 65.32}\n",
            "                                        {'loss': 0.2963, 'learning_rate': 0.00013451612903225807, 'epoch': 65.48}\n",
            "                                        {'loss': 0.3305, 'learning_rate': 0.00013435483870967741, 'epoch': 65.65}\n",
            " 33% 4080/12400 [39:15<54:25,  2.55it/s]{'loss': 0.3451, 'learning_rate': 0.00013419354838709678, 'epoch': 65.81}\n",
            " 33% 4090/12400 [39:19<52:00,  2.66it/s]{'loss': 0.4436, 'learning_rate': 0.00013403225806451615, 'epoch': 65.97}\n",
            " 33% 4092/12400 [39:19<51:01,  2.71it/s][INFO|trainer.py:2625] 2022-05-19 04:46:32,110 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:46:32,110 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:46:32,111 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.58it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.85it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.98it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.403294563293457, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.1342, 'eval_samples_per_second': 69.874, 'eval_steps_per_second': 2.233, 'epoch': 66.0}\n",
            " 33% 4092/12400 [39:23<51:01,  2.71it/s]\n",
            "100% 7/7 [00:00<00:00,  8.53it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:46:35,247 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4092\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:46:35,251 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4092/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:46:36,901 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4092/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:46:36,902 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4092/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:46:40,667 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-3968] due to args.save_total_limit\n",
            "                                          {'loss': 0.3486, 'learning_rate': 0.0001338709677419355, 'epoch': 66.13}\n",
            " 33% 4110/12400 [39:38<58:47,  2.35it/s]{'loss': 0.3283, 'learning_rate': 0.00013370967741935485, 'epoch': 66.29}\n",
            "{'loss': 0.371, 'learning_rate': 0.0001335483870967742, 'epoch': 66.45}\n",
            "{'loss': 0.3835, 'learning_rate': 0.00013338709677419356, 'epoch': 66.61}\n",
            "                                        {'loss': 0.2931, 'learning_rate': 0.0001332258064516129, 'epoch': 66.77}\n",
            "                                        {'loss': 0.3954, 'learning_rate': 0.00013306451612903227, 'epoch': 66.94}\n",
            " 34% 4154/12400 [39:55<50:09,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 04:47:08,008 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:47:08,008 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:47:08,008 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.03it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.33it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.65it/s]\u001b[A\n",
            "                                        \n",
            " 34% 4154/12400 [39:59<50:09,  2.74it/s]{'eval_loss': 2.423234462738037, 'eval_accuracy': 0.4931506849315068, 'eval_runtime': 3.0584, 'eval_samples_per_second': 71.606, 'eval_steps_per_second': 2.289, 'epoch': 67.0}\n",
            "\n",
            "100% 7/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:47:11,068 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4154\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:47:11,071 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4154/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:47:12,752 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4154/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:47:12,753 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4154/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:47:16,361 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4030] due to args.save_total_limit\n",
            "                                          {'loss': 0.2758, 'learning_rate': 0.00013290322580645163, 'epoch': 67.1}\n",
            " 34% 4170/12400 [40:13<1:00:47,  2.26it/s]{'loss': 0.3311, 'learning_rate': 0.00013274193548387097, 'epoch': 67.26}\n",
            "                                        {'loss': 0.3428, 'learning_rate': 0.00013258064516129034, 'epoch': 67.42}\n",
            " 34% 4190/12400 [40:21<55:09,  2.48it/s]{'loss': 0.2925, 'learning_rate': 0.00013241935483870968, 'epoch': 67.58}\n",
            "                                        {'loss': 0.2822, 'learning_rate': 0.00013225806451612905, 'epoch': 67.74}\n",
            "                                        {'loss': 0.3961, 'learning_rate': 0.0001320967741935484, 'epoch': 67.9}\n",
            " 34% 4216/12400 [40:31<50:03,  2.72it/s][INFO|trainer.py:2625] 2022-05-19 04:47:43,623 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:47:43,623 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:47:43,623 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.90it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.40it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.88it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.03it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.503878116607666, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.1366, 'eval_samples_per_second': 69.82, 'eval_steps_per_second': 2.232, 'epoch': 68.0}\n",
            " 34% 4216/12400 [40:34<50:03,  2.72it/s]\n",
            "100% 7/7 [00:00<00:00,  8.43it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:47:46,762 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4216\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:47:46,765 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4216/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:47:48,447 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4216/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:47:48,448 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4216/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:47:52,045 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4092] due to args.save_total_limit\n",
            " 34% 4220/12400 [40:43<3:22:26,  1.48s/it]{'loss': 0.3666, 'learning_rate': 0.00013193548387096775, 'epoch': 68.06}\n",
            "{'loss': 0.3648, 'learning_rate': 0.0001317741935483871, 'epoch': 68.23}\n",
            " 34% 4240/12400 [40:52<55:44,  2.44it/s]{'loss': 0.4681, 'learning_rate': 0.00013161290322580646, 'epoch': 68.39}\n",
            "                                        {'loss': 0.4563, 'learning_rate': 0.0001314516129032258, 'epoch': 68.55}\n",
            " 34% 4260/12400 [41:00<56:36,  2.40it/s]{'loss': 0.3659, 'learning_rate': 0.00013129032258064516, 'epoch': 68.71}\n",
            "                                        {'loss': 0.351, 'learning_rate': 0.00013112903225806453, 'epoch': 68.87}\n",
            " 34% 4278/12400 [41:07<50:15,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 04:48:19,480 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:48:19,480 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:48:19,480 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.16it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            "                                        {'eval_loss': 2.4805617332458496, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.0551, 'eval_samples_per_second': 71.683, 'eval_steps_per_second': 2.291, 'epoch': 69.0}\n",
            "\n",
            " 34% 4278/12400 [41:10<50:15,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:48:22,537 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4278\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:48:22,540 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4278/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:48:24,198 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4278/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:48:24,199 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4278/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:48:27,775 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4154] due to args.save_total_limit\n",
            "                                          {'loss': 0.3785, 'learning_rate': 0.00013096774193548387, 'epoch': 69.03}\n",
            "{'loss': 0.2745, 'learning_rate': 0.00013080645161290324, 'epoch': 69.19}\n",
            "{'loss': 0.2542, 'learning_rate': 0.00013064516129032258, 'epoch': 69.35}\n",
            "{'loss': 0.385, 'learning_rate': 0.00013048387096774194, 'epoch': 69.52}\n",
            "{'loss': 0.3756, 'learning_rate': 0.0001303225806451613, 'epoch': 69.68}\n",
            "                                        {'loss': 0.4096, 'learning_rate': 0.00013016129032258065, 'epoch': 69.84}\n",
            " 35% 4340/12400 [41:43<47:59,  2.80it/s]{'loss': 0.3656, 'learning_rate': 0.00013000000000000002, 'epoch': 70.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 04:48:54,920 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:48:54,920 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:48:54,921 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.29it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.33it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.4767603874206543, 'eval_accuracy': 0.4931506849315068, 'eval_runtime': 3.072, 'eval_samples_per_second': 71.288, 'eval_steps_per_second': 2.279, 'epoch': 70.0}\n",
            " 35% 4340/12400 [41:46<47:59,  2.80it/s]\n",
            "100% 7/7 [00:00<00:00,  8.55it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:48:57,995 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4340\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:48:57,998 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4340/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:48:59,631 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4340/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:48:59,632 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4340/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:49:03,080 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4216] due to args.save_total_limit\n",
            " 35% 4350/12400 [41:57<1:13:32,  1.82it/s]{'loss': 0.4295, 'learning_rate': 0.00012983870967741936, 'epoch': 70.16}\n",
            " 35% 4360/12400 [42:01<56:10,  2.39it/s]{'loss': 0.3291, 'learning_rate': 0.00012967741935483872, 'epoch': 70.32}\n",
            "{'loss': 0.2771, 'learning_rate': 0.00012951612903225806, 'epoch': 70.48}\n",
            "{'loss': 0.3515, 'learning_rate': 0.00012935483870967743, 'epoch': 70.65}\n",
            "                                        {'loss': 0.2439, 'learning_rate': 0.0001291935483870968, 'epoch': 70.81}\n",
            "                                        {'loss': 0.3008, 'learning_rate': 0.00012903225806451613, 'epoch': 70.97}\n",
            " 36% 4402/12400 [42:18<49:33,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 04:49:30,348 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:49:30,348 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:49:30,348 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.75it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "                                        \n",
            " 36% 4402/12400 [42:21<49:33,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.62it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.428549289703369, 'eval_accuracy': 0.5068493150684932, 'eval_runtime': 3.0812, 'eval_samples_per_second': 71.076, 'eval_steps_per_second': 2.272, 'epoch': 71.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 04:49:33,431 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4402\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:49:33,434 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4402/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:49:35,140 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4402/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:49:35,141 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4402/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:49:39,016 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4278] due to args.save_total_limit\n",
            "{'loss': 0.2972, 'learning_rate': 0.0001288709677419355, 'epoch': 71.13}\n",
            " 36% 4420/12400 [42:36<58:38,  2.27it/s]  {'loss': 0.4106, 'learning_rate': 0.00012870967741935484, 'epoch': 71.29}\n",
            "                                        {'loss': 0.3348, 'learning_rate': 0.00012854838709677418, 'epoch': 71.45}\n",
            " 36% 4440/12400 [42:45<54:38,  2.43it/s]{'loss': 0.2239, 'learning_rate': 0.00012838709677419357, 'epoch': 71.61}\n",
            "{'loss': 0.2412, 'learning_rate': 0.0001282258064516129, 'epoch': 71.77}\n",
            " 36% 4460/12400 [42:53<50:35,  2.62it/s]{'loss': 0.3481, 'learning_rate': 0.00012806451612903228, 'epoch': 71.94}\n",
            " 36% 4464/12400 [42:54<50:54,  2.60it/s][INFO|trainer.py:2625] 2022-05-19 04:50:06,514 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:50:06,514 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:50:06,514 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.11it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.77it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.24it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.56it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.5751442909240723, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.0909, 'eval_samples_per_second': 70.853, 'eval_steps_per_second': 2.265, 'epoch': 72.0}\n",
            " 36% 4464/12400 [42:57<50:54,  2.60it/s]\n",
            "100% 7/7 [00:00<00:00,  8.14it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:50:09,607 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4464\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:50:09,610 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4464/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:50:11,302 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4464/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:50:11,303 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4464/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:50:14,982 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4340] due to args.save_total_limit\n",
            "{'loss': 0.3111, 'learning_rate': 0.00012790322580645162, 'epoch': 72.1}\n",
            "                                          {'loss': 0.2834, 'learning_rate': 0.00012774193548387096, 'epoch': 72.26}\n",
            "                                        {'loss': 0.3788, 'learning_rate': 0.00012758064516129033, 'epoch': 72.42}\n",
            "                                        {'loss': 0.2699, 'learning_rate': 0.0001274193548387097, 'epoch': 72.58}\n",
            " 36% 4510/12400 [43:24<54:55,  2.39it/s]{'loss': 0.3662, 'learning_rate': 0.00012725806451612903, 'epoch': 72.74}\n",
            "                                        {'loss': 0.3241, 'learning_rate': 0.0001270967741935484, 'epoch': 72.9}\n",
            " 36% 4526/12400 [43:30<48:20,  2.71it/s][INFO|trainer.py:2625] 2022-05-19 04:50:42,833 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:50:42,833 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:50:42,833 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  5.93it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.46it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.01it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.38it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.63it/s]\u001b[A\n",
            "                                        \n",
            " 36% 4526/12400 [43:34<48:20,  2.71it/s]\n",
            "100% 7/7 [00:00<00:00,  8.06it/s]\u001b[A{'eval_loss': 2.4481401443481445, 'eval_accuracy': 0.502283105022831, 'eval_runtime': 3.1332, 'eval_samples_per_second': 69.896, 'eval_steps_per_second': 2.234, 'epoch': 73.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:50:45,969 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4526\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:50:45,972 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4526/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:50:47,685 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4526/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:50:47,686 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4526/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:50:51,187 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4402] due to args.save_total_limit\n",
            "{'loss': 0.2726, 'learning_rate': 0.00012693548387096774, 'epoch': 73.06}\n",
            "{'loss': 0.2824, 'learning_rate': 0.0001267741935483871, 'epoch': 73.23}\n",
            "{'loss': 0.2584, 'learning_rate': 0.00012661290322580644, 'epoch': 73.39}\n",
            " 37% 4560/12400 [43:55<53:46,  2.43it/s]{'loss': 0.3765, 'learning_rate': 0.0001264516129032258, 'epoch': 73.55}\n",
            " 37% 4570/12400 [43:59<52:43,  2.48it/s]{'loss': 0.3508, 'learning_rate': 0.00012629032258064518, 'epoch': 73.71}\n",
            " 37% 4580/12400 [44:03<51:46,  2.52it/s]{'loss': 0.3374, 'learning_rate': 0.00012612903225806452, 'epoch': 73.87}\n",
            " 37% 4588/12400 [44:06<48:50,  2.67it/s][INFO|trainer.py:2625] 2022-05-19 04:51:18,865 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:51:18,865 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:51:18,865 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.12it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.86it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.38it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.73it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            "100% 7/7 [00:00<00:00,  8.49it/s]\u001b[A{'eval_loss': 2.3931515216827393, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.1594, 'eval_samples_per_second': 69.318, 'eval_steps_per_second': 2.216, 'epoch': 74.0}\n",
            "                                        \n",
            " 37% 4588/12400 [44:10<48:50,  2.67it/s]\n",
            "100% 7/7 [00:00<00:00,  8.49it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:51:22,026 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4588\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:51:22,030 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4588/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:51:23,706 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4588/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:51:23,707 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4588/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:51:27,456 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4464] due to args.save_total_limit\n",
            "                                          {'loss': 0.3353, 'learning_rate': 0.00012596774193548388, 'epoch': 74.03}\n",
            "                                          {'loss': 0.3191, 'learning_rate': 0.00012580645161290322, 'epoch': 74.19}\n",
            "                                        {'loss': 0.2405, 'learning_rate': 0.0001256451612903226, 'epoch': 74.35}\n",
            "{'loss': 0.2581, 'learning_rate': 0.00012548387096774196, 'epoch': 74.52}\n",
            "{'loss': 0.3559, 'learning_rate': 0.0001253225806451613, 'epoch': 74.68}\n",
            "{'loss': 0.2996, 'learning_rate': 0.00012516129032258066, 'epoch': 74.84}\n",
            "                                        {'loss': 0.257, 'learning_rate': 0.000125, 'epoch': 75.0}\n",
            " 38% 4650/12400 [44:43<46:54,  2.75it/s][INFO|trainer.py:2625] 2022-05-19 04:51:54,977 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:51:54,977 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:51:54,977 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.02it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.33it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.61it/s]\u001b[A\n",
            "                                        \n",
            " 38% 4650/12400 [44:46<46:54,  2.75it/s]\n",
            "100% 7/7 [00:00<00:00,  8.16it/s]\u001b[A{'eval_loss': 2.506527900695801, 'eval_accuracy': 0.4931506849315068, 'eval_runtime': 3.0115, 'eval_samples_per_second': 72.721, 'eval_steps_per_second': 2.324, 'epoch': 75.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:51:57,991 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4650\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:51:57,993 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4650/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:51:59,633 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4650/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:51:59,634 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4650/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:52:03,210 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4526] due to args.save_total_limit\n",
            "                                          {'loss': 0.2388, 'learning_rate': 0.00012483870967741934, 'epoch': 75.16}\n",
            "{'loss': 0.2632, 'learning_rate': 0.0001246774193548387, 'epoch': 75.32}\n",
            "{'loss': 0.3116, 'learning_rate': 0.00012451612903225808, 'epoch': 75.48}\n",
            " 38% 4690/12400 [45:10<51:55,  2.47it/s]{'loss': 0.2735, 'learning_rate': 0.00012435483870967744, 'epoch': 75.65}\n",
            "                                        {'loss': 0.3747, 'learning_rate': 0.00012419354838709678, 'epoch': 75.81}\n",
            "                                        {'loss': 0.3365, 'learning_rate': 0.00012403225806451612, 'epoch': 75.97}\n",
            " 38% 4712/12400 [45:18<49:37,  2.58it/s][INFO|trainer.py:2625] 2022-05-19 04:52:30,718 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:52:30,718 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:52:30,719 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.30it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.5061001777648926, 'eval_accuracy': 0.4748858447488584, 'eval_runtime': 3.0942, 'eval_samples_per_second': 70.778, 'eval_steps_per_second': 2.262, 'epoch': 76.0}\n",
            " 38% 4712/12400 [45:21<49:37,  2.58it/s]\n",
            "100% 7/7 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:52:33,815 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4712\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:52:33,817 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4712/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:52:35,506 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4712/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:52:35,507 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4712/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:52:39,170 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4588] due to args.save_total_limit\n",
            "                                          {'loss': 0.3057, 'learning_rate': 0.0001238709677419355, 'epoch': 76.13}\n",
            "{'loss': 0.3239, 'learning_rate': 0.00012370967741935485, 'epoch': 76.29}\n",
            "{'loss': 0.2844, 'learning_rate': 0.00012354838709677422, 'epoch': 76.45}\n",
            " 38% 4750/12400 [45:45<52:24,  2.43it/s]{'loss': 0.2548, 'learning_rate': 0.00012338709677419356, 'epoch': 76.61}\n",
            " 38% 4760/12400 [45:49<51:32,  2.47it/s]{'loss': 0.238, 'learning_rate': 0.0001232258064516129, 'epoch': 76.77}\n",
            "{'loss': 0.2748, 'learning_rate': 0.00012306451612903227, 'epoch': 76.94}\n",
            " 38% 4774/12400 [45:54<48:09,  2.64it/s][INFO|trainer.py:2625] 2022-05-19 04:53:06,923 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:53:06,923 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:53:06,923 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.19it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.08it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.73it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.19it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.51it/s]\u001b[A\n",
            "                                        \n",
            " 38% 4774/12400 [45:58<48:09,  2.64it/s]\n",
            "100% 7/7 [00:00<00:00,  8.02it/s]{'eval_loss': 2.4724276065826416, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.1001, 'eval_samples_per_second': 70.642, 'eval_steps_per_second': 2.258, 'epoch': 77.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:53:10,026 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4774\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:53:10,029 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4774/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:53:11,738 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4774/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:53:11,739 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4774/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:53:15,573 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4650] due to args.save_total_limit\n",
            " 39% 4780/12400 [46:08<2:04:54,  1.02it/s]{'loss': 0.2715, 'learning_rate': 0.0001229032258064516, 'epoch': 77.1}\n",
            " 39% 4790/12400 [46:12<57:41,  2.20it/s]{'loss': 0.2913, 'learning_rate': 0.00012274193548387097, 'epoch': 77.26}\n",
            "                                        {'loss': 0.3115, 'learning_rate': 0.00012258064516129034, 'epoch': 77.42}\n",
            "{'loss': 0.2249, 'learning_rate': 0.00012241935483870968, 'epoch': 77.58}\n",
            " 39% 4820/12400 [46:25<53:36,  2.36it/s]{'loss': 0.2787, 'learning_rate': 0.00012225806451612905, 'epoch': 77.74}\n",
            "                                        {'loss': 0.2964, 'learning_rate': 0.00012209677419354839, 'epoch': 77.9}\n",
            " 39% 4836/12400 [46:31<47:12,  2.67it/s][INFO|trainer.py:2625] 2022-05-19 04:53:43,736 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:53:43,736 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:53:43,736 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.56it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.80it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.97it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.11it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            "                                        \n",
            " 39% 4836/12400 [46:34<47:12,  2.67it/s]\n",
            "{'eval_loss': 2.4177889823913574, 'eval_accuracy': 0.5296803652968036, 'eval_runtime': 3.1238, 'eval_samples_per_second': 70.107, 'eval_steps_per_second': 2.241, 'epoch': 78.0}\n",
            "100% 7/7 [00:00<00:00,  8.53it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:53:46,862 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4836\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:53:46,865 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4836/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:53:48,554 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4836/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:53:48,555 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4836/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:53:52,214 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4712] due to args.save_total_limit\n",
            "{'loss': 0.2467, 'learning_rate': 0.00012193548387096774, 'epoch': 78.06}\n",
            "{'loss': 0.2954, 'learning_rate': 0.0001217741935483871, 'epoch': 78.23}\n",
            " 39% 4860/12400 [46:52<52:32,  2.39it/s]{'loss': 0.315, 'learning_rate': 0.00012161290322580644, 'epoch': 78.39}\n",
            " 39% 4870/12400 [46:56<51:53,  2.42it/s]{'loss': 0.2938, 'learning_rate': 0.00012145161290322583, 'epoch': 78.55}\n",
            "{'loss': 0.1913, 'learning_rate': 0.00012129032258064516, 'epoch': 78.71}\n",
            "{'loss': 0.3046, 'learning_rate': 0.00012112903225806452, 'epoch': 78.87}\n",
            " 40% 4898/12400 [47:07<47:01,  2.66it/s][INFO|trainer.py:2625] 2022-05-19 04:54:19,893 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:54:19,893 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:54:19,893 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.85it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.02it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            "                                        \n",
            " 40% 4898/12400 [47:11<47:01,  2.66it/s]\n",
            "{'eval_loss': 2.443002700805664, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.1159, 'eval_samples_per_second': 70.284, 'eval_steps_per_second': 2.247, 'epoch': 79.0}\n",
            "100% 7/7 [00:00<00:00,  8.58it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:54:23,011 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4898\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:54:23,014 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4898/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:54:24,674 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4898/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:54:24,675 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4898/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:54:28,310 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4774] due to args.save_total_limit\n",
            " 40% 4900/12400 [47:19<5:20:19,  2.56s/it]{'loss': 0.3474, 'learning_rate': 0.00012096774193548388, 'epoch': 79.03}\n",
            "{'loss': 0.2301, 'learning_rate': 0.00012080645161290322, 'epoch': 79.19}\n",
            "{'loss': 0.2819, 'learning_rate': 0.00012064516129032259, 'epoch': 79.35}\n",
            "{'loss': 0.2994, 'learning_rate': 0.00012048387096774194, 'epoch': 79.52}\n",
            "{'loss': 0.2092, 'learning_rate': 0.00012032258064516128, 'epoch': 79.68}\n",
            " 40% 4950/12400 [47:40<48:43,  2.55it/s]{'loss': 0.2943, 'learning_rate': 0.00012016129032258065, 'epoch': 79.84}\n",
            " 40% 4960/12400 [47:43<44:27,  2.79it/s]{'loss': 0.2971, 'learning_rate': 0.00012, 'epoch': 80.0}\n",
            " 40% 4960/12400 [47:43<44:27,  2.79it/s][INFO|trainer.py:2625] 2022-05-19 04:54:55,772 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:54:55,773 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:54:55,773 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.17it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.12it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.80it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.60it/s]\u001b[A\n",
            "                                        \n",
            " 40% 4960/12400 [47:46<44:27,  2.79it/s]\n",
            "100% 7/7 [00:00<00:00,  8.15it/s]\u001b[A\n",
            "{'eval_loss': 2.4930408000946045, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.072, 'eval_samples_per_second': 71.288, 'eval_steps_per_second': 2.279, 'epoch': 80.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:54:58,847 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4960\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:54:58,850 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4960/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:55:00,519 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4960/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:55:00,519 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4960/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:55:04,430 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4836] due to args.save_total_limit\n",
            " 40% 4970/12400 [47:58<1:10:22,  1.76it/s]{'loss': 0.2759, 'learning_rate': 0.00011983870967741937, 'epoch': 80.16}\n",
            "{'loss': 0.2567, 'learning_rate': 0.00011967741935483871, 'epoch': 80.32}\n",
            " 40% 4990/12400 [48:07<51:34,  2.39it/s]{'loss': 0.3298, 'learning_rate': 0.00011951612903225806, 'epoch': 80.48}\n",
            "                                        {'loss': 0.2244, 'learning_rate': 0.00011935483870967743, 'epoch': 80.65}\n",
            "                                        {'loss': 0.2125, 'learning_rate': 0.00011919354838709678, 'epoch': 80.81}\n",
            "                                        {'loss': 0.2931, 'learning_rate': 0.00011903225806451615, 'epoch': 80.97}\n",
            " 40% 5022/12400 [48:19<45:09,  2.72it/s][INFO|trainer.py:2625] 2022-05-19 04:55:31,973 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:55:31,973 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:55:31,973 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.23it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.16it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.20it/s]\u001b[A\n",
            "                                        \n",
            " 40% 5022/12400 [48:23<45:09,  2.72it/s]\n",
            "100% 7/7 [00:00<00:00,  8.54it/s]\u001b[A\n",
            "                                 {'eval_loss': 2.4603641033172607, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.0961, 'eval_samples_per_second': 70.734, 'eval_steps_per_second': 2.261, 'epoch': 81.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 04:55:35,071 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5022\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:55:35,075 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5022/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:55:36,754 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5022/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:55:36,755 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5022/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:55:40,372 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4898] due to args.save_total_limit\n",
            "                                          {'loss': 0.2067, 'learning_rate': 0.00011887096774193549, 'epoch': 81.13}\n",
            " 41% 5040/12400 [48:37<54:05,  2.27it/s]{'loss': 0.2934, 'learning_rate': 0.00011870967741935484, 'epoch': 81.29}\n",
            "{'loss': 0.2835, 'learning_rate': 0.00011854838709677421, 'epoch': 81.45}\n",
            "                                        {'loss': 0.3683, 'learning_rate': 0.00011838709677419355, 'epoch': 81.61}\n",
            " 41% 5070/12400 [48:50<49:50,  2.45it/s]{'loss': 0.3679, 'learning_rate': 0.00011822580645161291, 'epoch': 81.77}\n",
            " 41% 5080/12400 [48:54<45:17,  2.69it/s]{'loss': 0.2597, 'learning_rate': 0.00011806451612903227, 'epoch': 81.94}\n",
            " 41% 5084/12400 [48:55<44:57,  2.71it/s][INFO|trainer.py:2625] 2022-05-19 04:56:07,833 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:56:07,834 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:56:07,834 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.89it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            "                                        \n",
            " 41% 5084/12400 [48:59<44:57,  2.71it/s]\n",
            "100% 7/7 [00:00<00:00,  8.65it/s]\u001b[A{'eval_loss': 2.440504550933838, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.1044, 'eval_samples_per_second': 70.544, 'eval_steps_per_second': 2.255, 'epoch': 82.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:56:10,940 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5084\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:56:10,944 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5084/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:56:12,617 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5084/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:56:12,618 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5084/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:56:16,296 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-4960] due to args.save_total_limit\n",
            "{'loss': 0.2658, 'learning_rate': 0.00011790322580645161, 'epoch': 82.1}\n",
            "                                        {'loss': 0.1918, 'learning_rate': 0.00011774193548387097, 'epoch': 82.26}\n",
            "                                        {'loss': 0.2743, 'learning_rate': 0.00011758064516129033, 'epoch': 82.42}\n",
            "{'loss': 0.2247, 'learning_rate': 0.00011741935483870967, 'epoch': 82.58}\n",
            "                                        {'loss': 0.1913, 'learning_rate': 0.00011725806451612905, 'epoch': 82.74}\n",
            "                                        {'loss': 0.2455, 'learning_rate': 0.00011709677419354839, 'epoch': 82.9}\n",
            " 42% 5146/12400 [49:31<42:22,  2.85it/s][INFO|trainer.py:2625] 2022-05-19 04:56:43,802 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:56:43,803 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:56:43,803 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.12it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.71it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.16it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.47it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.449181079864502, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.0846, 'eval_samples_per_second': 70.999, 'eval_steps_per_second': 2.269, 'epoch': 83.0}\n",
            " 42% 5146/12400 [49:34<42:22,  2.85it/s]\n",
            "100% 7/7 [00:00<00:00,  7.97it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:56:46,890 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5146\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:56:46,893 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5146/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:56:48,563 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5146/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:56:48,564 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5146/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:56:52,113 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5022] due to args.save_total_limit\n",
            "{'loss': 0.3137, 'learning_rate': 0.00011693548387096775, 'epoch': 83.06}\n",
            "{'loss': 0.2796, 'learning_rate': 0.0001167741935483871, 'epoch': 83.23}\n",
            "{'loss': 0.2636, 'learning_rate': 0.00011661290322580645, 'epoch': 83.39}\n",
            " 42% 5180/12400 [49:56<50:24,  2.39it/s]{'loss': 0.3248, 'learning_rate': 0.00011645161290322581, 'epoch': 83.55}\n",
            " 42% 5190/12400 [50:01<50:29,  2.38it/s]{'loss': 0.3501, 'learning_rate': 0.00011629032258064517, 'epoch': 83.71}\n",
            "{'loss': 0.2642, 'learning_rate': 0.00011612903225806453, 'epoch': 83.87}\n",
            " 42% 5208/12400 [50:07<44:14,  2.71it/s][INFO|trainer.py:2625] 2022-05-19 04:57:19,745 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:57:19,745 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:57:19,746 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.49it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.75it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.45it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.35it/s]\u001b[A\n",
            "                                        \n",
            " 42% 5208/12400 [50:10<44:14,  2.71it/s]\n",
            "100% 7/7 [00:00<00:00,  7.87it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.531144618988037, 'eval_accuracy': 0.4703196347031963, 'eval_runtime': 3.0338, 'eval_samples_per_second': 72.186, 'eval_steps_per_second': 2.307, 'epoch': 84.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 04:57:22,781 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5208\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:57:22,784 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5208/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:57:24,450 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5208/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:57:24,451 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5208/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:57:28,019 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5084] due to args.save_total_limit\n",
            "{'loss': 0.2369, 'learning_rate': 0.00011596774193548387, 'epoch': 84.03}\n",
            "{'loss': 0.2487, 'learning_rate': 0.00011580645161290322, 'epoch': 84.19}\n",
            "                                        {'loss': 0.2596, 'learning_rate': 0.00011564516129032259, 'epoch': 84.35}\n",
            " 42% 5240/12400 [50:31<49:16,  2.42it/s]{'loss': 0.2354, 'learning_rate': 0.00011548387096774193, 'epoch': 84.52}\n",
            "                                        {'loss': 0.3141, 'learning_rate': 0.00011532258064516131, 'epoch': 84.68}\n",
            "{'loss': 0.3473, 'learning_rate': 0.00011516129032258065, 'epoch': 84.84}\n",
            "{'loss': 0.3608, 'learning_rate': 0.00011499999999999999, 'epoch': 85.0}\n",
            " 42% 5270/12400 [50:43<44:03,  2.70it/s][INFO|trainer.py:2625] 2022-05-19 04:57:55,618 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:57:55,619 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:57:55,619 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.94it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.20it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            "                                        \n",
            " 42% 5270/12400 [50:46<44:03,  2.70it/s]\n",
            "100% 7/7 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.537144422531128, 'eval_accuracy': 0.4748858447488584, 'eval_runtime': 3.0796, 'eval_samples_per_second': 71.112, 'eval_steps_per_second': 2.273, 'epoch': 85.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 04:57:58,700 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5270\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:57:58,703 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5270/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:58:00,359 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5270/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:58:00,360 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5270/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:58:03,882 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5146] due to args.save_total_limit\n",
            " 43% 5280/12400 [50:58<1:08:00,  1.74it/s]{'loss': 0.3329, 'learning_rate': 0.00011483870967741937, 'epoch': 85.16}\n",
            "{'loss': 0.2129, 'learning_rate': 0.00011467741935483871, 'epoch': 85.32}\n",
            "                                        {'loss': 0.2634, 'learning_rate': 0.00011451612903225808, 'epoch': 85.48}\n",
            " 43% 5310/12400 [51:11<48:51,  2.42it/s]{'loss': 0.2571, 'learning_rate': 0.00011435483870967743, 'epoch': 85.65}\n",
            " 43% 5320/12400 [51:15<46:16,  2.55it/s]{'loss': 0.3142, 'learning_rate': 0.00011419354838709677, 'epoch': 85.81}\n",
            "                                        {'loss': 0.2903, 'learning_rate': 0.00011403225806451614, 'epoch': 85.97}\n",
            " 43% 5332/12400 [51:19<43:43,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 04:58:31,701 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:58:31,701 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:58:31,701 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.33it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.33it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.34it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.533423662185669, 'eval_accuracy': 0.502283105022831, 'eval_runtime': 3.047, 'eval_samples_per_second': 71.875, 'eval_steps_per_second': 2.297, 'epoch': 86.0}\n",
            " 43% 5332/12400 [51:22<43:43,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.68it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:58:34,750 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5332\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:58:34,753 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5332/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:58:36,444 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5332/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:58:36,445 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5332/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:58:40,114 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5208] due to args.save_total_limit\n",
            "                                          {'loss': 0.2022, 'learning_rate': 0.00011387096774193549, 'epoch': 86.13}\n",
            "{'loss': 0.2646, 'learning_rate': 0.00011370967741935486, 'epoch': 86.29}\n",
            "                                        {'loss': 0.2714, 'learning_rate': 0.0001135483870967742, 'epoch': 86.45}\n",
            "{'loss': 0.248, 'learning_rate': 0.00011338709677419355, 'epoch': 86.61}\n",
            "                                        {'loss': 0.2803, 'learning_rate': 0.00011322580645161291, 'epoch': 86.77}\n",
            "{'loss': 0.2198, 'learning_rate': 0.00011306451612903225, 'epoch': 86.94}\n",
            " 44% 5394/12400 [51:55<42:10,  2.77it/s][INFO|trainer.py:2625] 2022-05-19 04:59:07,447 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:59:07,448 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:59:07,448 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.23it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.32it/s]\u001b[A\n",
            "                                        \n",
            " 44% 5394/12400 [51:58<42:10,  2.77it/s]\n",
            "100% 7/7 [00:00<00:00,  8.69it/s]{'eval_loss': 2.5007236003875732, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.0666, 'eval_samples_per_second': 71.415, 'eval_steps_per_second': 2.283, 'epoch': 87.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:59:10,516 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5394\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:59:10,519 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5394/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:59:12,165 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5394/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:59:12,165 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5394/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:59:15,923 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5270] due to args.save_total_limit\n",
            "{'loss': 0.22, 'learning_rate': 0.00011290322580645163, 'epoch': 87.1}\n",
            "{'loss': 0.2729, 'learning_rate': 0.00011274193548387097, 'epoch': 87.26}\n",
            "                                        {'loss': 0.3078, 'learning_rate': 0.00011258064516129033, 'epoch': 87.42}\n",
            "{'loss': 0.3647, 'learning_rate': 0.0001124193548387097, 'epoch': 87.58}\n",
            "                                        {'loss': 0.1973, 'learning_rate': 0.00011225806451612903, 'epoch': 87.74}\n",
            " 44% 5450/12400 [52:29<43:34,  2.66it/s]{'loss': 0.3339, 'learning_rate': 0.00011209677419354839, 'epoch': 87.9}\n",
            " 44% 5456/12400 [52:31<40:51,  2.83it/s][INFO|trainer.py:2625] 2022-05-19 04:59:43,180 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 04:59:43,180 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 04:59:43,180 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.68it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.91it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.4132637977600098, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.0565, 'eval_samples_per_second': 71.65, 'eval_steps_per_second': 2.29, 'epoch': 88.0}\n",
            " 44% 5456/12400 [52:34<40:51,  2.83it/s]\n",
            "100% 7/7 [00:00<00:00,  8.57it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 04:59:46,238 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5456\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 04:59:46,241 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5456/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 04:59:47,920 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5456/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 04:59:47,921 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5456/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 04:59:51,481 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5332] due to args.save_total_limit\n",
            "{'loss': 0.19, 'learning_rate': 0.00011193548387096775, 'epoch': 88.06}\n",
            "{'loss': 0.2181, 'learning_rate': 0.00011177419354838709, 'epoch': 88.23}\n",
            " 44% 5480/12400 [52:51<46:37,  2.47it/s]{'loss': 0.2038, 'learning_rate': 0.00011161290322580646, 'epoch': 88.39}\n",
            "                                        {'loss': 0.2456, 'learning_rate': 0.00011145161290322581, 'epoch': 88.55}\n",
            " 44% 5500/12400 [52:59<47:44,  2.41it/s]{'loss': 0.289, 'learning_rate': 0.00011129032258064515, 'epoch': 88.71}\n",
            " 44% 5510/12400 [53:03<45:07,  2.54it/s]{'loss': 0.1977, 'learning_rate': 0.00011112903225806452, 'epoch': 88.87}\n",
            " 44% 5518/12400 [53:06<41:53,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 05:00:18,641 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:00:18,641 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:00:18,641 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.78it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.00it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "                                        \n",
            " 44% 5518/12400 [53:09<41:53,  2.74it/s]\n",
            "{'eval_loss': 2.59940505027771, 'eval_accuracy': 0.4748858447488584, 'eval_runtime': 3.1223, 'eval_samples_per_second': 70.14, 'eval_steps_per_second': 2.242, 'epoch': 89.0}\n",
            "100% 7/7 [00:00<00:00,  8.57it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:00:21,766 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5518\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:00:21,769 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5518/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:00:23,444 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5518/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:00:23,445 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5518/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:00:27,047 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5394] due to args.save_total_limit\n",
            " 45% 5520/12400 [53:17<4:51:31,  2.54s/it]{'loss': 0.2186, 'learning_rate': 0.00011096774193548387, 'epoch': 89.03}\n",
            " 45% 5530/12400 [53:22<57:08,  2.00it/s]{'loss': 0.2739, 'learning_rate': 0.00011080645161290324, 'epoch': 89.19}\n",
            "{'loss': 0.2851, 'learning_rate': 0.00011064516129032259, 'epoch': 89.35}\n",
            "{'loss': 0.2352, 'learning_rate': 0.00011048387096774193, 'epoch': 89.52}\n",
            "{'loss': 0.2753, 'learning_rate': 0.0001103225806451613, 'epoch': 89.68}\n",
            "                                        {'loss': 0.2325, 'learning_rate': 0.00011016129032258065, 'epoch': 89.84}\n",
            " 45% 5580/12400 [53:42<41:13,  2.76it/s]{'loss': 0.2764, 'learning_rate': 0.00011000000000000002, 'epoch': 90.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 05:00:54,441 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:00:54,441 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:00:54,441 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.19it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.04it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.68it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.12it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.591869831085205, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.1339, 'eval_samples_per_second': 69.881, 'eval_steps_per_second': 2.234, 'epoch': 90.0}\n",
            " 45% 5580/12400 [53:45<41:13,  2.76it/s]\n",
            "100% 7/7 [00:01<00:00,  7.69it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:00:57,577 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5580\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:00:57,585 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5580/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:00:59,244 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5580/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:00:59,245 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5580/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:01:02,776 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5456] due to args.save_total_limit\n",
            "                                          {'loss': 0.2734, 'learning_rate': 0.00010983870967741936, 'epoch': 90.16}\n",
            " 45% 5600/12400 [54:01<47:54,  2.37it/s]{'loss': 0.2953, 'learning_rate': 0.00010967741935483871, 'epoch': 90.32}\n",
            " 45% 5610/12400 [54:05<46:36,  2.43it/s]{'loss': 0.2698, 'learning_rate': 0.00010951612903225808, 'epoch': 90.48}\n",
            "{'loss': 0.2716, 'learning_rate': 0.00010935483870967742, 'epoch': 90.65}\n",
            "{'loss': 0.2568, 'learning_rate': 0.00010919354838709678, 'epoch': 90.81}\n",
            " 45% 5640/12400 [54:17<43:12,  2.61it/s]{'loss': 0.2614, 'learning_rate': 0.00010903225806451614, 'epoch': 90.97}\n",
            " 46% 5642/12400 [54:18<42:13,  2.67it/s][INFO|trainer.py:2625] 2022-05-19 05:01:30,132 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:01:30,132 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:01:30,132 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.53it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.78it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            "                                        \n",
            " 46% 5642/12400 [54:21<42:13,  2.67it/s]\n",
            "{'eval_loss': 2.554084300994873, 'eval_accuracy': 0.5068493150684932, 'eval_runtime': 3.1025, 'eval_samples_per_second': 70.588, 'eval_steps_per_second': 2.256, 'epoch': 91.0}\n",
            "100% 7/7 [00:00<00:00,  8.53it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:01:33,236 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5642\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:01:33,240 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5642/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:01:34,919 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5642/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:01:34,921 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5642/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:01:38,549 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5518] due to args.save_total_limit\n",
            "{'loss': 0.3014, 'learning_rate': 0.00010887096774193548, 'epoch': 91.13}\n",
            "{'loss': 0.213, 'learning_rate': 0.00010870967741935486, 'epoch': 91.29}\n",
            "{'loss': 0.2141, 'learning_rate': 0.0001085483870967742, 'epoch': 91.45}\n",
            " 46% 5680/12400 [54:44<45:54,  2.44it/s]{'loss': 0.2192, 'learning_rate': 0.00010838709677419356, 'epoch': 91.61}\n",
            " 46% 5690/12400 [54:48<45:28,  2.46it/s]{'loss': 0.2065, 'learning_rate': 0.00010822580645161292, 'epoch': 91.77}\n",
            " 46% 5700/12400 [54:52<42:45,  2.61it/s]{'loss': 0.2334, 'learning_rate': 0.00010806451612903225, 'epoch': 91.94}\n",
            " 46% 5704/12400 [54:53<40:45,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 05:02:05,963 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:02:05,963 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:02:05,963 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.89it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.16it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.22it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.26it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.5132319927215576, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.1049, 'eval_samples_per_second': 70.534, 'eval_steps_per_second': 2.255, 'epoch': 92.0}\n",
            " 46% 5704/12400 [54:57<40:45,  2.74it/s]\n",
            "100% 7/7 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:02:09,070 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5704\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:02:09,074 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5704/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:02:10,749 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5704/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:02:10,750 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5704/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:02:14,335 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5580] due to args.save_total_limit\n",
            " 46% 5710/12400 [55:06<1:45:42,  1.05it/s]{'loss': 0.1717, 'learning_rate': 0.00010790322580645162, 'epoch': 92.1}\n",
            " 46% 5720/12400 [55:10<49:04,  2.27it/s]{'loss': 0.2184, 'learning_rate': 0.00010774193548387097, 'epoch': 92.26}\n",
            "                                        {'loss': 0.2968, 'learning_rate': 0.00010758064516129034, 'epoch': 92.42}\n",
            " 46% 5740/12400 [55:19<44:56,  2.47it/s]{'loss': 0.1522, 'learning_rate': 0.00010741935483870968, 'epoch': 92.58}\n",
            "{'loss': 0.278, 'learning_rate': 0.00010725806451612903, 'epoch': 92.74}\n",
            " 46% 5760/12400 [55:27<41:25,  2.67it/s]{'loss': 0.3043, 'learning_rate': 0.0001070967741935484, 'epoch': 92.9}\n",
            " 46% 5766/12400 [55:29<40:52,  2.70it/s][INFO|trainer.py:2625] 2022-05-19 05:02:41,354 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:02:41,355 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:02:41,355 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.76it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.97it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "                                        \n",
            " 46% 5766/12400 [55:32<40:52,  2.70it/s]\n",
            "100% 7/7 [00:00<00:00,  8.61it/s]{'eval_loss': 2.5390965938568115, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.0542, 'eval_samples_per_second': 71.705, 'eval_steps_per_second': 2.292, 'epoch': 93.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:02:44,411 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5766\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:02:44,414 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5766/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:02:46,065 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5766/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:02:46,066 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5766/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:02:49,704 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5642] due to args.save_total_limit\n",
            "                                          {'loss': 0.174, 'learning_rate': 0.00010693548387096774, 'epoch': 93.06}\n",
            " 47% 5780/12400 [55:45<50:32,  2.18it/s]{'loss': 0.2127, 'learning_rate': 0.00010677419354838709, 'epoch': 93.23}\n",
            "{'loss': 0.3106, 'learning_rate': 0.00010661290322580646, 'epoch': 93.39}\n",
            "                                        {'loss': 0.2292, 'learning_rate': 0.0001064516129032258, 'epoch': 93.55}\n",
            "{'loss': 0.2241, 'learning_rate': 0.00010629032258064518, 'epoch': 93.71}\n",
            "                                        {'loss': 0.274, 'learning_rate': 0.00010612903225806452, 'epoch': 93.87}\n",
            " 47% 5828/12400 [56:04<40:05,  2.73it/s][INFO|trainer.py:2625] 2022-05-19 05:03:16,944 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:03:16,944 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:03:16,945 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.04it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.28it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.91it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.31it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.59it/s]\u001b[A\n",
            "                                        \n",
            " 47% 5828/12400 [56:08<40:05,  2.73it/s]\n",
            "100% 7/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            "{'eval_loss': 2.5860683917999268, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.0016, 'eval_samples_per_second': 72.96, 'eval_steps_per_second': 2.332, 'epoch': 94.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:03:19,948 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5828\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:03:19,952 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5828/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:03:21,614 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5828/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:03:21,615 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5828/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:03:25,362 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5704] due to args.save_total_limit\n",
            "{'loss': 0.2256, 'learning_rate': 0.00010596774193548387, 'epoch': 94.03}\n",
            "                                        {'loss': 0.2157, 'learning_rate': 0.00010580645161290324, 'epoch': 94.19}\n",
            " 47% 5850/12400 [56:24<45:33,  2.40it/s]{'loss': 0.1909, 'learning_rate': 0.00010564516129032258, 'epoch': 94.35}\n",
            " 47% 5860/12400 [56:28<44:46,  2.43it/s]{'loss': 0.2688, 'learning_rate': 0.00010548387096774195, 'epoch': 94.52}\n",
            "{'loss': 0.3261, 'learning_rate': 0.0001053225806451613, 'epoch': 94.68}\n",
            "                                        {'loss': 0.1691, 'learning_rate': 0.00010516129032258064, 'epoch': 94.84}\n",
            "{'loss': 0.2859, 'learning_rate': 0.000105, 'epoch': 95.0}\n",
            " 48% 5890/12400 [56:40<40:20,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 05:03:52,625 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:03:52,625 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:03:52,625 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.75it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.92it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.30it/s]\u001b[A\n",
            "                                        \n",
            " 48% 5890/12400 [56:43<40:20,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.71it/s]\u001b[A{'eval_loss': 2.6538379192352295, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.1022, 'eval_samples_per_second': 70.595, 'eval_steps_per_second': 2.256, 'epoch': 95.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:03:55,730 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5890\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:03:55,732 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5890/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:03:57,407 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5890/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:03:57,408 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5890/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:04:01,059 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5766] due to args.save_total_limit\n",
            "                                        {'loss': 0.1634, 'learning_rate': 0.00010483870967741936, 'epoch': 95.16}\n",
            " 48% 5910/12400 [56:59<44:44,  2.42it/s]{'loss': 0.1672, 'learning_rate': 0.00010467741935483872, 'epoch': 95.32}\n",
            "{'loss': 0.2262, 'learning_rate': 0.00010451612903225806, 'epoch': 95.48}\n",
            "                                        {'loss': 0.271, 'learning_rate': 0.00010435483870967742, 'epoch': 95.65}\n",
            "{'loss': 0.2439, 'learning_rate': 0.00010419354838709678, 'epoch': 95.81}\n",
            "{'loss': 0.2753, 'learning_rate': 0.00010403225806451614, 'epoch': 95.97}\n",
            " 48% 5952/12400 [57:16<41:08,  2.61it/s][INFO|trainer.py:2625] 2022-05-19 05:04:28,269 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:04:28,270 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:04:28,270 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.54it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.44it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.40it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.38it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.39it/s]\u001b[A\n",
            "                                        \n",
            " 48% 5952/12400 [57:19<41:08,  2.61it/s]\n",
            "100% 7/7 [00:00<00:00,  8.81it/s]\u001b[A\n",
            "                                 {'eval_loss': 2.5730161666870117, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.0947, 'eval_samples_per_second': 70.765, 'eval_steps_per_second': 2.262, 'epoch': 96.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 05:04:31,366 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5952\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:04:31,369 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5952/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:04:33,042 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5952/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:04:33,042 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5952/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:04:36,732 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5828] due to args.save_total_limit\n",
            "{'loss': 0.1499, 'learning_rate': 0.0001038709677419355, 'epoch': 96.13}\n",
            "                                        {'loss': 0.2629, 'learning_rate': 0.00010370967741935484, 'epoch': 96.29}\n",
            " 48% 5980/12400 [57:40<45:27,  2.35it/s]{'loss': 0.1904, 'learning_rate': 0.0001035483870967742, 'epoch': 96.45}\n",
            " 48% 5990/12400 [57:44<43:55,  2.43it/s]{'loss': 0.2472, 'learning_rate': 0.00010338709677419356, 'epoch': 96.61}\n",
            " 48% 6000/12400 [57:48<41:42,  2.56it/s]{'loss': 0.2027, 'learning_rate': 0.0001032258064516129, 'epoch': 96.77}\n",
            "{'loss': 0.2886, 'learning_rate': 0.00010306451612903227, 'epoch': 96.94}\n",
            " 48% 6014/12400 [57:53<39:46,  2.68it/s][INFO|trainer.py:2625] 2022-05-19 05:05:05,726 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:05:05,726 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:05:05,726 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.06it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.80it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.59it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.82it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.650381326675415, 'eval_accuracy': 0.4703196347031963, 'eval_runtime': 3.1165, 'eval_samples_per_second': 70.272, 'eval_steps_per_second': 2.246, 'epoch': 97.0}\n",
            " 48% 6014/12400 [57:56<39:46,  2.68it/s]\n",
            "100% 7/7 [00:00<00:00,  8.22it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:05:08,844 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6014\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:05:08,848 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6014/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:05:10,574 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6014/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:05:10,575 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6014/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:05:14,474 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5890] due to args.save_total_limit\n",
            " 49% 6020/12400 [58:07<1:43:05,  1.03it/s]{'loss': 0.2402, 'learning_rate': 0.00010290322580645162, 'epoch': 97.1}\n",
            " 49% 6030/12400 [58:11<47:07,  2.25it/s]{'loss': 0.1884, 'learning_rate': 0.00010274193548387096, 'epoch': 97.26}\n",
            "{'loss': 0.2086, 'learning_rate': 0.00010258064516129033, 'epoch': 97.42}\n",
            " 49% 6050/12400 [58:19<42:30,  2.49it/s]{'loss': 0.1867, 'learning_rate': 0.00010241935483870968, 'epoch': 97.58}\n",
            "                                        {'loss': 0.1965, 'learning_rate': 0.00010225806451612902, 'epoch': 97.74}\n",
            "{'loss': 0.1774, 'learning_rate': 0.0001020967741935484, 'epoch': 97.9}\n",
            " 49% 6076/12400 [58:29<39:55,  2.64it/s][INFO|trainer.py:2625] 2022-05-19 05:05:41,767 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:05:41,767 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:05:41,767 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.85it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.98it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.11it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.7091565132141113, 'eval_accuracy': 0.4657534246575342, 'eval_runtime': 3.0715, 'eval_samples_per_second': 71.301, 'eval_steps_per_second': 2.279, 'epoch': 98.0}\n",
            " 49% 6076/12400 [58:32<39:55,  2.64it/s]\n",
            "100% 7/7 [00:00<00:00,  8.51it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:05:44,840 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6076\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:05:44,844 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6076/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:05:46,509 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6076/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:05:46,510 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6076/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:05:50,059 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-5952] due to args.save_total_limit\n",
            "                                          {'loss': 0.2124, 'learning_rate': 0.00010193548387096774, 'epoch': 98.06}\n",
            "                                        {'loss': 0.2884, 'learning_rate': 0.00010177419354838711, 'epoch': 98.23}\n",
            " 49% 6100/12400 [58:50<42:13,  2.49it/s]{'loss': 0.2028, 'learning_rate': 0.00010161290322580646, 'epoch': 98.39}\n",
            " 49% 6110/12400 [58:54<42:27,  2.47it/s]{'loss': 0.2419, 'learning_rate': 0.0001014516129032258, 'epoch': 98.55}\n",
            "                                        {'loss': 0.1973, 'learning_rate': 0.00010129032258064517, 'epoch': 98.71}\n",
            "{'loss': 0.2021, 'learning_rate': 0.00010112903225806452, 'epoch': 98.87}\n",
            " 50% 6138/12400 [59:05<38:48,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 05:06:17,225 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:06:17,225 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:06:17,225 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.57it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.80it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.94it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.07it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.14it/s]\u001b[A\n",
            "                                        \n",
            " 50% 6138/12400 [59:08<38:48,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.49it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.566579818725586, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.1058, 'eval_samples_per_second': 70.514, 'eval_steps_per_second': 2.254, 'epoch': 99.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:06:20,333 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6138\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:06:20,336 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6138/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:06:22,044 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6138/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:06:22,045 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6138/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:06:25,683 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6014] due to args.save_total_limit\n",
            " 50% 6140/12400 [59:16<4:28:07,  2.57s/it]{'loss': 0.2107, 'learning_rate': 0.00010096774193548389, 'epoch': 99.03}\n",
            "                                        {'loss': 0.2469, 'learning_rate': 0.00010080645161290323, 'epoch': 99.19}\n",
            " 50% 6160/12400 [59:25<43:06,  2.41it/s]{'loss': 0.1533, 'learning_rate': 0.00010064516129032258, 'epoch': 99.35}\n",
            " 50% 6170/12400 [59:29<42:42,  2.43it/s]{'loss': 0.1756, 'learning_rate': 0.00010048387096774195, 'epoch': 99.52}\n",
            "{'loss': 0.3106, 'learning_rate': 0.00010032258064516129, 'epoch': 99.68}\n",
            "                                        {'loss': 0.256, 'learning_rate': 0.00010016129032258067, 'epoch': 99.84}\n",
            "{'loss': 0.2322, 'learning_rate': 0.0001, 'epoch': 100.0}\n",
            " 50% 6200/12400 [59:41<38:51,  2.66it/s][INFO|trainer.py:2625] 2022-05-19 05:06:52,918 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:06:52,918 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:06:52,918 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.29it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.26it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.36it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.33it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.32it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.62593412399292, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.0683, 'eval_samples_per_second': 71.375, 'eval_steps_per_second': 2.281, 'epoch': 100.0}\n",
            " 50% 6200/12400 [59:44<38:51,  2.66it/s]\n",
            "100% 7/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:06:55,988 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6200\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:06:55,991 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6200/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:06:57,649 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6200/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:06:57,650 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6200/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:07:01,266 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6076] due to args.save_total_limit\n",
            "{'loss': 0.1906, 'learning_rate': 9.983870967741936e-05, 'epoch': 100.16}\n",
            "                                        {'loss': 0.2432, 'learning_rate': 9.967741935483872e-05, 'epoch': 100.32}\n",
            "                                          {'loss': 0.1783, 'learning_rate': 9.951612903225806e-05, 'epoch': 100.48}\n",
            " 50% 6240/12400 [1:00:08<43:05,  2.38it/s]{'loss': 0.1866, 'learning_rate': 9.935483870967742e-05, 'epoch': 100.65}\n",
            "                                          {'loss': 0.2222, 'learning_rate': 9.919354838709678e-05, 'epoch': 100.81}\n",
            " 50% 6260/12400 [1:00:15<37:25,  2.73it/s]{'loss': 0.3168, 'learning_rate': 9.903225806451614e-05, 'epoch': 100.97}\n",
            " 50% 6262/12400 [1:00:16<37:25,  2.73it/s][INFO|trainer.py:2625] 2022-05-19 05:07:28,831 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:07:28,831 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:07:28,831 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.57it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.82it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.11it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.6791646480560303, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.1463, 'eval_samples_per_second': 69.606, 'eval_steps_per_second': 2.225, 'epoch': 101.0}\n",
            " 50% 6262/12400 [1:00:20<37:25,  2.73it/s]\n",
            "100% 7/7 [00:00<00:00,  8.49it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:07:31,979 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6262\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:07:31,983 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6262/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:07:33,652 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6262/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:07:33,653 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6262/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:07:37,140 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6138] due to args.save_total_limit\n",
            " 51% 6270/12400 [1:00:30<1:10:03,  1.46it/s]{'loss': 0.1464, 'learning_rate': 9.887096774193549e-05, 'epoch': 101.13}\n",
            "{'loss': 0.1679, 'learning_rate': 9.870967741935484e-05, 'epoch': 101.29}\n",
            " 51% 6290/12400 [1:00:38<41:14,  2.47it/s]{'loss': 0.1722, 'learning_rate': 9.85483870967742e-05, 'epoch': 101.45}\n",
            "{'loss': 0.2834, 'learning_rate': 9.838709677419355e-05, 'epoch': 101.61}\n",
            "                                          {'loss': 0.1917, 'learning_rate': 9.822580645161292e-05, 'epoch': 101.77}\n",
            "{'loss': 0.2921, 'learning_rate': 9.806451612903226e-05, 'epoch': 101.94}\n",
            " 51% 6324/12400 [1:00:52<36:57,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 05:08:04,277 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:08:04,277 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:08:04,277 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.62it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.92it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            "                                          \n",
            " 51% 6324/12400 [1:00:55<36:57,  2.74it/s]\n",
            "{'eval_loss': 2.778499126434326, 'eval_accuracy': 0.5251141552511416, 'eval_runtime': 3.0588, 'eval_samples_per_second': 71.598, 'eval_steps_per_second': 2.289, 'epoch': 102.0}\n",
            "100% 7/7 [00:00<00:00,  8.48it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:08:07,338 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6324\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:08:07,342 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6324/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:08:09,010 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6324/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:08:09,010 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6324/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:08:12,888 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6200] due to args.save_total_limit\n",
            " 51% 6330/12400 [1:01:05<1:38:06,  1.03it/s]{'loss': 0.1821, 'learning_rate': 9.790322580645161e-05, 'epoch': 102.1}\n",
            "{'loss': 0.1561, 'learning_rate': 9.774193548387098e-05, 'epoch': 102.26}\n",
            "{'loss': 0.2307, 'learning_rate': 9.758064516129033e-05, 'epoch': 102.42}\n",
            " 51% 6360/12400 [1:01:17<40:14,  2.50it/s]{'loss': 0.2292, 'learning_rate': 9.741935483870968e-05, 'epoch': 102.58}\n",
            "{'loss': 0.2034, 'learning_rate': 9.725806451612903e-05, 'epoch': 102.74}\n",
            "                                          {'loss': 0.1784, 'learning_rate': 9.709677419354839e-05, 'epoch': 102.9}\n",
            " 52% 6386/12400 [1:01:27<35:07,  2.85it/s][INFO|trainer.py:2625] 2022-05-19 05:08:39,993 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:08:39,994 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:08:39,994 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.23it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            "{'eval_loss': 2.74528431892395, 'eval_accuracy': 0.5068493150684932, 'eval_runtime': 3.0824, 'eval_samples_per_second': 71.048, 'eval_steps_per_second': 2.271, 'epoch': 103.0}\n",
            "\n",
            " 52% 6386/12400 [1:01:31<35:07,  2.85it/s]\n",
            "100% 7/7 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:08:43,078 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6386\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:08:43,081 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6386/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:08:44,748 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6386/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:08:44,748 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6386/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:08:48,197 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6262] due to args.save_total_limit\n",
            "                                            {'loss': 0.1622, 'learning_rate': 9.693548387096774e-05, 'epoch': 103.06}\n",
            " 52% 6400/12400 [1:01:44<45:45,  2.19it/s]{'loss': 0.1775, 'learning_rate': 9.677419354838711e-05, 'epoch': 103.23}\n",
            "{'loss': 0.2048, 'learning_rate': 9.661290322580646e-05, 'epoch': 103.39}\n",
            "{'loss': 0.1217, 'learning_rate': 9.645161290322581e-05, 'epoch': 103.55}\n",
            "{'loss': 0.2043, 'learning_rate': 9.629032258064517e-05, 'epoch': 103.71}\n",
            "                                          {'loss': 0.2759, 'learning_rate': 9.612903225806452e-05, 'epoch': 103.87}\n",
            " 52% 6448/12400 [1:02:02<36:15,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 05:09:15,075 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:09:15,075 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:09:15,075 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.99it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.02it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            "                                          \n",
            "                                 {'eval_loss': 2.612912178039551, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.0686, 'eval_samples_per_second': 71.367, 'eval_steps_per_second': 2.281, 'epoch': 104.0}\n",
            " 52% 6448/12400 [1:02:06<36:15,  2.74it/s]\n",
            "100% 7/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:09:18,146 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6448\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:09:18,149 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6448/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:09:19,820 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6448/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:09:19,821 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6448/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:09:23,453 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6324] due to args.save_total_limit\n",
            " 52% 6450/12400 [1:02:14<4:11:27,  2.54s/it]{'loss': 0.1845, 'learning_rate': 9.596774193548387e-05, 'epoch': 104.03}\n",
            "                                          {'loss': 0.2503, 'learning_rate': 9.580645161290323e-05, 'epoch': 104.19}\n",
            "{'loss': 0.2155, 'learning_rate': 9.564516129032258e-05, 'epoch': 104.35}\n",
            "{'loss': 0.2102, 'learning_rate': 9.548387096774195e-05, 'epoch': 104.52}\n",
            "{'loss': 0.2108, 'learning_rate': 9.53225806451613e-05, 'epoch': 104.68}\n",
            " 52% 6500/12400 [1:02:35<38:11,  2.58it/s]{'loss': 0.1856, 'learning_rate': 9.516129032258065e-05, 'epoch': 104.84}\n",
            "{'loss': 0.2004, 'learning_rate': 9.5e-05, 'epoch': 105.0}\n",
            " 52% 6510/12400 [1:02:38<35:52,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 05:09:50,615 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:09:50,615 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:09:50,615 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.35it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.50it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.27it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.86it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.30it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.4885730743408203, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.0417, 'eval_samples_per_second': 72.0, 'eval_steps_per_second': 2.301, 'epoch': 105.0}\n",
            " 52% 6510/12400 [1:02:41<35:52,  2.74it/s]\n",
            "100% 7/7 [00:01<00:00,  7.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:09:53,659 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6510\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:09:53,662 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6510/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:09:55,330 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6510/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:09:55,331 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6510/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:09:58,993 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6386] due to args.save_total_limit\n",
            "{'loss': 0.1885, 'learning_rate': 9.483870967741936e-05, 'epoch': 105.16}\n",
            " 53% 6530/12400 [1:02:57<41:06,  2.38it/s]{'loss': 0.3116, 'learning_rate': 9.467741935483871e-05, 'epoch': 105.32}\n",
            "                                          {'loss': 0.1953, 'learning_rate': 9.451612903225808e-05, 'epoch': 105.48}\n",
            "{'loss': 0.2004, 'learning_rate': 9.435483870967743e-05, 'epoch': 105.65}\n",
            " 53% 6560/12400 [1:03:10<39:03,  2.49it/s]{'loss': 0.1907, 'learning_rate': 9.419354838709677e-05, 'epoch': 105.81}\n",
            "{'loss': 0.1871, 'learning_rate': 9.403225806451614e-05, 'epoch': 105.97}\n",
            " 53% 6572/12400 [1:03:14<36:03,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 05:10:26,426 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:10:26,426 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:10:26,426 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.08it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.28it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.89it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.34it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.66it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.6451728343963623, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.0104, 'eval_samples_per_second': 72.748, 'eval_steps_per_second': 2.325, 'epoch': 106.0}\n",
            " 53% 6572/12400 [1:03:17<36:03,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.20it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:10:29,438 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6572\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:10:29,441 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6572/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:10:31,108 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6572/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:10:31,109 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6572/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:10:34,935 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6448] due to args.save_total_limit\n",
            "{'loss': 0.1265, 'learning_rate': 9.387096774193549e-05, 'epoch': 106.13}\n",
            " 53% 6590/12400 [1:03:33<42:29,  2.28it/s]{'loss': 0.2121, 'learning_rate': 9.370967741935484e-05, 'epoch': 106.29}\n",
            "{'loss': 0.2236, 'learning_rate': 9.35483870967742e-05, 'epoch': 106.45}\n",
            "{'loss': 0.2042, 'learning_rate': 9.338709677419355e-05, 'epoch': 106.61}\n",
            "{'loss': 0.2236, 'learning_rate': 9.32258064516129e-05, 'epoch': 106.77}\n",
            " 53% 6630/12400 [1:03:49<35:56,  2.68it/s]{'loss': 0.1519, 'learning_rate': 9.306451612903227e-05, 'epoch': 106.94}\n",
            " 54% 6634/12400 [1:03:50<35:06,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 05:11:02,481 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:11:02,481 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:11:02,481 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.17it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.60it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.85it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.01it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.07it/s]\u001b[A\n",
            "100% 7/7 [00:00<00:00,  8.52it/s]\u001b[A{'eval_loss': 2.605072498321533, 'eval_accuracy': 0.5159817351598174, 'eval_runtime': 3.0867, 'eval_samples_per_second': 70.949, 'eval_steps_per_second': 2.268, 'epoch': 107.0}\n",
            "                                          \n",
            " 54% 6634/12400 [1:03:53<35:06,  2.74it/s]\n",
            "100% 7/7 [00:00<00:00,  8.52it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:11:05,569 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6634\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:11:05,572 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6634/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:11:07,242 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6634/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:11:07,243 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6634/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:11:10,780 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6510] due to args.save_total_limit\n",
            " 54% 6640/12400 [1:04:03<1:29:37,  1.07it/s]{'loss': 0.1291, 'learning_rate': 9.290322580645162e-05, 'epoch': 107.1}\n",
            "                                          {'loss': 0.2486, 'learning_rate': 9.274193548387096e-05, 'epoch': 107.26}\n",
            " 54% 6660/12400 [1:04:11<38:55,  2.46it/s]{'loss': 0.2334, 'learning_rate': 9.258064516129033e-05, 'epoch': 107.42}\n",
            "{'loss': 0.2118, 'learning_rate': 9.241935483870968e-05, 'epoch': 107.58}\n",
            " 54% 6680/12400 [1:04:19<39:02,  2.44it/s]{'loss': 0.1829, 'learning_rate': 9.225806451612904e-05, 'epoch': 107.74}\n",
            "{'loss': 0.2552, 'learning_rate': 9.20967741935484e-05, 'epoch': 107.9}\n",
            " 54% 6696/12400 [1:04:25<34:27,  2.76it/s][INFO|trainer.py:2625] 2022-05-19 05:11:37,871 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:11:37,871 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:11:37,871 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.53it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.96it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.65it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.13it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.48it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.638660430908203, 'eval_accuracy': 0.5159817351598174, 'eval_runtime': 3.0738, 'eval_samples_per_second': 71.247, 'eval_steps_per_second': 2.277, 'epoch': 108.0}\n",
            " 54% 6696/12400 [1:04:29<34:27,  2.76it/s]\n",
            "100% 7/7 [00:00<00:00,  8.03it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:11:40,947 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6696\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:11:40,950 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6696/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:11:42,627 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6696/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:11:42,628 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6696/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:11:46,268 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6572] due to args.save_total_limit\n",
            "{'loss': 0.2354, 'learning_rate': 9.193548387096774e-05, 'epoch': 108.06}\n",
            " 54% 6710/12400 [1:04:42<44:30,  2.13it/s]{'loss': 0.1278, 'learning_rate': 9.17741935483871e-05, 'epoch': 108.23}\n",
            "{'loss': 0.1572, 'learning_rate': 9.161290322580646e-05, 'epoch': 108.39}\n",
            "{'loss': 0.202, 'learning_rate': 9.145161290322581e-05, 'epoch': 108.55}\n",
            "{'loss': 0.156, 'learning_rate': 9.129032258064517e-05, 'epoch': 108.71}\n",
            "                                          {'loss': 0.1783, 'learning_rate': 9.112903225806452e-05, 'epoch': 108.87}\n",
            " 55% 6758/12400 [1:05:01<35:20,  2.66it/s][INFO|trainer.py:2625] 2022-05-19 05:12:13,847 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:12:13,848 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:12:13,848 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.32it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.15it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.14it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7103919982910156, 'eval_accuracy': 0.5068493150684932, 'eval_runtime': 3.0663, 'eval_samples_per_second': 71.421, 'eval_steps_per_second': 2.283, 'epoch': 109.0}\n",
            " 55% 6758/12400 [1:05:05<35:20,  2.66it/s]\n",
            "100% 7/7 [00:00<00:00,  8.45it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:12:16,916 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6758\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:12:16,919 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6758/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:12:18,591 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6758/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:12:18,592 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6758/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:12:22,287 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6634] due to args.save_total_limit\n",
            " 55% 6760/12400 [1:05:12<4:01:20,  2.57s/it]{'loss': 0.2536, 'learning_rate': 9.096774193548387e-05, 'epoch': 109.03}\n",
            "{'loss': 0.1698, 'learning_rate': 9.080645161290323e-05, 'epoch': 109.19}\n",
            " 55% 6780/12400 [1:05:21<38:55,  2.41it/s]{'loss': 0.126, 'learning_rate': 9.06451612903226e-05, 'epoch': 109.35}\n",
            " 55% 6790/12400 [1:05:25<37:51,  2.47it/s]{'loss': 0.1861, 'learning_rate': 9.048387096774193e-05, 'epoch': 109.52}\n",
            "                                          {'loss': 0.1747, 'learning_rate': 9.032258064516129e-05, 'epoch': 109.68}\n",
            "                                          {'loss': 0.1875, 'learning_rate': 9.016129032258065e-05, 'epoch': 109.84}\n",
            "                                          {'loss': 0.1786, 'learning_rate': 9e-05, 'epoch': 110.0}\n",
            " 55% 6820/12400 [1:05:37<33:50,  2.75it/s][INFO|trainer.py:2625] 2022-05-19 05:12:49,414 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:12:49,415 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:12:49,415 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.57it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.82it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.97it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7063584327697754, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.0682, 'eval_samples_per_second': 71.376, 'eval_steps_per_second': 2.281, 'epoch': 110.0}\n",
            " 55% 6820/12400 [1:05:40<33:50,  2.75it/s]\n",
            "100% 7/7 [00:00<00:00,  8.61it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:12:52,485 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6820\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:12:52,488 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6820/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:12:54,166 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6820/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:12:54,167 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6820/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:12:57,824 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6696] due to args.save_total_limit\n",
            "{'loss': 0.1519, 'learning_rate': 8.983870967741936e-05, 'epoch': 110.16}\n",
            "{'loss': 0.1291, 'learning_rate': 8.967741935483871e-05, 'epoch': 110.32}\n",
            "{'loss': 0.3208, 'learning_rate': 8.951612903225806e-05, 'epoch': 110.48}\n",
            "                                          {'loss': 0.1067, 'learning_rate': 8.935483870967742e-05, 'epoch': 110.65}\n",
            " 55% 6870/12400 [1:06:08<35:07,  2.62it/s]{'loss': 0.1803, 'learning_rate': 8.919354838709678e-05, 'epoch': 110.81}\n",
            "                                          {'loss': 0.2197, 'learning_rate': 8.903225806451614e-05, 'epoch': 110.97}\n",
            " 56% 6882/12400 [1:06:12<34:43,  2.65it/s][INFO|trainer.py:2625] 2022-05-19 05:13:24,991 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:13:24,991 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:13:24,991 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.26it/s]\u001b[A\n",
            "                                          \n",
            "                                 {'eval_loss': 2.6894307136535645, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.0708, 'eval_samples_per_second': 71.317, 'eval_steps_per_second': 2.28, 'epoch': 111.0}\n",
            " 56% 6882/12400 [1:06:16<34:43,  2.65it/s]\n",
            "100% 7/7 [00:00<00:00,  8.56it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:13:28,064 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6882\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:13:28,068 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6882/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:13:29,743 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6882/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:13:29,743 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6882/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:13:33,433 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6758] due to args.save_total_limit\n",
            "                                            {'loss': 0.1193, 'learning_rate': 8.887096774193549e-05, 'epoch': 111.13}\n",
            "                                          {'loss': 0.1282, 'learning_rate': 8.870967741935484e-05, 'epoch': 111.29}\n",
            "{'loss': 0.1235, 'learning_rate': 8.85483870967742e-05, 'epoch': 111.45}\n",
            "{'loss': 0.2568, 'learning_rate': 8.838709677419355e-05, 'epoch': 111.61}\n",
            " 56% 6930/12400 [1:06:43<36:07,  2.52it/s]{'loss': 0.1853, 'learning_rate': 8.82258064516129e-05, 'epoch': 111.77}\n",
            "                                          {'loss': 0.2334, 'learning_rate': 8.806451612903226e-05, 'epoch': 111.94}\n",
            " 56% 6944/12400 [1:06:48<32:56,  2.76it/s][INFO|trainer.py:2625] 2022-05-19 05:14:00,584 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:14:00,584 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:14:00,584 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.58it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.90it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.61it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.11it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.48it/s]\u001b[A\n",
            "                                          \n",
            " 56% 6944/12400 [1:06:51<32:56,  2.76it/s]\n",
            "100% 7/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.689336061477661, 'eval_accuracy': 0.5068493150684932, 'eval_runtime': 3.0299, 'eval_samples_per_second': 72.279, 'eval_steps_per_second': 2.31, 'epoch': 112.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:14:03,616 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6944\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:14:03,619 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6944/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:14:05,280 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6944/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:14:05,281 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6944/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:14:08,782 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6820] due to args.save_total_limit\n",
            "{'loss': 0.1915, 'learning_rate': 8.790322580645162e-05, 'epoch': 112.1}\n",
            " 56% 6960/12400 [1:07:05<41:03,  2.21it/s]{'loss': 0.151, 'learning_rate': 8.774193548387098e-05, 'epoch': 112.26}\n",
            "{'loss': 0.1835, 'learning_rate': 8.758064516129033e-05, 'epoch': 112.42}\n",
            " 56% 6980/12400 [1:07:13<36:42,  2.46it/s]{'loss': 0.2121, 'learning_rate': 8.741935483870968e-05, 'epoch': 112.58}\n",
            " 56% 6990/12400 [1:07:17<37:21,  2.41it/s]{'loss': 0.1766, 'learning_rate': 8.725806451612904e-05, 'epoch': 112.74}\n",
            " 56% 7000/12400 [1:07:21<35:14,  2.55it/s]{'loss': 0.2248, 'learning_rate': 8.709677419354839e-05, 'epoch': 112.9}\n",
            " 56% 7006/12400 [1:07:24<34:21,  2.62it/s][INFO|trainer.py:2625] 2022-05-19 05:14:36,274 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:14:36,274 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:14:36,274 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.16it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.23it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.32it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.740659713745117, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.1201, 'eval_samples_per_second': 70.19, 'eval_steps_per_second': 2.244, 'epoch': 113.0}\n",
            " 56% 7006/12400 [1:07:27<34:21,  2.62it/s]\n",
            "100% 7/7 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:14:39,396 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7006\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:14:39,399 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7006/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:14:41,040 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7006/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:14:41,041 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7006/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:14:44,541 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6882] due to args.save_total_limit\n",
            "{'loss': 0.1212, 'learning_rate': 8.693548387096776e-05, 'epoch': 113.06}\n",
            "                                          {'loss': 0.1336, 'learning_rate': 8.677419354838711e-05, 'epoch': 113.23}\n",
            " 57% 7030/12400 [1:07:44<36:08,  2.48it/s]{'loss': 0.1952, 'learning_rate': 8.661290322580645e-05, 'epoch': 113.39}\n",
            " 57% 7040/12400 [1:07:48<36:38,  2.44it/s]{'loss': 0.1805, 'learning_rate': 8.645161290322581e-05, 'epoch': 113.55}\n",
            " 57% 7050/12400 [1:07:52<36:44,  2.43it/s]{'loss': 0.1447, 'learning_rate': 8.629032258064517e-05, 'epoch': 113.71}\n",
            "{'loss': 0.1757, 'learning_rate': 8.612903225806452e-05, 'epoch': 113.87}\n",
            " 57% 7068/12400 [1:07:59<32:50,  2.71it/s][INFO|trainer.py:2625] 2022-05-19 05:15:11,795 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:15:11,795 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:15:11,795 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.77it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7831242084503174, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.0584, 'eval_samples_per_second': 71.607, 'eval_steps_per_second': 2.289, 'epoch': 114.0}\n",
            " 57% 7068/12400 [1:08:02<32:50,  2.71it/s]\n",
            "100% 7/7 [00:00<00:00,  8.57it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:15:14,856 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7068\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:15:14,859 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7068/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:15:16,513 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7068/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:15:16,514 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7068/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:15:20,165 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-6944] due to args.save_total_limit\n",
            " 57% 7070/12400 [1:08:10<3:46:34,  2.55s/it]{'loss': 0.1513, 'learning_rate': 8.596774193548387e-05, 'epoch': 114.03}\n",
            "{'loss': 0.1808, 'learning_rate': 8.580645161290323e-05, 'epoch': 114.19}\n",
            " 57% 7090/12400 [1:08:19<36:19,  2.44it/s]{'loss': 0.1214, 'learning_rate': 8.564516129032258e-05, 'epoch': 114.35}\n",
            "{'loss': 0.194, 'learning_rate': 8.548387096774195e-05, 'epoch': 114.52}\n",
            " 57% 7110/12400 [1:08:27<35:45,  2.47it/s]{'loss': 0.1579, 'learning_rate': 8.53225806451613e-05, 'epoch': 114.68}\n",
            "                                          {'loss': 0.1317, 'learning_rate': 8.516129032258064e-05, 'epoch': 114.84}\n",
            "{'loss': 0.1792, 'learning_rate': 8.5e-05, 'epoch': 115.0}\n",
            " 57% 7130/12400 [1:08:35<33:02,  2.66it/s][INFO|trainer.py:2625] 2022-05-19 05:15:47,458 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:15:47,458 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:15:47,458 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.85it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.38it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.69it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.90it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.02it/s]\u001b[A\n",
            "{'eval_loss': 2.7243573665618896, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.1483, 'eval_samples_per_second': 69.561, 'eval_steps_per_second': 2.223, 'epoch': 115.0}\n",
            "\n",
            " 57% 7130/12400 [1:08:38<33:02,  2.66it/s]\n",
            "100% 7/7 [00:00<00:00,  8.41it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:15:50,609 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7130\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:15:50,612 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7130/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:15:52,288 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7130/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:15:52,290 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7130/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:15:55,863 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7006] due to args.save_total_limit\n",
            " 58% 7140/12400 [1:08:50<48:59,  1.79it/s]{'loss': 0.0935, 'learning_rate': 8.483870967741936e-05, 'epoch': 115.16}\n",
            "                                          {'loss': 0.179, 'learning_rate': 8.467741935483871e-05, 'epoch': 115.32}\n",
            " 58% 7160/12400 [1:08:58<35:56,  2.43it/s]{'loss': 0.1679, 'learning_rate': 8.451612903225808e-05, 'epoch': 115.48}\n",
            " 58% 7170/12400 [1:09:02<35:55,  2.43it/s]{'loss': 0.1558, 'learning_rate': 8.435483870967742e-05, 'epoch': 115.65}\n",
            "                                          {'loss': 0.1714, 'learning_rate': 8.419354838709677e-05, 'epoch': 115.81}\n",
            " 58% 7190/12400 [1:09:10<31:53,  2.72it/s]{'loss': 0.2575, 'learning_rate': 8.403225806451614e-05, 'epoch': 115.97}\n",
            " 58% 7192/12400 [1:09:11<32:43,  2.65it/s][INFO|trainer.py:2625] 2022-05-19 05:16:23,252 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:16:23,252 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:16:23,252 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.69it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.63it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.85it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.02it/s]\u001b[A\n",
            "                                          \n",
            " 58% 7192/12400 [1:09:14<32:43,  2.65it/s]\n",
            "{'eval_loss': 2.7428691387176514, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.1005, 'eval_samples_per_second': 70.633, 'eval_steps_per_second': 2.258, 'epoch': 116.0}\n",
            "100% 7/7 [00:00<00:00,  8.37it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:16:26,355 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7192\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:16:26,358 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7192/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:16:28,015 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7192/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:16:28,016 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7192/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:16:31,598 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7068] due to args.save_total_limit\n",
            "{'loss': 0.2386, 'learning_rate': 8.387096774193549e-05, 'epoch': 116.13}\n",
            "                                          {'loss': 0.1275, 'learning_rate': 8.370967741935483e-05, 'epoch': 116.29}\n",
            " 58% 7220/12400 [1:09:33<34:40,  2.49it/s]{'loss': 0.212, 'learning_rate': 8.35483870967742e-05, 'epoch': 116.45}\n",
            "                                          {'loss': 0.2005, 'learning_rate': 8.338709677419355e-05, 'epoch': 116.61}\n",
            " 58% 7240/12400 [1:09:41<34:44,  2.48it/s]{'loss': 0.2244, 'learning_rate': 8.32258064516129e-05, 'epoch': 116.77}\n",
            " 58% 7250/12400 [1:09:45<31:49,  2.70it/s]{'loss': 0.2051, 'learning_rate': 8.306451612903227e-05, 'epoch': 116.94}\n",
            " 58% 7254/12400 [1:09:46<32:38,  2.63it/s][INFO|trainer.py:2625] 2022-05-19 05:16:58,846 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:16:58,846 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:16:58,846 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.37it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.39it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.35it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.586216926574707, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.07, 'eval_samples_per_second': 71.335, 'eval_steps_per_second': 2.28, 'epoch': 117.0}\n",
            " 58% 7254/12400 [1:09:50<32:38,  2.63it/s]\n",
            "100% 7/7 [00:00<00:00,  8.61it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:17:01,918 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7254\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:17:01,921 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7254/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:17:03,571 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7254/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:17:03,572 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7254/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:17:07,031 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7130] due to args.save_total_limit\n",
            "{'loss': 0.2153, 'learning_rate': 8.290322580645161e-05, 'epoch': 117.1}\n",
            " 59% 7270/12400 [1:10:03<37:56,  2.25it/s]{'loss': 0.1748, 'learning_rate': 8.274193548387096e-05, 'epoch': 117.26}\n",
            " 59% 7280/12400 [1:10:07<35:10,  2.43it/s]{'loss': 0.1346, 'learning_rate': 8.258064516129033e-05, 'epoch': 117.42}\n",
            "                                          {'loss': 0.247, 'learning_rate': 8.241935483870968e-05, 'epoch': 117.58}\n",
            "{'loss': 0.1585, 'learning_rate': 8.225806451612904e-05, 'epoch': 117.74}\n",
            "                                          {'loss': 0.103, 'learning_rate': 8.209677419354839e-05, 'epoch': 117.9}\n",
            " 59% 7316/12400 [1:10:22<31:20,  2.70it/s][INFO|trainer.py:2625] 2022-05-19 05:17:34,182 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:17:34,182 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:17:34,182 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.12it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.41it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.03it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.43it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.69it/s]\u001b[A\n",
            "                                          \n",
            " 59% 7316/12400 [1:10:25<31:20,  2.70it/s]\n",
            "100% 7/7 [00:00<00:00,  8.11it/s]\u001b[A\n",
            "{'eval_loss': 2.667973756790161, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.1657, 'eval_samples_per_second': 69.178, 'eval_steps_per_second': 2.211, 'epoch': 118.0}\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:17:37,350 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7316\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:17:37,354 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7316/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:17:39,026 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7316/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:17:39,027 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7316/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:17:42,645 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7192] due to args.save_total_limit\n",
            " 59% 7320/12400 [1:10:34<2:07:05,  1.50s/it]{'loss': 0.2147, 'learning_rate': 8.193548387096774e-05, 'epoch': 118.06}\n",
            "                                          {'loss': 0.2311, 'learning_rate': 8.17741935483871e-05, 'epoch': 118.23}\n",
            "{'loss': 0.1772, 'learning_rate': 8.161290322580646e-05, 'epoch': 118.39}\n",
            " 59% 7350/12400 [1:10:47<35:00,  2.40it/s]{'loss': 0.1698, 'learning_rate': 8.145161290322582e-05, 'epoch': 118.55}\n",
            " 59% 7360/12400 [1:10:51<34:41,  2.42it/s]{'loss': 0.178, 'learning_rate': 8.129032258064517e-05, 'epoch': 118.71}\n",
            " 59% 7370/12400 [1:10:55<32:09,  2.61it/s]{'loss': 0.1565, 'learning_rate': 8.112903225806452e-05, 'epoch': 118.87}\n",
            " 60% 7378/12400 [1:10:57<30:38,  2.73it/s][INFO|trainer.py:2625] 2022-05-19 05:18:10,113 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:18:10,114 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:18:10,114 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.60it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.70it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.43it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.34it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 2.626892566680908, 'eval_accuracy': 0.502283105022831, 'eval_runtime': 3.1786, 'eval_samples_per_second': 68.898, 'eval_steps_per_second': 2.202, 'epoch': 119.0}\n",
            " 60% 7378/12400 [1:11:01<30:38,  2.73it/s]\n",
            "100% 7/7 [00:00<00:00,  7.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:18:13,295 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7378\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:18:13,298 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7378/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:18:15,010 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7378/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:18:15,011 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7378/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:18:18,543 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7254] due to args.save_total_limit\n",
            " 60% 7380/12400 [1:11:09<3:33:35,  2.55s/it]{'loss': 0.1377, 'learning_rate': 8.096774193548387e-05, 'epoch': 119.03}\n",
            " 60% 7390/12400 [1:11:13<42:07,  1.98it/s]{'loss': 0.1613, 'learning_rate': 8.080645161290323e-05, 'epoch': 119.19}\n",
            "                                          {'loss': 0.0867, 'learning_rate': 8.064516129032258e-05, 'epoch': 119.35}\n",
            "{'loss': 0.1832, 'learning_rate': 8.048387096774193e-05, 'epoch': 119.52}\n",
            " 60% 7420/12400 [1:11:26<34:15,  2.42it/s]{'loss': 0.1249, 'learning_rate': 8.03225806451613e-05, 'epoch': 119.68}\n",
            "{'loss': 0.1135, 'learning_rate': 8.016129032258065e-05, 'epoch': 119.84}\n",
            "                                          {'loss': 0.2095, 'learning_rate': 8e-05, 'epoch': 120.0}\n",
            " 60% 7440/12400 [1:11:33<30:15,  2.73it/s][INFO|trainer.py:2625] 2022-05-19 05:18:45,883 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:18:45,883 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:18:45,883 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.94it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.30it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.30it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 2.727788209915161, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.0564, 'eval_samples_per_second': 71.654, 'eval_steps_per_second': 2.29, 'epoch': 120.0}\n",
            " 60% 7440/12400 [1:11:37<30:15,  2.73it/s]\n",
            "100% 7/7 [00:00<00:00,  8.63it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:18:48,941 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7440\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:18:48,944 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7440/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:18:50,589 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7440/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:18:50,590 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7440/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:18:54,111 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7316] due to args.save_total_limit\n",
            " 60% 7450/12400 [1:11:48<45:13,  1.82it/s]{'loss': 0.1251, 'learning_rate': 7.983870967741936e-05, 'epoch': 120.16}\n",
            " 60% 7460/12400 [1:11:52<33:57,  2.42it/s]{'loss': 0.2049, 'learning_rate': 7.967741935483871e-05, 'epoch': 120.32}\n",
            " 60% 7470/12400 [1:11:56<33:36,  2.44it/s]{'loss': 0.1485, 'learning_rate': 7.951612903225807e-05, 'epoch': 120.48}\n",
            "                                          {'loss': 0.1638, 'learning_rate': 7.935483870967743e-05, 'epoch': 120.65}\n",
            "                                          {'loss': 0.1731, 'learning_rate': 7.919354838709679e-05, 'epoch': 120.81}\n",
            " 60% 7500/12400 [1:12:08<29:46,  2.74it/s]{'loss': 0.191, 'learning_rate': 7.903225806451613e-05, 'epoch': 120.97}\n",
            " 60% 7502/12400 [1:12:08<29:44,  2.74it/s][INFO|trainer.py:2625] 2022-05-19 05:19:21,114 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:19:21,114 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:19:21,114 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.09it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.35it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.95it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.34it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.68it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.805978775024414, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.0089, 'eval_samples_per_second': 72.785, 'eval_steps_per_second': 2.326, 'epoch': 121.0}\n",
            " 60% 7502/12400 [1:12:12<29:44,  2.74it/s]\n",
            "100% 7/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:19:24,125 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7502\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:19:24,128 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7502/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:19:25,802 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7502/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:19:25,803 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7502/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:19:29,415 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7378] due to args.save_total_limit\n",
            " 61% 7510/12400 [1:12:22<55:42,  1.46it/s]{'loss': 0.1089, 'learning_rate': 7.887096774193549e-05, 'epoch': 121.13}\n",
            "{'loss': 0.1573, 'learning_rate': 7.870967741935484e-05, 'epoch': 121.29}\n",
            "                                          {'loss': 0.1618, 'learning_rate': 7.85483870967742e-05, 'epoch': 121.45}\n",
            "{'loss': 0.1824, 'learning_rate': 7.838709677419355e-05, 'epoch': 121.61}\n",
            " 61% 7550/12400 [1:12:39<32:27,  2.49it/s]{'loss': 0.141, 'learning_rate': 7.82258064516129e-05, 'epoch': 121.77}\n",
            "{'loss': 0.1052, 'learning_rate': 7.806451612903226e-05, 'epoch': 121.94}\n",
            " 61% 7564/12400 [1:12:44<29:30,  2.73it/s][INFO|trainer.py:2625] 2022-05-19 05:19:56,492 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:19:56,492 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:19:56,492 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  5.98it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.38it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.67it/s]\u001b[A\n",
            "                                          \n",
            " 61% 7564/12400 [1:12:47<29:30,  2.73it/s]\n",
            "100% 7/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:19:59,524 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7564\n",
            "{'eval_loss': 2.7155351638793945, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.0301, 'eval_samples_per_second': 72.275, 'eval_steps_per_second': 2.31, 'epoch': 122.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:19:59,527 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7564/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:20:01,181 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7564/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:20:01,182 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7564/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:20:04,841 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7440] due to args.save_total_limit\n",
            "{'loss': 0.1519, 'learning_rate': 7.790322580645162e-05, 'epoch': 122.1}\n",
            " 61% 7580/12400 [1:13:01<36:30,  2.20it/s]{'loss': 0.1159, 'learning_rate': 7.774193548387098e-05, 'epoch': 122.26}\n",
            "{'loss': 0.0916, 'learning_rate': 7.758064516129032e-05, 'epoch': 122.42}\n",
            "{'loss': 0.1337, 'learning_rate': 7.741935483870968e-05, 'epoch': 122.58}\n",
            "{'loss': 0.1737, 'learning_rate': 7.725806451612904e-05, 'epoch': 122.74}\n",
            "{'loss': 0.1605, 'learning_rate': 7.709677419354839e-05, 'epoch': 122.9}\n",
            " 62% 7626/12400 [1:13:20<29:34,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 05:20:32,183 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:20:32,183 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:20:32,183 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.81it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.00it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.10it/s]\u001b[A\n",
            "                                          \n",
            " 62% 7626/12400 [1:13:23<29:34,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.46it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:20:35,280 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7626\n",
            "{'eval_loss': 2.838111639022827, 'eval_accuracy': 0.4931506849315068, 'eval_runtime': 3.0952, 'eval_samples_per_second': 70.755, 'eval_steps_per_second': 2.262, 'epoch': 123.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:20:35,284 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7626/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:20:36,946 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7626/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:20:36,947 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7626/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:20:40,800 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7502] due to args.save_total_limit\n",
            "{'loss': 0.1601, 'learning_rate': 7.693548387096776e-05, 'epoch': 123.06}\n",
            "{'loss': 0.1562, 'learning_rate': 7.67741935483871e-05, 'epoch': 123.23}\n",
            " 62% 7650/12400 [1:13:41<31:54,  2.48it/s]{'loss': 0.1216, 'learning_rate': 7.661290322580645e-05, 'epoch': 123.39}\n",
            "{'loss': 0.1879, 'learning_rate': 7.645161290322582e-05, 'epoch': 123.55}\n",
            " 62% 7670/12400 [1:13:48<32:42,  2.41it/s]{'loss': 0.1062, 'learning_rate': 7.629032258064517e-05, 'epoch': 123.71}\n",
            " 62% 7680/12400 [1:13:52<29:59,  2.62it/s]{'loss': 0.1734, 'learning_rate': 7.612903225806451e-05, 'epoch': 123.87}\n",
            " 62% 7688/12400 [1:13:55<28:48,  2.73it/s][INFO|trainer.py:2625] 2022-05-19 05:21:07,966 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:21:07,966 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:21:07,966 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.98it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.16it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7617850303649902, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.0589, 'eval_samples_per_second': 71.594, 'eval_steps_per_second': 2.288, 'epoch': 124.0}\n",
            " 62% 7688/12400 [1:13:59<28:48,  2.73it/s]\n",
            "100% 7/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:21:11,027 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7688\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:21:11,030 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7688/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:21:12,688 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7688/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:21:12,690 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7688/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:21:16,324 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7564] due to args.save_total_limit\n",
            " 62% 7690/12400 [1:14:06<3:19:15,  2.54s/it]{'loss': 0.1893, 'learning_rate': 7.596774193548387e-05, 'epoch': 124.03}\n",
            "{'loss': 0.1187, 'learning_rate': 7.580645161290323e-05, 'epoch': 124.19}\n",
            " 62% 7710/12400 [1:14:15<32:22,  2.41it/s]{'loss': 0.1384, 'learning_rate': 7.564516129032258e-05, 'epoch': 124.35}\n",
            " 62% 7720/12400 [1:14:19<32:06,  2.43it/s]{'loss': 0.1246, 'learning_rate': 7.548387096774195e-05, 'epoch': 124.52}\n",
            "                                          {'loss': 0.1838, 'learning_rate': 7.532258064516129e-05, 'epoch': 124.68}\n",
            " 62% 7740/12400 [1:14:27<30:49,  2.52it/s]{'loss': 0.14, 'learning_rate': 7.516129032258064e-05, 'epoch': 124.84}\n",
            " 62% 7750/12400 [1:14:31<27:45,  2.79it/s]{'loss': 0.1619, 'learning_rate': 7.500000000000001e-05, 'epoch': 125.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 05:21:43,508 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:21:43,508 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:21:43,508 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.70it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.81it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.03it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            "                                          \n",
            " 62% 7750/12400 [1:14:34<27:45,  2.79it/s]\n",
            "100% 7/7 [00:00<00:00,  8.67it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:21:46,623 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7750\n",
            "{'eval_loss': 2.6193161010742188, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.1124, 'eval_samples_per_second': 70.363, 'eval_steps_per_second': 2.249, 'epoch': 125.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:21:46,626 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7750/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:21:48,313 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7750/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:21:48,314 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7750/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:21:51,871 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7626] due to args.save_total_limit\n",
            "{'loss': 0.1453, 'learning_rate': 7.483870967741936e-05, 'epoch': 125.16}\n",
            " 63% 7770/12400 [1:14:50<32:21,  2.38it/s]{'loss': 0.1429, 'learning_rate': 7.467741935483871e-05, 'epoch': 125.32}\n",
            "{'loss': 0.1835, 'learning_rate': 7.451612903225807e-05, 'epoch': 125.48}\n",
            " 63% 7790/12400 [1:14:58<31:03,  2.47it/s]{'loss': 0.1939, 'learning_rate': 7.435483870967742e-05, 'epoch': 125.65}\n",
            "                                          {'loss': 0.0939, 'learning_rate': 7.419354838709677e-05, 'epoch': 125.81}\n",
            "{'loss': 0.1415, 'learning_rate': 7.403225806451614e-05, 'epoch': 125.97}\n",
            " 63% 7812/12400 [1:15:06<27:42,  2.76it/s][INFO|trainer.py:2625] 2022-05-19 05:22:18,804 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:22:18,804 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:22:18,804 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.59it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.83it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.97it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 2.61922287940979, 'eval_accuracy': 0.5251141552511416, 'eval_runtime': 3.0922, 'eval_samples_per_second': 70.823, 'eval_steps_per_second': 2.264, 'epoch': 126.0}\n",
            " 63% 7812/12400 [1:15:09<27:42,  2.76it/s]\n",
            "100% 7/7 [00:00<00:00,  8.54it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:22:21,898 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7812\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:22:21,901 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7812/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:22:23,552 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7812/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:22:23,553 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7812/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:22:27,130 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7688] due to args.save_total_limit\n",
            " 63% 7820/12400 [1:15:20<51:48,  1.47it/s]{'loss': 0.1059, 'learning_rate': 7.387096774193549e-05, 'epoch': 126.13}\n",
            "{'loss': 0.1734, 'learning_rate': 7.370967741935485e-05, 'epoch': 126.29}\n",
            "                                          {'loss': 0.1652, 'learning_rate': 7.35483870967742e-05, 'epoch': 126.45}\n",
            " 63% 7850/12400 [1:15:32<30:54,  2.45it/s]{'loss': 0.1103, 'learning_rate': 7.338709677419355e-05, 'epoch': 126.61}\n",
            " 63% 7860/12400 [1:15:36<29:40,  2.55it/s]{'loss': 0.1435, 'learning_rate': 7.32258064516129e-05, 'epoch': 126.77}\n",
            "{'loss': 0.11, 'learning_rate': 7.306451612903226e-05, 'epoch': 126.94}\n",
            " 64% 7874/12400 [1:15:42<28:53,  2.61it/s][INFO|trainer.py:2625] 2022-05-19 05:22:54,182 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:22:54,182 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:22:54,182 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.03it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.10it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.22it/s]\u001b[A\n",
            "                                          \n",
            " 64% 7874/12400 [1:15:45<28:53,  2.61it/s]\n",
            "{'eval_loss': 2.6422431468963623, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.0833, 'eval_samples_per_second': 71.027, 'eval_steps_per_second': 2.27, 'epoch': 127.0}\n",
            "100% 7/7 [00:00<00:00,  8.55it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:22:57,267 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7874\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:22:57,270 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7874/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:22:58,962 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7874/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:22:58,963 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7874/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:23:02,579 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7750] due to args.save_total_limit\n",
            " 64% 7880/12400 [1:15:55<1:11:57,  1.05it/s]{'loss': 0.1115, 'learning_rate': 7.290322580645161e-05, 'epoch': 127.1}\n",
            "{'loss': 0.1673, 'learning_rate': 7.274193548387098e-05, 'epoch': 127.26}\n",
            "{'loss': 0.1295, 'learning_rate': 7.258064516129033e-05, 'epoch': 127.42}\n",
            "                                          {'loss': 0.1611, 'learning_rate': 7.241935483870968e-05, 'epoch': 127.58}\n",
            "{'loss': 0.1536, 'learning_rate': 7.225806451612904e-05, 'epoch': 127.74}\n",
            "{'loss': 0.0945, 'learning_rate': 7.209677419354839e-05, 'epoch': 127.9}\n",
            " 64% 7936/12400 [1:16:17<27:48,  2.68it/s][INFO|trainer.py:2625] 2022-05-19 05:23:29,828 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:23:29,828 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:23:29,829 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.61it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.87it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.20it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.8019514083862305, 'eval_accuracy': 0.5159817351598174, 'eval_runtime': 3.0613, 'eval_samples_per_second': 71.539, 'eval_steps_per_second': 2.287, 'epoch': 128.0}\n",
            " 64% 7936/12400 [1:16:20<27:48,  2.68it/s]\n",
            "100% 7/7 [00:00<00:00,  8.61it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:23:32,892 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7936\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:23:32,895 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7936/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:23:34,573 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7936/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:23:34,573 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7936/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:23:38,197 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7812] due to args.save_total_limit\n",
            "{'loss': 0.2361, 'learning_rate': 7.193548387096774e-05, 'epoch': 128.06}\n",
            "{'loss': 0.1077, 'learning_rate': 7.177419354838711e-05, 'epoch': 128.23}\n",
            "{'loss': 0.0905, 'learning_rate': 7.161290322580646e-05, 'epoch': 128.39}\n",
            "{'loss': 0.0893, 'learning_rate': 7.14516129032258e-05, 'epoch': 128.55}\n",
            "{'loss': 0.2267, 'learning_rate': 7.129032258064517e-05, 'epoch': 128.71}\n",
            "                                          {'loss': 0.1563, 'learning_rate': 7.112903225806452e-05, 'epoch': 128.87}\n",
            " 64% 7998/12400 [1:16:53<27:40,  2.65it/s][INFO|trainer.py:2625] 2022-05-19 05:24:05,359 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:24:05,359 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:24:05,359 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.09it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.25it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.89it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.35it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.65it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.756502389907837, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.0397, 'eval_samples_per_second': 72.047, 'eval_steps_per_second': 2.303, 'epoch': 129.0}\n",
            " 64% 7998/12400 [1:16:56<27:40,  2.65it/s]\n",
            "100% 7/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:24:08,400 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7998\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:24:08,403 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7998/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:24:10,053 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7998/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:24:10,054 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7998/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:24:13,900 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7874] due to args.save_total_limit\n",
            " 65% 8000/12400 [1:17:04<3:09:51,  2.59s/it]{'loss': 0.2077, 'learning_rate': 7.096774193548388e-05, 'epoch': 129.03}\n",
            "{'loss': 0.2169, 'learning_rate': 7.080645161290323e-05, 'epoch': 129.19}\n",
            " 65% 8020/12400 [1:17:13<29:52,  2.44it/s]{'loss': 0.2131, 'learning_rate': 7.064516129032258e-05, 'epoch': 129.35}\n",
            " 65% 8030/12400 [1:17:17<29:38,  2.46it/s]{'loss': 0.132, 'learning_rate': 7.048387096774193e-05, 'epoch': 129.52}\n",
            " 65% 8040/12400 [1:17:21<28:36,  2.54it/s]{'loss': 0.1782, 'learning_rate': 7.03225806451613e-05, 'epoch': 129.68}\n",
            " 65% 8050/12400 [1:17:25<29:25,  2.46it/s]{'loss': 0.1239, 'learning_rate': 7.016129032258065e-05, 'epoch': 129.84}\n",
            "{'loss': 0.1024, 'learning_rate': 7e-05, 'epoch': 130.0}\n",
            " 65% 8060/12400 [1:17:29<26:41,  2.71it/s][INFO|trainer.py:2625] 2022-05-19 05:24:41,040 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:24:41,041 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:24:41,041 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.25it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.09it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.71it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.20it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.58it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.6658055782318115, 'eval_accuracy': 0.5068493150684932, 'eval_runtime': 3.0545, 'eval_samples_per_second': 71.698, 'eval_steps_per_second': 2.292, 'epoch': 130.0}\n",
            " 65% 8060/12400 [1:17:32<26:41,  2.71it/s]\n",
            "100% 7/7 [00:00<00:00,  8.08it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:24:44,097 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8060\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:24:44,101 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8060/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:24:45,736 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8060/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:24:45,737 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8060/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:24:49,157 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7936] due to args.save_total_limit\n",
            " 65% 8070/12400 [1:17:43<38:58,  1.85it/s]{'loss': 0.1494, 'learning_rate': 6.983870967741936e-05, 'epoch': 130.16}\n",
            "{'loss': 0.1682, 'learning_rate': 6.967741935483871e-05, 'epoch': 130.32}\n",
            "{'loss': 0.1056, 'learning_rate': 6.951612903225807e-05, 'epoch': 130.48}\n",
            " 65% 8100/12400 [1:17:55<29:36,  2.42it/s]{'loss': 0.1337, 'learning_rate': 6.935483870967743e-05, 'epoch': 130.65}\n",
            " 65% 8110/12400 [1:17:59<27:56,  2.56it/s]{'loss': 0.1411, 'learning_rate': 6.919354838709677e-05, 'epoch': 130.81}\n",
            "{'loss': 0.1262, 'learning_rate': 6.903225806451613e-05, 'epoch': 130.97}\n",
            " 66% 8122/12400 [1:18:04<26:38,  2.68it/s][INFO|trainer.py:2625] 2022-05-19 05:25:16,269 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:25:16,269 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:25:16,269 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.38it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.61it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.29it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.86it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.24it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7194290161132812, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.049, 'eval_samples_per_second': 71.827, 'eval_steps_per_second': 2.296, 'epoch': 131.0}\n",
            " 66% 8122/12400 [1:18:07<26:38,  2.68it/s]\n",
            "100% 7/7 [00:01<00:00,  7.84it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:25:19,320 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8122\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:25:19,323 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8122/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:25:20,977 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8122/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:25:20,978 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8122/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:25:24,548 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-7998] due to args.save_total_limit\n",
            "{'loss': 0.1159, 'learning_rate': 6.887096774193549e-05, 'epoch': 131.13}\n",
            " 66% 8140/12400 [1:18:22<30:27,  2.33it/s]{'loss': 0.1402, 'learning_rate': 6.870967741935485e-05, 'epoch': 131.29}\n",
            "{'loss': 0.177, 'learning_rate': 6.854838709677419e-05, 'epoch': 131.45}\n",
            "                                          {'loss': 0.0742, 'learning_rate': 6.838709677419355e-05, 'epoch': 131.61}\n",
            "                                          {'loss': 0.2021, 'learning_rate': 6.82258064516129e-05, 'epoch': 131.77}\n",
            "                                          {'loss': 0.1003, 'learning_rate': 6.806451612903226e-05, 'epoch': 131.94}\n",
            " 66% 8184/12400 [1:18:39<26:24,  2.66it/s][INFO|trainer.py:2625] 2022-05-19 05:25:51,914 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:25:51,914 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:25:51,914 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.22it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.29it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            "                                          \n",
            " 66% 8184/12400 [1:18:43<26:24,  2.66it/s]\n",
            "100% 7/7 [00:00<00:00,  8.57it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.7159435749053955, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.1236, 'eval_samples_per_second': 70.112, 'eval_steps_per_second': 2.241, 'epoch': 132.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:25:55,040 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8184\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:25:55,044 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8184/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:25:56,726 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8184/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:25:56,727 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8184/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:26:00,275 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8060] due to args.save_total_limit\n",
            "{'loss': 0.1154, 'learning_rate': 6.790322580645162e-05, 'epoch': 132.1}\n",
            "{'loss': 0.0928, 'learning_rate': 6.774193548387096e-05, 'epoch': 132.26}\n",
            " 66% 8210/12400 [1:19:01<28:52,  2.42it/s]{'loss': 0.0942, 'learning_rate': 6.758064516129032e-05, 'epoch': 132.42}\n",
            " 66% 8220/12400 [1:19:05<27:50,  2.50it/s]{'loss': 0.1758, 'learning_rate': 6.741935483870968e-05, 'epoch': 132.58}\n",
            "{'loss': 0.1992, 'learning_rate': 6.725806451612904e-05, 'epoch': 132.74}\n",
            "{'loss': 0.1794, 'learning_rate': 6.709677419354839e-05, 'epoch': 132.9}\n",
            " 66% 8246/12400 [1:19:15<25:47,  2.68it/s][INFO|trainer.py:2625] 2022-05-19 05:26:27,703 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:26:27,704 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:26:27,704 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.07it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.20it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.85it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.26it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.58it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.786877155303955, 'eval_accuracy': 0.5068493150684932, 'eval_runtime': 3.0358, 'eval_samples_per_second': 72.139, 'eval_steps_per_second': 2.306, 'epoch': 133.0}\n",
            " 66% 8246/12400 [1:19:18<25:47,  2.68it/s]\n",
            "100% 7/7 [00:00<00:00,  8.10it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:26:30,741 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8246\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:26:30,745 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8246/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:26:32,461 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8246/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:26:32,462 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8246/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:26:36,070 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8122] due to args.save_total_limit\n",
            " 67% 8250/12400 [1:19:27<1:41:15,  1.46s/it]{'loss': 0.139, 'learning_rate': 6.693548387096774e-05, 'epoch': 133.06}\n",
            "{'loss': 0.0521, 'learning_rate': 6.67741935483871e-05, 'epoch': 133.23}\n",
            " 67% 8270/12400 [1:19:36<28:02,  2.46it/s]{'loss': 0.0864, 'learning_rate': 6.661290322580645e-05, 'epoch': 133.39}\n",
            " 67% 8280/12400 [1:19:40<28:25,  2.42it/s]{'loss': 0.1192, 'learning_rate': 6.645161290322582e-05, 'epoch': 133.55}\n",
            "                                          {'loss': 0.1449, 'learning_rate': 6.629032258064517e-05, 'epoch': 133.71}\n",
            " 67% 8300/12400 [1:19:48<27:13,  2.51it/s]{'loss': 0.1826, 'learning_rate': 6.612903225806452e-05, 'epoch': 133.87}\n",
            " 67% 8308/12400 [1:19:51<25:25,  2.68it/s][INFO|trainer.py:2625] 2022-05-19 05:27:03,581 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:27:03,581 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:27:03,581 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.58it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.43it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.35it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.37it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.37it/s]\u001b[A\n",
            "                                          \n",
            " 67% 8308/12400 [1:19:54<25:25,  2.68it/s]\n",
            "100% 7/7 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.7496285438537598, 'eval_accuracy': 0.502283105022831, 'eval_runtime': 3.0615, 'eval_samples_per_second': 71.533, 'eval_steps_per_second': 2.286, 'epoch': 134.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:27:06,645 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8308\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:27:06,647 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8308/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:27:08,306 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8308/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:27:08,307 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8308/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:27:11,870 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8184] due to args.save_total_limit\n",
            "{'loss': 0.1408, 'learning_rate': 6.596774193548388e-05, 'epoch': 134.03}\n",
            "                                          {'loss': 0.0978, 'learning_rate': 6.580645161290323e-05, 'epoch': 134.19}\n",
            " 67% 8330/12400 [1:20:10<27:54,  2.43it/s]{'loss': 0.1699, 'learning_rate': 6.564516129032258e-05, 'epoch': 134.35}\n",
            "{'loss': 0.1875, 'learning_rate': 6.548387096774193e-05, 'epoch': 134.52}\n",
            " 67% 8350/12400 [1:20:19<27:19,  2.47it/s]{'loss': 0.1014, 'learning_rate': 6.532258064516129e-05, 'epoch': 134.68}\n",
            "                                          {'loss': 0.1508, 'learning_rate': 6.516129032258065e-05, 'epoch': 134.84}\n",
            " 68% 8370/12400 [1:20:26<24:10,  2.78it/s]{'loss': 0.1676, 'learning_rate': 6.500000000000001e-05, 'epoch': 135.0}\n",
            " 68% 8370/12400 [1:20:26<24:10,  2.78it/s][INFO|trainer.py:2625] 2022-05-19 05:27:38,824 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:27:38,824 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:27:38,824 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.22it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.27it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.35it/s]\u001b[A\n",
            "                                          \n",
            " 68% 8370/12400 [1:20:30<24:10,  2.78it/s]\n",
            "100% 7/7 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.73272442817688, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.1174, 'eval_samples_per_second': 70.252, 'eval_steps_per_second': 2.245, 'epoch': 135.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:27:41,944 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8370\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:27:41,947 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8370/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:27:43,610 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8370/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:27:43,611 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8370/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:27:47,117 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8246] due to args.save_total_limit\n",
            "{'loss': 0.138, 'learning_rate': 6.483870967741936e-05, 'epoch': 135.16}\n",
            "{'loss': 0.1627, 'learning_rate': 6.467741935483871e-05, 'epoch': 135.32}\n",
            "{'loss': 0.1023, 'learning_rate': 6.451612903225807e-05, 'epoch': 135.48}\n",
            "{'loss': 0.0964, 'learning_rate': 6.435483870967742e-05, 'epoch': 135.65}\n",
            "                                          {'loss': 0.1711, 'learning_rate': 6.419354838709679e-05, 'epoch': 135.81}\n",
            " 68% 8430/12400 [1:21:01<25:05,  2.64it/s]{'loss': 0.1518, 'learning_rate': 6.403225806451614e-05, 'epoch': 135.97}\n",
            " 68% 8432/12400 [1:21:02<24:40,  2.68it/s][INFO|trainer.py:2625] 2022-05-19 05:28:14,452 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:28:14,452 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:28:14,452 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.51it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.82it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.56it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.10it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.45it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7643518447875977, 'eval_accuracy': 0.4931506849315068, 'eval_runtime': 3.1156, 'eval_samples_per_second': 70.291, 'eval_steps_per_second': 2.247, 'epoch': 136.0}\n",
            " 68% 8432/12400 [1:21:05<24:40,  2.68it/s]\n",
            "100% 7/7 [00:00<00:00,  8.04it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:28:17,570 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8432\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:28:17,572 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8432/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:28:19,265 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8432/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:28:19,266 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8432/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:28:22,895 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8308] due to args.save_total_limit\n",
            "                                          {'loss': 0.1031, 'learning_rate': 6.387096774193548e-05, 'epoch': 136.13}\n",
            "                                          {'loss': 0.104, 'learning_rate': 6.370967741935485e-05, 'epoch': 136.29}\n",
            "{'loss': 0.1287, 'learning_rate': 6.35483870967742e-05, 'epoch': 136.45}\n",
            "{'loss': 0.0835, 'learning_rate': 6.338709677419355e-05, 'epoch': 136.61}\n",
            " 68% 8480/12400 [1:21:33<25:53,  2.52it/s]{'loss': 0.1987, 'learning_rate': 6.32258064516129e-05, 'epoch': 136.77}\n",
            " 68% 8490/12400 [1:21:36<24:25,  2.67it/s]{'loss': 0.1119, 'learning_rate': 6.306451612903226e-05, 'epoch': 136.94}\n",
            " 68% 8494/12400 [1:21:38<27:43,  2.35it/s][INFO|trainer.py:2625] 2022-05-19 05:28:50,441 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:28:50,442 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:28:50,442 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00, 13.67it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  5.02it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  5.12it/s]\u001b[A\n",
            " 86% 6/7 [00:01<00:00,  5.50it/s]\u001b[A\n",
            "                                          \n",
            "                                 {'eval_loss': 2.6700451374053955, 'eval_accuracy': 0.5251141552511416, 'eval_runtime': 3.7348, 'eval_samples_per_second': 58.637, 'eval_steps_per_second': 1.874, 'epoch': 137.0}\n",
            " 68% 8494/12400 [1:21:42<27:43,  2.35it/s]\n",
            "100% 7/7 [00:01<00:00,  5.93it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:28:54,180 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8494\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:28:54,187 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8494/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:28:55,886 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8494/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:28:55,887 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8494/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:28:59,548 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8370] due to args.save_total_limit\n",
            " 69% 8500/12400 [1:21:52<1:04:30,  1.01it/s]{'loss': 0.1578, 'learning_rate': 6.290322580645161e-05, 'epoch': 137.1}\n",
            " 69% 8510/12400 [1:21:56<28:40,  2.26it/s]{'loss': 0.1827, 'learning_rate': 6.274193548387098e-05, 'epoch': 137.26}\n",
            "{'loss': 0.0963, 'learning_rate': 6.258064516129033e-05, 'epoch': 137.42}\n",
            " 69% 8530/12400 [1:22:04<25:59,  2.48it/s]{'loss': 0.1398, 'learning_rate': 6.241935483870967e-05, 'epoch': 137.58}\n",
            " 69% 8540/12400 [1:22:08<25:59,  2.48it/s]{'loss': 0.144, 'learning_rate': 6.225806451612904e-05, 'epoch': 137.74}\n",
            "                                          {'loss': 0.1605, 'learning_rate': 6.209677419354839e-05, 'epoch': 137.9}\n",
            " 69% 8556/12400 [1:22:14<23:50,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 05:29:26,327 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:29:26,327 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:29:26,327 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.53it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.83it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.14it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.20it/s]\u001b[A\n",
            "                                          \n",
            " 69% 8556/12400 [1:22:17<23:50,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.55it/s]\u001b[A{'eval_loss': 2.734464406967163, 'eval_accuracy': 0.54337899543379, 'eval_runtime': 3.0876, 'eval_samples_per_second': 70.928, 'eval_steps_per_second': 2.267, 'epoch': 138.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:29:29,417 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8556\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:29:29,420 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8556/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:29:31,088 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8556/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:29:31,089 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8556/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:29:34,588 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8432] due to args.save_total_limit\n",
            " 69% 8560/12400 [1:22:26<1:34:52,  1.48s/it]{'loss': 0.0919, 'learning_rate': 6.193548387096774e-05, 'epoch': 138.06}\n",
            "{'loss': 0.1851, 'learning_rate': 6.177419354838711e-05, 'epoch': 138.23}\n",
            "                                          {'loss': 0.231, 'learning_rate': 6.161290322580645e-05, 'epoch': 138.39}\n",
            " 69% 8590/12400 [1:22:39<25:54,  2.45it/s]{'loss': 0.1306, 'learning_rate': 6.14516129032258e-05, 'epoch': 138.55}\n",
            "{'loss': 0.0969, 'learning_rate': 6.129032258064517e-05, 'epoch': 138.71}\n",
            "                                          {'loss': 0.1025, 'learning_rate': 6.112903225806452e-05, 'epoch': 138.87}\n",
            " 70% 8618/12400 [1:22:49<22:56,  2.75it/s][INFO|trainer.py:2625] 2022-05-19 05:30:01,834 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:30:01,834 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:30:01,834 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  5.98it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.38it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.63it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7090721130371094, 'eval_accuracy': 0.5159817351598174, 'eval_runtime': 3.1317, 'eval_samples_per_second': 69.93, 'eval_steps_per_second': 2.235, 'epoch': 139.0}\n",
            " 70% 8618/12400 [1:22:53<22:56,  2.75it/s]\n",
            "100% 7/7 [00:00<00:00,  8.10it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:30:04,968 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8618\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:30:04,971 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8618/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:30:06,621 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8618/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:30:06,622 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8618/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:30:10,149 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8494] due to args.save_total_limit\n",
            " 70% 8620/12400 [1:23:00<2:39:13,  2.53s/it]{'loss': 0.1322, 'learning_rate': 6.096774193548387e-05, 'epoch': 139.03}\n",
            " 70% 8630/12400 [1:23:05<30:47,  2.04it/s]{'loss': 0.0904, 'learning_rate': 6.080645161290322e-05, 'epoch': 139.19}\n",
            " 70% 8640/12400 [1:23:09<26:38,  2.35it/s]{'loss': 0.1499, 'learning_rate': 6.064516129032258e-05, 'epoch': 139.35}\n",
            " 70% 8650/12400 [1:23:13<25:08,  2.49it/s]{'loss': 0.1664, 'learning_rate': 6.048387096774194e-05, 'epoch': 139.52}\n",
            "{'loss': 0.1436, 'learning_rate': 6.0322580645161295e-05, 'epoch': 139.68}\n",
            "                                          {'loss': 0.1114, 'learning_rate': 6.016129032258064e-05, 'epoch': 139.84}\n",
            " 70% 8680/12400 [1:23:25<22:52,  2.71it/s]{'loss': 0.1082, 'learning_rate': 6e-05, 'epoch': 140.0}\n",
            " 70% 8680/12400 [1:23:25<22:52,  2.71it/s][INFO|trainer.py:2625] 2022-05-19 05:30:37,405 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:30:37,405 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:30:37,405 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.56it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.83it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.00it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.14it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            "                                          \n",
            " 70% 8680/12400 [1:23:28<22:52,  2.71it/s]\n",
            "100% 7/7 [00:00<00:00,  8.61it/s]{'eval_loss': 2.707608222961426, 'eval_accuracy': 0.5251141552511416, 'eval_runtime': 3.1119, 'eval_samples_per_second': 70.374, 'eval_steps_per_second': 2.249, 'epoch': 140.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:30:40,519 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8680\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:30:40,521 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8680/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:30:42,160 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8680/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:30:42,161 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8680/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:30:45,624 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8556] due to args.save_total_limit\n",
            "                                          {'loss': 0.0966, 'learning_rate': 5.9838709677419355e-05, 'epoch': 140.16}\n",
            "                                          {'loss': 0.1129, 'learning_rate': 5.9677419354838715e-05, 'epoch': 140.32}\n",
            "                                          {'loss': 0.142, 'learning_rate': 5.9516129032258074e-05, 'epoch': 140.48}\n",
            "{'loss': 0.1821, 'learning_rate': 5.935483870967742e-05, 'epoch': 140.65}\n",
            "{'loss': 0.0875, 'learning_rate': 5.9193548387096774e-05, 'epoch': 140.81}\n",
            "                                          {'loss': 0.0653, 'learning_rate': 5.9032258064516134e-05, 'epoch': 140.97}\n",
            " 70% 8742/12400 [1:24:00<22:58,  2.65it/s][INFO|trainer.py:2625] 2022-05-19 05:31:13,003 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:31:13,003 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:31:13,003 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.92it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.01it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.23it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            "                                          \n",
            " 70% 8742/12400 [1:24:04<22:58,  2.65it/s]\n",
            "100% 7/7 [00:00<00:00,  8.51it/s]{'eval_loss': 2.628310441970825, 'eval_accuracy': 0.5616438356164384, 'eval_runtime': 3.069, 'eval_samples_per_second': 71.359, 'eval_steps_per_second': 2.281, 'epoch': 141.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:31:16,075 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8742\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:31:16,078 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8742/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:31:17,732 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8742/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:31:17,733 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8742/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:31:21,347 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8618] due to args.save_total_limit\n",
            "{'loss': 0.1029, 'learning_rate': 5.887096774193549e-05, 'epoch': 141.13}\n",
            "                                          {'loss': 0.1286, 'learning_rate': 5.870967741935483e-05, 'epoch': 141.29}\n",
            " 71% 8770/12400 [1:24:23<24:48,  2.44it/s]{'loss': 0.1559, 'learning_rate': 5.854838709677419e-05, 'epoch': 141.45}\n",
            "{'loss': 0.0913, 'learning_rate': 5.838709677419355e-05, 'epoch': 141.61}\n",
            " 71% 8790/12400 [1:24:31<23:47,  2.53it/s]{'loss': 0.126, 'learning_rate': 5.8225806451612906e-05, 'epoch': 141.77}\n",
            "{'loss': 0.1664, 'learning_rate': 5.8064516129032266e-05, 'epoch': 141.94}\n",
            " 71% 8804/12400 [1:24:36<21:12,  2.83it/s][INFO|trainer.py:2625] 2022-05-19 05:31:48,697 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:31:48,697 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:31:48,697 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.49it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.41it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.35it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.34it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.32it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7549338340759277, 'eval_accuracy': 0.502283105022831, 'eval_runtime': 3.0782, 'eval_samples_per_second': 71.146, 'eval_steps_per_second': 2.274, 'epoch': 142.0}\n",
            " 71% 8804/12400 [1:24:39<21:12,  2.83it/s]\n",
            "100% 7/7 [00:00<00:00,  8.67it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:31:51,777 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8804\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:31:51,780 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8804/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:31:53,459 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8804/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:31:53,460 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8804/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:31:57,026 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8680] due to args.save_total_limit\n",
            "{'loss': 0.1623, 'learning_rate': 5.790322580645161e-05, 'epoch': 142.1}\n",
            "                                          {'loss': 0.166, 'learning_rate': 5.7741935483870965e-05, 'epoch': 142.26}\n",
            "{'loss': 0.0867, 'learning_rate': 5.7580645161290325e-05, 'epoch': 142.42}\n",
            " 71% 8840/12400 [1:25:02<24:16,  2.44it/s]{'loss': 0.1151, 'learning_rate': 5.7419354838709685e-05, 'epoch': 142.58}\n",
            "{'loss': 0.1687, 'learning_rate': 5.725806451612904e-05, 'epoch': 142.74}\n",
            "{'loss': 0.1653, 'learning_rate': 5.7096774193548384e-05, 'epoch': 142.9}\n",
            " 72% 8866/12400 [1:25:12<21:48,  2.70it/s][INFO|trainer.py:2625] 2022-05-19 05:32:24,404 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:32:24,404 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:32:24,404 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.26it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.43it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.20it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.82it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.25it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 2.760237216949463, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.1507, 'eval_samples_per_second': 69.509, 'eval_steps_per_second': 2.222, 'epoch': 143.0}\n",
            " 72% 8866/12400 [1:25:15<21:48,  2.70it/s]\n",
            "100% 7/7 [00:01<00:00,  7.77it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:32:27,557 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8866\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:32:27,561 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8866/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:32:29,237 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8866/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:32:29,238 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8866/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:32:32,991 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8742] due to args.save_total_limit\n",
            " 72% 8870/12400 [1:25:24<1:28:35,  1.51s/it]{'loss': 0.1685, 'learning_rate': 5.6935483870967744e-05, 'epoch': 143.06}\n",
            " 72% 8880/12400 [1:25:29<27:51,  2.11it/s]{'loss': 0.0912, 'learning_rate': 5.67741935483871e-05, 'epoch': 143.23}\n",
            " 72% 8890/12400 [1:25:33<24:29,  2.39it/s]{'loss': 0.1484, 'learning_rate': 5.661290322580646e-05, 'epoch': 143.39}\n",
            "{'loss': 0.1298, 'learning_rate': 5.645161290322582e-05, 'epoch': 143.55}\n",
            "{'loss': 0.0888, 'learning_rate': 5.6290322580645164e-05, 'epoch': 143.71}\n",
            " 72% 8920/12400 [1:25:46<22:49,  2.54it/s]{'loss': 0.2305, 'learning_rate': 5.612903225806452e-05, 'epoch': 143.87}\n",
            " 72% 8928/12400 [1:25:48<22:07,  2.61it/s][INFO|trainer.py:2625] 2022-05-19 05:33:00,851 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:33:00,851 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:33:00,851 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.10it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.13it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.77it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.24it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.54it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.809077501296997, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.0896, 'eval_samples_per_second': 70.884, 'eval_steps_per_second': 2.266, 'epoch': 144.0}\n",
            " 72% 8928/12400 [1:25:52<22:07,  2.61it/s]\n",
            "100% 7/7 [00:00<00:00,  8.02it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:33:03,943 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8928\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:33:03,947 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8928/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:33:05,636 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8928/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:33:05,637 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8928/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:33:09,242 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8804] due to args.save_total_limit\n",
            "{'loss': 0.1624, 'learning_rate': 5.5967741935483877e-05, 'epoch': 144.03}\n",
            " 72% 8940/12400 [1:26:04<29:43,  1.94it/s]{'loss': 0.111, 'learning_rate': 5.580645161290323e-05, 'epoch': 144.19}\n",
            " 72% 8950/12400 [1:26:09<25:53,  2.22it/s]{'loss': 0.1267, 'learning_rate': 5.5645161290322576e-05, 'epoch': 144.35}\n",
            " 72% 8960/12400 [1:26:13<23:41,  2.42it/s]{'loss': 0.1437, 'learning_rate': 5.5483870967741936e-05, 'epoch': 144.52}\n",
            "{'loss': 0.1447, 'learning_rate': 5.5322580645161296e-05, 'epoch': 144.68}\n",
            "                                          {'loss': 0.1176, 'learning_rate': 5.516129032258065e-05, 'epoch': 144.84}\n",
            " 72% 8990/12400 [1:26:25<21:46,  2.61it/s]{'loss': 0.0617, 'learning_rate': 5.500000000000001e-05, 'epoch': 145.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 05:33:37,591 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:33:37,591 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:33:37,592 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.83it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.99it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.17it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.23it/s]\u001b[A\n",
            "                                          \n",
            " 72% 8990/12400 [1:26:28<21:46,  2.61it/s]\n",
            "100% 7/7 [00:00<00:00,  8.62it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.7873265743255615, 'eval_accuracy': 0.5159817351598174, 'eval_runtime': 3.0953, 'eval_samples_per_second': 70.753, 'eval_steps_per_second': 2.262, 'epoch': 145.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:33:40,688 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8990\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:33:40,692 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8990/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:33:42,383 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8990/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:33:42,384 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8990/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:33:46,121 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8866] due to args.save_total_limit\n",
            "{'loss': 0.0749, 'learning_rate': 5.4838709677419355e-05, 'epoch': 145.16}\n",
            " 73% 9010/12400 [1:26:44<23:42,  2.38it/s]{'loss': 0.1731, 'learning_rate': 5.467741935483871e-05, 'epoch': 145.32}\n",
            "                                          {'loss': 0.1635, 'learning_rate': 5.451612903225807e-05, 'epoch': 145.48}\n",
            "                                          {'loss': 0.0949, 'learning_rate': 5.435483870967743e-05, 'epoch': 145.65}\n",
            " 73% 9040/12400 [1:26:57<22:01,  2.54it/s]{'loss': 0.0818, 'learning_rate': 5.419354838709678e-05, 'epoch': 145.81}\n",
            "{'loss': 0.1164, 'learning_rate': 5.403225806451613e-05, 'epoch': 145.97}\n",
            " 73% 9052/12400 [1:27:01<21:36,  2.58it/s][INFO|trainer.py:2625] 2022-05-19 05:34:13,713 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:34:13,713 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:34:13,713 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.71it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.93it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.04it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.12it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.16it/s]\u001b[A\n",
            "                                          \n",
            " 73% 9052/12400 [1:27:04<21:36,  2.58it/s]\n",
            "100% 7/7 [00:00<00:00,  8.50it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.7886204719543457, 'eval_accuracy': 0.4931506849315068, 'eval_runtime': 3.1134, 'eval_samples_per_second': 70.341, 'eval_steps_per_second': 2.248, 'epoch': 146.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:34:16,828 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9052\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:34:16,832 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9052/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:34:18,521 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9052/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:34:18,522 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9052/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:34:22,164 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8928] due to args.save_total_limit\n",
            "                                          {'loss': 0.127, 'learning_rate': 5.387096774193549e-05, 'epoch': 146.13}\n",
            "{'loss': 0.1488, 'learning_rate': 5.370967741935484e-05, 'epoch': 146.29}\n",
            "{'loss': 0.0976, 'learning_rate': 5.35483870967742e-05, 'epoch': 146.45}\n",
            "{'loss': 0.1138, 'learning_rate': 5.3387096774193547e-05, 'epoch': 146.61}\n",
            " 73% 9100/12400 [1:27:32<21:56,  2.51it/s]{'loss': 0.1312, 'learning_rate': 5.32258064516129e-05, 'epoch': 146.77}\n",
            "{'loss': 0.1588, 'learning_rate': 5.306451612903226e-05, 'epoch': 146.94}\n",
            " 74% 9114/12400 [1:27:37<21:06,  2.59it/s][INFO|trainer.py:2625] 2022-05-19 05:34:49,996 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:34:49,996 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:34:49,996 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.06it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.15it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.15it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.21it/s]\u001b[A\n",
            "                                          \n",
            " 74% 9114/12400 [1:27:41<21:06,  2.59it/s]\n",
            "100% 7/7 [00:00<00:00,  8.55it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.7748892307281494, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.0886, 'eval_samples_per_second': 70.905, 'eval_steps_per_second': 2.266, 'epoch': 147.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:34:53,087 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9114\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:34:53,091 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9114/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:34:54,777 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9114/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:34:54,778 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9114/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:34:58,544 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-8990] due to args.save_total_limit\n",
            " 74% 9120/12400 [1:27:51<53:10,  1.03it/s]{'loss': 0.1624, 'learning_rate': 5.290322580645162e-05, 'epoch': 147.1}\n",
            " 74% 9130/12400 [1:27:55<24:20,  2.24it/s]{'loss': 0.072, 'learning_rate': 5.274193548387097e-05, 'epoch': 147.26}\n",
            " 74% 9140/12400 [1:27:59<22:35,  2.40it/s]{'loss': 0.1622, 'learning_rate': 5.258064516129032e-05, 'epoch': 147.42}\n",
            " 74% 9150/12400 [1:28:03<22:26,  2.41it/s]{'loss': 0.1103, 'learning_rate': 5.241935483870968e-05, 'epoch': 147.58}\n",
            "                                          {'loss': 0.1197, 'learning_rate': 5.225806451612903e-05, 'epoch': 147.74}\n",
            "{'loss': 0.1503, 'learning_rate': 5.209677419354839e-05, 'epoch': 147.9}\n",
            " 74% 9176/12400 [1:28:14<20:35,  2.61it/s][INFO|trainer.py:2625] 2022-05-19 05:35:26,225 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:35:26,225 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:35:26,225 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.43it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.02it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.67it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.11it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.45it/s]\u001b[A\n",
            "                                          \n",
            " 74% 9176/12400 [1:28:17<20:35,  2.61it/s]\n",
            "100% 7/7 [00:00<00:00,  7.97it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:35:29,297 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9176\n",
            "{'eval_loss': 2.715317487716675, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.0701, 'eval_samples_per_second': 71.334, 'eval_steps_per_second': 2.28, 'epoch': 148.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:35:29,301 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9176/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:35:31,090 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9176/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:35:31,091 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9176/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:35:34,964 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9052] due to args.save_total_limit\n",
            " 74% 9180/12400 [1:28:26<1:22:29,  1.54s/it]{'loss': 0.0881, 'learning_rate': 5.193548387096775e-05, 'epoch': 148.06}\n",
            "{'loss': 0.081, 'learning_rate': 5.17741935483871e-05, 'epoch': 148.23}\n",
            "{'loss': 0.116, 'learning_rate': 5.161290322580645e-05, 'epoch': 148.39}\n",
            " 74% 9210/12400 [1:28:39<22:01,  2.41it/s]{'loss': 0.0724, 'learning_rate': 5.145161290322581e-05, 'epoch': 148.55}\n",
            " 74% 9220/12400 [1:28:44<22:12,  2.39it/s]{'loss': 0.1115, 'learning_rate': 5.1290322580645164e-05, 'epoch': 148.71}\n",
            "{'loss': 0.1477, 'learning_rate': 5.112903225806451e-05, 'epoch': 148.87}\n",
            " 74% 9238/12400 [1:28:50<19:38,  2.68it/s][INFO|trainer.py:2625] 2022-05-19 05:36:03,024 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:36:03,024 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:36:03,024 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.63it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.87it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7479615211486816, 'eval_accuracy': 0.4931506849315068, 'eval_runtime': 3.1222, 'eval_samples_per_second': 70.143, 'eval_steps_per_second': 2.242, 'epoch': 149.0}\n",
            " 74% 9238/12400 [1:28:54<19:38,  2.68it/s]\n",
            "100% 7/7 [00:00<00:00,  8.46it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:36:06,148 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9238\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:36:06,152 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9238/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:36:07,841 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9238/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:36:07,842 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9238/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:36:11,671 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9114] due to args.save_total_limit\n",
            "{'loss': 0.0831, 'learning_rate': 5.096774193548387e-05, 'epoch': 149.03}\n",
            "{'loss': 0.1004, 'learning_rate': 5.080645161290323e-05, 'epoch': 149.19}\n",
            " 75% 9260/12400 [1:29:11<21:55,  2.39it/s]{'loss': 0.1223, 'learning_rate': 5.064516129032258e-05, 'epoch': 149.35}\n",
            " 75% 9270/12400 [1:29:15<21:31,  2.42it/s]{'loss': 0.0999, 'learning_rate': 5.048387096774194e-05, 'epoch': 149.52}\n",
            " 75% 9280/12400 [1:29:19<22:01,  2.36it/s]{'loss': 0.1192, 'learning_rate': 5.032258064516129e-05, 'epoch': 149.68}\n",
            "{'loss': 0.0893, 'learning_rate': 5.016129032258064e-05, 'epoch': 149.84}\n",
            "                                          {'loss': 0.0983, 'learning_rate': 5e-05, 'epoch': 150.0}\n",
            " 75% 9300/12400 [1:29:27<18:37,  2.77it/s][INFO|trainer.py:2625] 2022-05-19 05:36:39,456 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:36:39,457 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:36:39,457 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.85it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.00it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.13it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.807807445526123, 'eval_accuracy': 0.4885844748858447, 'eval_runtime': 3.0832, 'eval_samples_per_second': 71.03, 'eval_steps_per_second': 2.27, 'epoch': 150.0}\n",
            " 75% 9300/12400 [1:29:30<18:37,  2.77it/s]\n",
            "100% 7/7 [00:00<00:00,  8.61it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:36:42,542 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9300\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:36:42,545 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9300/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:36:44,200 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9300/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:36:44,201 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9300/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:36:47,806 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9176] due to args.save_total_limit\n",
            "{'loss': 0.1072, 'learning_rate': 4.983870967741936e-05, 'epoch': 150.16}\n",
            " 75% 9320/12400 [1:29:46<21:54,  2.34it/s]{'loss': 0.1007, 'learning_rate': 4.967741935483871e-05, 'epoch': 150.32}\n",
            "                                          {'loss': 0.1142, 'learning_rate': 4.951612903225807e-05, 'epoch': 150.48}\n",
            " 75% 9340/12400 [1:29:54<20:49,  2.45it/s]{'loss': 0.1436, 'learning_rate': 4.935483870967742e-05, 'epoch': 150.65}\n",
            " 75% 9350/12400 [1:29:58<19:47,  2.57it/s]{'loss': 0.0946, 'learning_rate': 4.9193548387096775e-05, 'epoch': 150.81}\n",
            "                                          {'loss': 0.1355, 'learning_rate': 4.903225806451613e-05, 'epoch': 150.97}\n",
            " 76% 9362/12400 [1:30:02<18:37,  2.72it/s][INFO|trainer.py:2625] 2022-05-19 05:37:15,035 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:37:15,036 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:37:15,036 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  8.77it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.47it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.38it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.29it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.25it/s]\u001b[A\n",
            "                                          \n",
            " 76% 9362/12400 [1:30:06<18:37,  2.72it/s]\n",
            "{'eval_loss': 2.873919725418091, 'eval_accuracy': 0.4794520547945205, 'eval_runtime': 3.0503, 'eval_samples_per_second': 71.796, 'eval_steps_per_second': 2.295, 'epoch': 151.0}\n",
            "100% 7/7 [00:00<00:00,  8.57it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:37:18,088 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9362\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:37:18,091 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9362/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:37:19,749 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9362/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:37:19,750 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9362/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:37:23,286 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9238] due to args.save_total_limit\n",
            " 76% 9370/12400 [1:30:16<34:56,  1.45it/s]{'loss': 0.0828, 'learning_rate': 4.887096774193549e-05, 'epoch': 151.13}\n",
            "                                          {'loss': 0.0955, 'learning_rate': 4.870967741935484e-05, 'epoch': 151.29}\n",
            "                                          {'loss': 0.1368, 'learning_rate': 4.8548387096774194e-05, 'epoch': 151.45}\n",
            "{'loss': 0.1405, 'learning_rate': 4.8387096774193554e-05, 'epoch': 151.61}\n",
            " 76% 9410/12400 [1:30:33<19:19,  2.58it/s]{'loss': 0.068, 'learning_rate': 4.822580645161291e-05, 'epoch': 151.77}\n",
            "{'loss': 0.0749, 'learning_rate': 4.806451612903226e-05, 'epoch': 151.94}\n",
            " 76% 9424/12400 [1:30:38<18:01,  2.75it/s][INFO|trainer.py:2625] 2022-05-19 05:37:50,514 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:37:50,515 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:37:50,515 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.55it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.94it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.61it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.14it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.48it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 2.8412413597106934, 'eval_accuracy': 0.4840182648401826, 'eval_runtime': 3.0552, 'eval_samples_per_second': 71.682, 'eval_steps_per_second': 2.291, 'epoch': 152.0}\n",
            " 76% 9424/12400 [1:30:41<18:01,  2.75it/s]\n",
            "100% 7/7 [00:00<00:00,  8.03it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:37:53,572 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9424\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:37:53,575 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9424/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:37:55,265 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9424/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:37:55,266 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9424/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:37:58,820 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9300] due to args.save_total_limit\n",
            "{'loss': 0.1097, 'learning_rate': 4.790322580645161e-05, 'epoch': 152.1}\n",
            "{'loss': 0.1156, 'learning_rate': 4.774193548387097e-05, 'epoch': 152.26}\n",
            "{'loss': 0.1817, 'learning_rate': 4.7580645161290326e-05, 'epoch': 152.42}\n",
            " 76% 9460/12400 [1:31:04<20:15,  2.42it/s]{'loss': 0.0587, 'learning_rate': 4.741935483870968e-05, 'epoch': 152.58}\n",
            "                                          {'loss': 0.0847, 'learning_rate': 4.725806451612904e-05, 'epoch': 152.74}\n",
            "{'loss': 0.1559, 'learning_rate': 4.7096774193548385e-05, 'epoch': 152.9}\n",
            " 76% 9486/12400 [1:31:14<17:35,  2.76it/s][INFO|trainer.py:2625] 2022-05-19 05:38:26,476 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:38:26,476 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:38:26,476 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.02it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.32it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.60it/s]\u001b[A\n",
            "                                          \n",
            " 76% 9486/12400 [1:31:17<17:35,  2.76it/s]\n",
            "100% 7/7 [00:00<00:00,  8.10it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.743701457977295, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.0936, 'eval_samples_per_second': 70.792, 'eval_steps_per_second': 2.263, 'epoch': 153.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:38:29,572 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9486\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:38:29,577 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9486/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:38:31,267 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9486/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:38:31,268 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9486/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:38:34,792 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9362] due to args.save_total_limit\n",
            "                                            {'loss': 0.1177, 'learning_rate': 4.6935483870967745e-05, 'epoch': 153.06}\n",
            "{'loss': 0.2009, 'learning_rate': 4.67741935483871e-05, 'epoch': 153.23}\n",
            "                                          {'loss': 0.0941, 'learning_rate': 4.661290322580645e-05, 'epoch': 153.39}\n",
            " 77% 9520/12400 [1:31:39<19:42,  2.44it/s]{'loss': 0.1215, 'learning_rate': 4.645161290322581e-05, 'epoch': 153.55}\n",
            "{'loss': 0.1586, 'learning_rate': 4.6290322580645164e-05, 'epoch': 153.71}\n",
            "{'loss': 0.1245, 'learning_rate': 4.612903225806452e-05, 'epoch': 153.87}\n",
            " 77% 9548/12400 [1:31:50<17:27,  2.72it/s][INFO|trainer.py:2625] 2022-05-19 05:39:02,577 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:39:02,577 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:39:02,577 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.05it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.81it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.30it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.62it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.88it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.7031683921813965, 'eval_accuracy': 0.4977168949771689, 'eval_runtime': 3.0843, 'eval_samples_per_second': 71.005, 'eval_steps_per_second': 2.27, 'epoch': 154.0}\n",
            " 77% 9548/12400 [1:31:53<17:27,  2.72it/s]\n",
            "100% 7/7 [00:00<00:00,  8.29it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:39:05,663 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9548\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:39:05,666 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9548/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:39:07,334 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9548/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:39:07,335 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9548/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:39:11,023 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9424] due to args.save_total_limit\n",
            "                                            {'loss': 0.1596, 'learning_rate': 4.596774193548387e-05, 'epoch': 154.03}\n",
            "{'loss': 0.0734, 'learning_rate': 4.580645161290323e-05, 'epoch': 154.19}\n",
            "                                          {'loss': 0.1128, 'learning_rate': 4.5645161290322584e-05, 'epoch': 154.35}\n",
            "{'loss': 0.1073, 'learning_rate': 4.548387096774194e-05, 'epoch': 154.52}\n",
            "{'loss': 0.1506, 'learning_rate': 4.53225806451613e-05, 'epoch': 154.68}\n",
            "{'loss': 0.1305, 'learning_rate': 4.516129032258064e-05, 'epoch': 154.84}\n",
            "                                          {'loss': 0.1008, 'learning_rate': 4.5e-05, 'epoch': 155.0}\n",
            " 78% 9610/12400 [1:32:26<17:37,  2.64it/s][INFO|trainer.py:2625] 2022-05-19 05:39:38,464 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:39:38,464 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:39:38,464 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.07it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.36it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.99it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.44it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.70it/s]\u001b[A\n",
            "                                          \n",
            " 78% 9610/12400 [1:32:29<17:37,  2.64it/s]\n",
            "100% 7/7 [00:00<00:00,  8.23it/s]\u001b[A\n",
            "                                 {'eval_loss': 2.7308108806610107, 'eval_accuracy': 0.5296803652968036, 'eval_runtime': 3.0439, 'eval_samples_per_second': 71.947, 'eval_steps_per_second': 2.3, 'epoch': 155.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 05:39:41,510 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9610\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:39:41,515 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9610/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:39:43,180 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9610/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:39:43,181 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9610/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:39:46,725 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9486] due to args.save_total_limit\n",
            " 78% 9620/12400 [1:32:41<26:04,  1.78it/s]{'loss': 0.0868, 'learning_rate': 4.4838709677419356e-05, 'epoch': 155.16}\n",
            "{'loss': 0.1364, 'learning_rate': 4.467741935483871e-05, 'epoch': 155.32}\n",
            "{'loss': 0.0891, 'learning_rate': 4.451612903225807e-05, 'epoch': 155.48}\n",
            "{'loss': 0.1364, 'learning_rate': 4.435483870967742e-05, 'epoch': 155.65}\n",
            " 78% 9660/12400 [1:32:57<17:42,  2.58it/s]{'loss': 0.1043, 'learning_rate': 4.4193548387096775e-05, 'epoch': 155.81}\n",
            "                                          {'loss': 0.1268, 'learning_rate': 4.403225806451613e-05, 'epoch': 155.97}\n",
            " 78% 9672/12400 [1:33:01<16:43,  2.72it/s][INFO|trainer.py:2625] 2022-05-19 05:40:14,076 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:40:14,076 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:40:14,076 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.84it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.00it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.08it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.19it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "100% 7/7 [00:00<00:00,  8.64it/s]\u001b[A{'eval_loss': 2.6965599060058594, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.1271, 'eval_samples_per_second': 70.034, 'eval_steps_per_second': 2.239, 'epoch': 156.0}\n",
            "                                          \n",
            " 78% 9672/12400 [1:33:05<16:43,  2.72it/s]\n",
            "100% 7/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:40:17,205 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9672\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:40:17,208 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9672/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:40:18,888 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9672/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:40:18,889 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9672/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:40:22,634 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9548] due to args.save_total_limit\n",
            "                                          {'loss': 0.1338, 'learning_rate': 4.387096774193549e-05, 'epoch': 156.13}\n",
            " 78% 9690/12400 [1:33:20<20:17,  2.23it/s]{'loss': 0.0797, 'learning_rate': 4.370967741935484e-05, 'epoch': 156.29}\n",
            " 78% 9700/12400 [1:33:24<18:48,  2.39it/s]{'loss': 0.1498, 'learning_rate': 4.3548387096774194e-05, 'epoch': 156.45}\n",
            " 78% 9710/12400 [1:33:28<18:21,  2.44it/s]{'loss': 0.1323, 'learning_rate': 4.3387096774193554e-05, 'epoch': 156.61}\n",
            "{'loss': 0.1655, 'learning_rate': 4.322580645161291e-05, 'epoch': 156.77}\n",
            "{'loss': 0.093, 'learning_rate': 4.306451612903226e-05, 'epoch': 156.94}\n",
            " 78% 9734/12400 [1:33:37<16:32,  2.69it/s][INFO|trainer.py:2625] 2022-05-19 05:40:50,064 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:40:50,064 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:40:50,064 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  7.94it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  8.09it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  8.28it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  8.29it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.707522392272949, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.0914, 'eval_samples_per_second': 70.841, 'eval_steps_per_second': 2.264, 'epoch': 157.0}\n",
            " 78% 9734/12400 [1:33:41<16:32,  2.69it/s]\n",
            "100% 7/7 [00:00<00:00,  8.64it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:40:53,157 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9734\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:40:53,163 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9734/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:40:54,840 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9734/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:40:54,840 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9734/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:40:58,436 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9610] due to args.save_total_limit\n",
            "{'loss': 0.0618, 'learning_rate': 4.2903225806451614e-05, 'epoch': 157.1}\n",
            "{'loss': 0.1101, 'learning_rate': 4.2741935483870973e-05, 'epoch': 157.26}\n",
            " 79% 9760/12400 [1:33:59<18:13,  2.41it/s]{'loss': 0.0916, 'learning_rate': 4.258064516129032e-05, 'epoch': 157.42}\n",
            "                                          {'loss': 0.1172, 'learning_rate': 4.241935483870968e-05, 'epoch': 157.58}\n",
            "                                          {'loss': 0.1411, 'learning_rate': 4.225806451612904e-05, 'epoch': 157.74}\n",
            " 79% 9790/12400 [1:34:11<16:40,  2.61it/s]{'loss': 0.1026, 'learning_rate': 4.2096774193548386e-05, 'epoch': 157.9}\n",
            " 79% 9796/12400 [1:34:13<16:21,  2.65it/s][INFO|trainer.py:2625] 2022-05-19 05:41:25,906 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:41:25,906 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:41:25,907 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.85it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.35it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.69it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.85it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.98it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_loss': 2.713461399078369, 'eval_accuracy': 0.5068493150684932, 'eval_runtime': 3.1483, 'eval_samples_per_second': 69.562, 'eval_steps_per_second': 2.223, 'epoch': 158.0}\n",
            " 79% 9796/12400 [1:34:17<16:21,  2.65it/s]\n",
            "100% 7/7 [00:00<00:00,  8.31it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:41:29,057 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9796\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:41:29,060 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9796/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:41:30,753 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9796/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:41:30,754 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9796/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:41:34,459 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9672] due to args.save_total_limit\n",
            "                                            {'loss': 0.0765, 'learning_rate': 4.1935483870967746e-05, 'epoch': 158.06}\n",
            " 79% 9810/12400 [1:34:30<20:10,  2.14it/s]{'loss': 0.1238, 'learning_rate': 4.17741935483871e-05, 'epoch': 158.23}\n",
            " 79% 9820/12400 [1:34:34<18:08,  2.37it/s]{'loss': 0.0823, 'learning_rate': 4.161290322580645e-05, 'epoch': 158.39}\n",
            " 79% 9830/12400 [1:34:39<17:57,  2.39it/s]{'loss': 0.1072, 'learning_rate': 4.1451612903225805e-05, 'epoch': 158.55}\n",
            " 79% 9840/12400 [1:34:43<17:51,  2.39it/s]{'loss': 0.1219, 'learning_rate': 4.1290322580645165e-05, 'epoch': 158.71}\n",
            "{'loss': 0.1076, 'learning_rate': 4.112903225806452e-05, 'epoch': 158.87}\n",
            " 80% 9858/12400 [1:34:50<16:21,  2.59it/s][INFO|trainer.py:2625] 2022-05-19 05:42:02,523 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:42:02,524 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:42:02,524 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.76it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.31it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.65it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.79it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.97it/s]\u001b[A\n",
            "{'eval_loss': 2.7386438846588135, 'eval_accuracy': 0.5251141552511416, 'eval_runtime': 3.1196, 'eval_samples_per_second': 70.202, 'eval_steps_per_second': 2.244, 'epoch': 159.0}\n",
            "\n",
            " 80% 9858/12400 [1:34:53<16:21,  2.59it/s]\n",
            "100% 7/7 [00:00<00:00,  8.45it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:42:05,645 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9858\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:42:05,650 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9858/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:42:07,312 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9858/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:42:07,313 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9858/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:42:10,946 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9734] due to args.save_total_limit\n",
            "                                            {'loss': 0.1015, 'learning_rate': 4.096774193548387e-05, 'epoch': 159.03}\n",
            "{'loss': 0.1013, 'learning_rate': 4.080645161290323e-05, 'epoch': 159.19}\n",
            "{'loss': 0.1672, 'learning_rate': 4.0645161290322584e-05, 'epoch': 159.35}\n",
            "{'loss': 0.0841, 'learning_rate': 4.048387096774194e-05, 'epoch': 159.52}\n",
            "                                          {'loss': 0.0868, 'learning_rate': 4.032258064516129e-05, 'epoch': 159.68}\n",
            "                                          {'loss': 0.1189, 'learning_rate': 4.016129032258065e-05, 'epoch': 159.84}\n",
            "{'loss': 0.1071, 'learning_rate': 4e-05, 'epoch': 160.0}\n",
            " 80% 9920/12400 [1:35:26<15:42,  2.63it/s][INFO|trainer.py:2625] 2022-05-19 05:42:38,529 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:42:38,530 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:42:38,530 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.01it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.92it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.37it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.63it/s]\u001b[A\n",
            "                                          \n",
            " 80% 9920/12400 [1:35:29<15:42,  2.63it/s]\n",
            "100% 7/7 [00:00<00:00,  8.18it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.7331347465515137, 'eval_accuracy': 0.5251141552511416, 'eval_runtime': 3.0256, 'eval_samples_per_second': 72.381, 'eval_steps_per_second': 2.314, 'epoch': 160.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 05:42:41,557 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9920\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:42:41,560 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9920/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:42:43,231 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9920/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:42:43,232 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9920/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:42:46,812 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9796] due to args.save_total_limit\n",
            " 80% 9930/12400 [1:35:40<22:47,  1.81it/s]{'loss': 0.0602, 'learning_rate': 3.9838709677419356e-05, 'epoch': 160.16}\n",
            "{'loss': 0.0856, 'learning_rate': 3.9677419354838716e-05, 'epoch': 160.32}\n",
            "{'loss': 0.0856, 'learning_rate': 3.951612903225806e-05, 'epoch': 160.48}\n",
            " 80% 9960/12400 [1:35:53<16:49,  2.42it/s]{'loss': 0.1518, 'learning_rate': 3.935483870967742e-05, 'epoch': 160.65}\n",
            "                                          {'loss': 0.1082, 'learning_rate': 3.9193548387096776e-05, 'epoch': 160.81}\n",
            " 80% 9980/12400 [1:36:01<15:44,  2.56it/s]{'loss': 0.062, 'learning_rate': 3.903225806451613e-05, 'epoch': 160.97}\n",
            " 80% 9982/12400 [1:36:02<15:51,  2.54it/s][INFO|trainer.py:2625] 2022-05-19 05:43:14,331 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:43:14,331 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:43:14,331 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.05it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.22it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.81it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.56it/s]\u001b[A\n",
            "                                          \n",
            "{'eval_loss': 2.7497408390045166, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.058, 'eval_samples_per_second': 71.616, 'eval_steps_per_second': 2.289, 'epoch': 161.0}\n",
            " 80% 9982/12400 [1:36:05<15:51,  2.54it/s]\n",
            "100% 7/7 [00:00<00:00,  8.00it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:43:17,391 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9982\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:43:17,397 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9982/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:43:19,083 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9982/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:43:19,084 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9982/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:43:22,799 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9858] due to args.save_total_limit\n",
            " 81% 9990/12400 [1:36:16<27:53,  1.44it/s]{'loss': 0.0991, 'learning_rate': 3.887096774193549e-05, 'epoch': 161.13}\n",
            "                                           {'loss': 0.0911, 'learning_rate': 3.870967741935484e-05, 'epoch': 161.29}\n",
            " 81% 10010/12400 [1:36:24<16:45,  2.38it/s]{'loss': 0.1174, 'learning_rate': 3.8548387096774195e-05, 'epoch': 161.45}\n",
            " 81% 10020/12400 [1:36:28<16:13,  2.44it/s]{'loss': 0.1168, 'learning_rate': 3.838709677419355e-05, 'epoch': 161.61}\n",
            " 81% 10030/12400 [1:36:33<16:01,  2.46it/s]{'loss': 0.1144, 'learning_rate': 3.822580645161291e-05, 'epoch': 161.77}\n",
            "{'loss': 0.1238, 'learning_rate': 3.8064516129032254e-05, 'epoch': 161.94}\n",
            " 81% 10044/12400 [1:36:38<14:26,  2.72it/s][INFO|trainer.py:2625] 2022-05-19 05:43:50,166 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:43:50,167 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:43:50,167 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.08it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.37it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.37it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.63it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.7635815143585205, 'eval_accuracy': 0.5114155251141552, 'eval_runtime': 3.0677, 'eval_samples_per_second': 71.389, 'eval_steps_per_second': 2.282, 'epoch': 162.0}\n",
            " 81% 10044/12400 [1:36:41<14:26,  2.72it/s]\n",
            "100% 7/7 [00:00<00:00,  8.11it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:43:53,236 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10044\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:43:53,240 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10044/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:43:54,945 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10044/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:43:54,946 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10044/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:43:58,756 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9920] due to args.save_total_limit\n",
            "{'loss': 0.0756, 'learning_rate': 3.7903225806451614e-05, 'epoch': 162.1}\n",
            "                                           {'loss': 0.091, 'learning_rate': 3.7741935483870974e-05, 'epoch': 162.26}\n",
            " 81% 10070/12400 [1:37:00<17:00,  2.28it/s]{'loss': 0.0654, 'learning_rate': 3.758064516129032e-05, 'epoch': 162.42}\n",
            " 81% 10080/12400 [1:37:04<16:17,  2.37it/s]{'loss': 0.1008, 'learning_rate': 3.741935483870968e-05, 'epoch': 162.58}\n",
            "                                           {'loss': 0.0337, 'learning_rate': 3.725806451612903e-05, 'epoch': 162.74}\n",
            "                                           {'loss': 0.0965, 'learning_rate': 3.7096774193548386e-05, 'epoch': 162.9}\n",
            " 82% 10106/12400 [1:37:15<14:51,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 05:44:27,491 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:44:27,491 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:44:27,491 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.54it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.83it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.51it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.02it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.39it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 2.7173736095428467, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.2071, 'eval_samples_per_second': 68.286, 'eval_steps_per_second': 2.183, 'epoch': 163.0}\n",
            " 82% 10106/12400 [1:37:18<14:51,  2.57it/s]\n",
            "100% 7/7 [00:00<00:00,  7.90it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:44:30,700 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10106\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:44:30,706 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10106/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:44:32,430 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10106/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:44:32,431 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10106/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:44:36,250 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-9982] due to args.save_total_limit\n",
            " 82% 10110/12400 [1:37:27<59:21,  1.56s/it]  {'loss': 0.0887, 'learning_rate': 3.6935483870967746e-05, 'epoch': 163.06}\n",
            "                                           {'loss': 0.1248, 'learning_rate': 3.67741935483871e-05, 'epoch': 163.23}\n",
            "                                           {'loss': 0.0566, 'learning_rate': 3.661290322580645e-05, 'epoch': 163.39}\n",
            " 82% 10140/12400 [1:37:42<16:31,  2.28it/s]{'loss': 0.0899, 'learning_rate': 3.6451612903225805e-05, 'epoch': 163.55}\n",
            "{'loss': 0.0901, 'learning_rate': 3.6290322580645165e-05, 'epoch': 163.71}\n",
            " 82% 10160/12400 [1:37:50<15:04,  2.48it/s]{'loss': 0.0921, 'learning_rate': 3.612903225806452e-05, 'epoch': 163.87}\n",
            " 82% 10168/12400 [1:37:53<14:08,  2.63it/s][INFO|trainer.py:2625] 2022-05-19 05:45:05,333 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:45:05,333 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:45:05,333 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.55it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.77it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.45it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.99it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.36it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.6928014755249023, 'eval_accuracy': 0.54337899543379, 'eval_runtime': 3.2197, 'eval_samples_per_second': 68.018, 'eval_steps_per_second': 2.174, 'epoch': 164.0}\n",
            " 82% 10168/12400 [1:37:56<14:08,  2.63it/s]\n",
            "100% 7/7 [00:00<00:00,  7.94it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:45:08,555 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10168\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:45:08,558 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10168/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:45:10,258 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10168/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:45:10,259 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10168/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:45:14,067 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10044] due to args.save_total_limit\n",
            " 82% 10170/12400 [1:38:04<1:38:23,  2.65s/it]{'loss': 0.079, 'learning_rate': 3.596774193548387e-05, 'epoch': 164.03}\n",
            "{'loss': 0.1355, 'learning_rate': 3.580645161290323e-05, 'epoch': 164.19}\n",
            " 82% 10190/12400 [1:38:13<16:01,  2.30it/s]{'loss': 0.0949, 'learning_rate': 3.5645161290322585e-05, 'epoch': 164.35}\n",
            " 82% 10200/12400 [1:38:18<15:35,  2.35it/s]{'loss': 0.0785, 'learning_rate': 3.548387096774194e-05, 'epoch': 164.52}\n",
            " 82% 10210/12400 [1:38:22<15:29,  2.36it/s]{'loss': 0.0677, 'learning_rate': 3.532258064516129e-05, 'epoch': 164.68}\n",
            "                                           {'loss': 0.0843, 'learning_rate': 3.516129032258065e-05, 'epoch': 164.84}\n",
            " 82% 10230/12400 [1:38:30<13:29,  2.68it/s]{'loss': 0.1052, 'learning_rate': 3.5e-05, 'epoch': 165.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 05:45:42,447 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:45:42,447 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:45:42,447 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.55it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.74it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.48it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.00it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.39it/s]\u001b[A\n",
            "{'eval_loss': 2.6324217319488525, 'eval_accuracy': 0.547945205479452, 'eval_runtime': 3.1706, 'eval_samples_per_second': 69.072, 'eval_steps_per_second': 2.208, 'epoch': 165.0}\n",
            "\n",
            " 82% 10230/12400 [1:38:33<13:29,  2.68it/s]\n",
            "100% 7/7 [00:00<00:00,  7.96it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:45:45,620 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10230\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:45:45,626 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10230/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:45:47,317 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10230/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:45:47,318 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10230/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:45:51,172 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10106] due to args.save_total_limit\n",
            "{'loss': 0.0944, 'learning_rate': 3.483870967741936e-05, 'epoch': 165.16}\n",
            "{'loss': 0.0825, 'learning_rate': 3.467741935483872e-05, 'epoch': 165.32}\n",
            " 83% 10260/12400 [1:38:54<15:08,  2.36it/s]{'loss': 0.0689, 'learning_rate': 3.451612903225806e-05, 'epoch': 165.48}\n",
            " 83% 10270/12400 [1:38:58<14:36,  2.43it/s]{'loss': 0.1146, 'learning_rate': 3.435483870967742e-05, 'epoch': 165.65}\n",
            " 83% 10280/12400 [1:39:03<14:00,  2.52it/s]{'loss': 0.0649, 'learning_rate': 3.4193548387096776e-05, 'epoch': 165.81}\n",
            " 83% 10290/12400 [1:39:07<13:41,  2.57it/s]{'loss': 0.052, 'learning_rate': 3.403225806451613e-05, 'epoch': 165.97}\n",
            " 83% 10292/12400 [1:39:07<13:19,  2.64it/s][INFO|trainer.py:2625] 2022-05-19 05:46:19,545 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:46:19,546 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:46:19,546 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.22it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.37it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.15it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.76it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.19it/s]\u001b[A\n",
            "                                           \n",
            " 83% 10292/12400 [1:39:10<13:19,  2.64it/s]\n",
            "100% 7/7 [00:01<00:00,  7.74it/s]{'eval_loss': 2.6006739139556885, 'eval_accuracy': 0.547945205479452, 'eval_runtime': 3.2266, 'eval_samples_per_second': 67.874, 'eval_steps_per_second': 2.169, 'epoch': 166.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:46:22,775 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10292\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:46:22,779 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10292/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:46:24,486 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10292/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:46:24,487 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10292/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:46:28,206 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10168] due to args.save_total_limit\n",
            "                                           {'loss': 0.056, 'learning_rate': 3.387096774193548e-05, 'epoch': 166.13}\n",
            " 83% 10310/12400 [1:39:26<16:17,  2.14it/s]{'loss': 0.1024, 'learning_rate': 3.370967741935484e-05, 'epoch': 166.29}\n",
            "{'loss': 0.0987, 'learning_rate': 3.3548387096774195e-05, 'epoch': 166.45}\n",
            "                                           {'loss': 0.0694, 'learning_rate': 3.338709677419355e-05, 'epoch': 166.61}\n",
            "                                           {'loss': 0.0868, 'learning_rate': 3.322580645161291e-05, 'epoch': 166.77}\n",
            "{'loss': 0.0477, 'learning_rate': 3.306451612903226e-05, 'epoch': 166.94}\n",
            " 84% 10354/12400 [1:39:45<13:15,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 05:46:57,135 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:46:57,135 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:46:57,135 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.54it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.28it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.77it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.17it/s]\u001b[A\n",
            "100% 7/7 [00:01<00:00,  7.76it/s]\u001b[A{'eval_loss': 2.6446003913879395, 'eval_accuracy': 0.5525114155251142, 'eval_runtime': 3.2317, 'eval_samples_per_second': 67.767, 'eval_steps_per_second': 2.166, 'epoch': 167.0}\n",
            "                                           \n",
            " 84% 10354/12400 [1:39:48<13:15,  2.57it/s]\n",
            "100% 7/7 [00:01<00:00,  7.76it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:47:00,369 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10354\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:47:00,374 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10354/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:47:02,067 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10354/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:47:02,068 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10354/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:47:05,987 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10230] due to args.save_total_limit\n",
            "{'loss': 0.0642, 'learning_rate': 3.2903225806451614e-05, 'epoch': 167.1}\n",
            " 84% 10370/12400 [1:40:03<15:43,  2.15it/s]{'loss': 0.0772, 'learning_rate': 3.274193548387097e-05, 'epoch': 167.26}\n",
            "{'loss': 0.0548, 'learning_rate': 3.258064516129033e-05, 'epoch': 167.42}\n",
            " 84% 10390/12400 [1:40:12<14:24,  2.32it/s]{'loss': 0.0346, 'learning_rate': 3.241935483870968e-05, 'epoch': 167.58}\n",
            "                                           {'loss': 0.1802, 'learning_rate': 3.2258064516129034e-05, 'epoch': 167.74}\n",
            "{'loss': 0.0679, 'learning_rate': 3.2096774193548393e-05, 'epoch': 167.9}\n",
            " 84% 10416/12400 [1:40:22<12:56,  2.56it/s][INFO|trainer.py:2625] 2022-05-19 05:47:34,615 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:47:34,615 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:47:34,615 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.40it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.88it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.59it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.09it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.45it/s]\u001b[A\n",
            "                                           \n",
            "                                 {'eval_loss': 2.743839740753174, 'eval_accuracy': 0.5342465753424658, 'eval_runtime': 3.2158, 'eval_samples_per_second': 68.102, 'eval_steps_per_second': 2.177, 'epoch': 168.0}\n",
            " 84% 10416/12400 [1:40:25<12:56,  2.56it/s]\n",
            "100% 7/7 [00:00<00:00,  7.95it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:47:37,833 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10416\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:47:37,837 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10416/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:47:39,539 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10416/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:47:39,540 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10416/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:47:43,453 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10292] due to args.save_total_limit\n",
            " 84% 10420/12400 [1:40:35<51:31,  1.56s/it]{'loss': 0.0517, 'learning_rate': 3.193548387096774e-05, 'epoch': 168.06}\n",
            "{'loss': 0.1255, 'learning_rate': 3.17741935483871e-05, 'epoch': 168.23}\n",
            "{'loss': 0.0416, 'learning_rate': 3.161290322580645e-05, 'epoch': 168.39}\n",
            "{'loss': 0.0454, 'learning_rate': 3.1451612903225806e-05, 'epoch': 168.55}\n",
            "                                           {'loss': 0.0821, 'learning_rate': 3.1290322580645166e-05, 'epoch': 168.71}\n",
            "                                           {'loss': 0.1364, 'learning_rate': 3.112903225806452e-05, 'epoch': 168.87}\n",
            " 84% 10478/12400 [1:41:00<12:36,  2.54it/s][INFO|trainer.py:2625] 2022-05-19 05:48:12,542 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:48:12,542 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:48:12,542 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.25it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.12it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  5.93it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.58it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.02it/s]\u001b[A\n",
            "                                           \n",
            "                                 {'eval_loss': 2.6888482570648193, 'eval_accuracy': 0.5296803652968036, 'eval_runtime': 3.3339, 'eval_samples_per_second': 65.69, 'eval_steps_per_second': 2.1, 'epoch': 169.0}\n",
            " 84% 10478/12400 [1:41:03<12:36,  2.54it/s]\n",
            "100% 7/7 [00:01<00:00,  7.56it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:48:15,878 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10478\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:48:15,883 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10478/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:48:17,604 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10478/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:48:17,605 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10478/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:48:21,217 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10354] due to args.save_total_limit\n",
            "{'loss': 0.0672, 'learning_rate': 3.096774193548387e-05, 'epoch': 169.03}\n",
            " 85% 10490/12400 [1:41:16<16:26,  1.94it/s]{'loss': 0.128, 'learning_rate': 3.0806451612903225e-05, 'epoch': 169.19}\n",
            "                                           {'loss': 0.0408, 'learning_rate': 3.0645161290322585e-05, 'epoch': 169.35}\n",
            "{'loss': 0.0852, 'learning_rate': 3.0483870967741935e-05, 'epoch': 169.52}\n",
            " 85% 10520/12400 [1:41:29<13:21,  2.35it/s]{'loss': 0.1, 'learning_rate': 3.032258064516129e-05, 'epoch': 169.68}\n",
            "                                           {'loss': 0.1056, 'learning_rate': 3.0161290322580648e-05, 'epoch': 169.84}\n",
            "{'loss': 0.1389, 'learning_rate': 3e-05, 'epoch': 170.0}\n",
            " 85% 10540/12400 [1:41:37<12:03,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 05:48:49,841 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:48:49,841 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:48:49,841 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.26it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  4.95it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  5.82it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.50it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  6.99it/s]\u001b[A\n",
            "100% 7/7 [00:01<00:00,  7.59it/s]\u001b[A{'eval_loss': 2.6394076347351074, 'eval_accuracy': 0.5342465753424658, 'eval_runtime': 3.2882, 'eval_samples_per_second': 66.602, 'eval_steps_per_second': 2.129, 'epoch': 170.0}\n",
            "                                           \n",
            " 85% 10540/12400 [1:41:41<12:03,  2.57it/s]\n",
            "100% 7/7 [00:01<00:00,  7.59it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:48:53,132 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10540\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:48:53,135 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10540/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:48:54,824 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10540/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:48:54,825 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10540/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:48:58,552 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10416] due to args.save_total_limit\n",
            " 85% 10550/12400 [1:41:53<18:06,  1.70it/s]{'loss': 0.0662, 'learning_rate': 2.9838709677419357e-05, 'epoch': 170.16}\n",
            " 85% 10560/12400 [1:41:57<14:00,  2.19it/s]{'loss': 0.1094, 'learning_rate': 2.967741935483871e-05, 'epoch': 170.32}\n",
            "                                           {'loss': 0.0659, 'learning_rate': 2.9516129032258067e-05, 'epoch': 170.48}\n",
            " 85% 10580/12400 [1:42:06<12:43,  2.38it/s]{'loss': 0.0732, 'learning_rate': 2.9354838709677417e-05, 'epoch': 170.65}\n",
            "{'loss': 0.073, 'learning_rate': 2.9193548387096776e-05, 'epoch': 170.81}\n",
            "{'loss': 0.0651, 'learning_rate': 2.9032258064516133e-05, 'epoch': 170.97}\n",
            " 86% 10602/12400 [1:42:15<11:45,  2.55it/s][INFO|trainer.py:2625] 2022-05-19 05:49:27,369 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:49:27,369 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:49:27,370 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.55it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.80it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.53it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.08it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.43it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.644637107849121, 'eval_accuracy': 0.5570776255707762, 'eval_runtime': 3.181, 'eval_samples_per_second': 68.846, 'eval_steps_per_second': 2.201, 'epoch': 171.0}\n",
            " 86% 10602/12400 [1:42:18<11:45,  2.55it/s]\n",
            "100% 7/7 [00:00<00:00,  8.03it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:49:30,553 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10602\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:49:30,557 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10602/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:49:32,245 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10602/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:49:32,246 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10602/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:49:36,157 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10478] due to args.save_total_limit\n",
            " 86% 10610/12400 [1:42:30<21:28,  1.39it/s]{'loss': 0.0465, 'learning_rate': 2.8870967741935483e-05, 'epoch': 171.13}\n",
            "{'loss': 0.0854, 'learning_rate': 2.8709677419354843e-05, 'epoch': 171.29}\n",
            "{'loss': 0.0744, 'learning_rate': 2.8548387096774192e-05, 'epoch': 171.45}\n",
            "{'loss': 0.0664, 'learning_rate': 2.838709677419355e-05, 'epoch': 171.61}\n",
            "                                           {'loss': 0.0596, 'learning_rate': 2.822580645161291e-05, 'epoch': 171.77}\n",
            "                                           {'loss': 0.08, 'learning_rate': 2.806451612903226e-05, 'epoch': 171.94}\n",
            " 86% 10664/12400 [1:42:52<11:18,  2.56it/s][INFO|trainer.py:2625] 2022-05-19 05:50:04,918 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:50:04,918 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:50:04,918 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.39it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.69it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.39it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.34it/s]\u001b[A\n",
            "                                           \n",
            " 86% 10664/12400 [1:42:56<11:18,  2.56it/s]\n",
            "100% 7/7 [00:00<00:00,  7.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:50:08,099 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10664\n",
            "{'eval_loss': 2.632594108581543, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.179, 'eval_samples_per_second': 68.89, 'eval_steps_per_second': 2.202, 'epoch': 172.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:50:08,103 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10664/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:50:09,802 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10664/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:50:09,804 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10664/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:50:13,633 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10540] due to args.save_total_limit\n",
            " 86% 10670/12400 [1:43:06<29:14,  1.01s/it]{'loss': 0.0871, 'learning_rate': 2.7903225806451615e-05, 'epoch': 172.1}\n",
            "{'loss': 0.0671, 'learning_rate': 2.7741935483870968e-05, 'epoch': 172.26}\n",
            " 86% 10690/12400 [1:43:15<12:54,  2.21it/s]{'loss': 0.0967, 'learning_rate': 2.7580645161290324e-05, 'epoch': 172.42}\n",
            "                                           {'loss': 0.1139, 'learning_rate': 2.7419354838709678e-05, 'epoch': 172.58}\n",
            " 86% 10710/12400 [1:43:24<12:17,  2.29it/s]{'loss': 0.0934, 'learning_rate': 2.7258064516129034e-05, 'epoch': 172.74}\n",
            " 86% 10720/12400 [1:43:28<10:55,  2.56it/s]{'loss': 0.0952, 'learning_rate': 2.709677419354839e-05, 'epoch': 172.9}\n",
            " 86% 10726/12400 [1:43:30<10:28,  2.67it/s][INFO|trainer.py:2625] 2022-05-19 05:50:42,619 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:50:42,619 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:50:42,619 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.41it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.20it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.77it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.18it/s]\u001b[A\n",
            "                                           \n",
            "                                 {'eval_loss': 2.675231456756592, 'eval_accuracy': 0.54337899543379, 'eval_runtime': 3.2202, 'eval_samples_per_second': 68.008, 'eval_steps_per_second': 2.174, 'epoch': 173.0}\n",
            " 86% 10726/12400 [1:43:33<10:28,  2.67it/s]\n",
            "100% 7/7 [00:01<00:00,  7.74it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:50:45,842 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10726\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:50:45,849 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10726/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:50:47,539 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10726/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:50:47,540 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10726/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:50:51,322 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10602] due to args.save_total_limit\n",
            " 87% 10730/12400 [1:43:42<42:52,  1.54s/it]{'loss': 0.1437, 'learning_rate': 2.6935483870967744e-05, 'epoch': 173.06}\n",
            "                                           {'loss': 0.0502, 'learning_rate': 2.67741935483871e-05, 'epoch': 173.23}\n",
            " 87% 10750/12400 [1:43:52<11:58,  2.30it/s]{'loss': 0.1087, 'learning_rate': 2.661290322580645e-05, 'epoch': 173.39}\n",
            " 87% 10760/12400 [1:43:56<12:03,  2.27it/s]{'loss': 0.0852, 'learning_rate': 2.645161290322581e-05, 'epoch': 173.55}\n",
            "{'loss': 0.1102, 'learning_rate': 2.629032258064516e-05, 'epoch': 173.71}\n",
            "                                           {'loss': 0.0884, 'learning_rate': 2.6129032258064516e-05, 'epoch': 173.87}\n",
            " 87% 10788/12400 [1:44:07<10:26,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 05:51:20,095 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:51:20,096 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:51:20,096 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.41it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.66it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.41it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.33it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.641814947128296, 'eval_accuracy': 0.54337899543379, 'eval_runtime': 3.1879, 'eval_samples_per_second': 68.697, 'eval_steps_per_second': 2.196, 'epoch': 174.0}\n",
            " 87% 10788/12400 [1:44:11<10:26,  2.57it/s]\n",
            "100% 7/7 [00:00<00:00,  7.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:51:23,286 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10788\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:51:23,289 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10788/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:51:25,017 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10788/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:51:25,018 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10788/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:51:28,785 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10664] due to args.save_total_limit\n",
            " 87% 10790/12400 [1:44:19<1:11:33,  2.67s/it]{'loss': 0.0993, 'learning_rate': 2.5967741935483876e-05, 'epoch': 174.03}\n",
            " 87% 10800/12400 [1:44:24<13:48,  1.93it/s]{'loss': 0.1303, 'learning_rate': 2.5806451612903226e-05, 'epoch': 174.19}\n",
            "{'loss': 0.0403, 'learning_rate': 2.5645161290322582e-05, 'epoch': 174.35}\n",
            " 87% 10820/12400 [1:44:33<11:17,  2.33it/s]{'loss': 0.1109, 'learning_rate': 2.5483870967741935e-05, 'epoch': 174.52}\n",
            "{'loss': 0.03, 'learning_rate': 2.532258064516129e-05, 'epoch': 174.68}\n",
            "                                           {'loss': 0.0283, 'learning_rate': 2.5161290322580645e-05, 'epoch': 174.84}\n",
            "{'loss': 0.0739, 'learning_rate': 2.5e-05, 'epoch': 175.0}\n",
            " 88% 10850/12400 [1:44:45<10:01,  2.58it/s][INFO|trainer.py:2625] 2022-05-19 05:51:57,475 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:51:57,475 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:51:57,475 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.42it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.20it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.73it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.16it/s]\u001b[A\n",
            "{'eval_loss': 2.6942825317382812, 'eval_accuracy': 0.5251141552511416, 'eval_runtime': 3.2138, 'eval_samples_per_second': 68.144, 'eval_steps_per_second': 2.178, 'epoch': 175.0}\n",
            "\n",
            " 88% 10850/12400 [1:44:48<10:01,  2.58it/s]\n",
            "100% 7/7 [00:01<00:00,  7.71it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:52:00,691 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10850\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:52:00,696 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10850/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:52:02,422 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10850/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:52:02,423 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10850/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:52:06,152 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10726] due to args.save_total_limit\n",
            " 88% 10860/12400 [1:45:01<15:08,  1.70it/s]{'loss': 0.0946, 'learning_rate': 2.4838709677419354e-05, 'epoch': 175.16}\n",
            "                                           {'loss': 0.0542, 'learning_rate': 2.467741935483871e-05, 'epoch': 175.32}\n",
            "{'loss': 0.0926, 'learning_rate': 2.4516129032258064e-05, 'epoch': 175.48}\n",
            "{'loss': 0.0424, 'learning_rate': 2.435483870967742e-05, 'epoch': 175.65}\n",
            " 88% 10900/12400 [1:45:18<10:16,  2.43it/s]{'loss': 0.0867, 'learning_rate': 2.4193548387096777e-05, 'epoch': 175.81}\n",
            "{'loss': 0.0797, 'learning_rate': 2.403225806451613e-05, 'epoch': 175.97}\n",
            " 88% 10912/12400 [1:45:22<09:48,  2.53it/s][INFO|trainer.py:2625] 2022-05-19 05:52:35,109 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:52:35,109 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:52:35,109 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.00it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.94it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.36it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.61it/s]\u001b[A\n",
            "                                           \n",
            " 88% 10912/12400 [1:45:26<09:48,  2.53it/s]\n",
            "{'eval_loss': 2.6624839305877686, 'eval_accuracy': 0.5159817351598174, 'eval_runtime': 3.1921, 'eval_samples_per_second': 68.607, 'eval_steps_per_second': 2.193, 'epoch': 176.0}\n",
            "100% 7/7 [00:00<00:00,  8.05it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:52:38,304 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10912\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:52:38,309 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10912/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:52:40,025 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10912/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:52:40,026 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10912/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:52:43,812 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10788] due to args.save_total_limit\n",
            " 88% 10920/12400 [1:45:37<17:36,  1.40it/s]{'loss': 0.1018, 'learning_rate': 2.3870967741935486e-05, 'epoch': 176.13}\n",
            " 88% 10930/12400 [1:45:42<11:12,  2.19it/s]{'loss': 0.0395, 'learning_rate': 2.370967741935484e-05, 'epoch': 176.29}\n",
            "{'loss': 0.0726, 'learning_rate': 2.3548387096774193e-05, 'epoch': 176.45}\n",
            "{'loss': 0.1249, 'learning_rate': 2.338709677419355e-05, 'epoch': 176.61}\n",
            "{'loss': 0.1385, 'learning_rate': 2.3225806451612906e-05, 'epoch': 176.77}\n",
            " 88% 10970/12400 [1:46:01<09:22,  2.54it/s]{'loss': 0.0503, 'learning_rate': 2.306451612903226e-05, 'epoch': 176.94}\n",
            " 88% 10974/12400 [1:46:02<09:20,  2.54it/s][INFO|trainer.py:2625] 2022-05-19 05:53:14,491 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:53:14,491 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:53:14,491 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.32it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.36it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.19it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.74it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.15it/s]\u001b[A\n",
            "                                           \n",
            " 88% 10974/12400 [1:46:05<09:20,  2.54it/s]\n",
            "100% 7/7 [00:01<00:00,  7.72it/s]{'eval_loss': 2.653620481491089, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.258, 'eval_samples_per_second': 67.219, 'eval_steps_per_second': 2.149, 'epoch': 177.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:53:17,751 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10974\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:53:17,756 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10974/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:53:19,491 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10974/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:53:19,493 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10974/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:53:23,130 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10850] due to args.save_total_limit\n",
            " 89% 10980/12400 [1:46:15<23:35,  1.00it/s]{'loss': 0.1048, 'learning_rate': 2.2903225806451615e-05, 'epoch': 177.1}\n",
            "{'loss': 0.0921, 'learning_rate': 2.274193548387097e-05, 'epoch': 177.26}\n",
            "                                           {'loss': 0.0711, 'learning_rate': 2.258064516129032e-05, 'epoch': 177.42}\n",
            "{'loss': 0.0672, 'learning_rate': 2.2419354838709678e-05, 'epoch': 177.58}\n",
            "{'loss': 0.0504, 'learning_rate': 2.2258064516129034e-05, 'epoch': 177.74}\n",
            "                                           {'loss': 0.1232, 'learning_rate': 2.2096774193548388e-05, 'epoch': 177.9}\n",
            " 89% 11036/12400 [1:46:39<08:49,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 05:53:51,814 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:53:51,814 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:53:51,814 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.05it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.20it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.82it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.25it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.56it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.712564706802368, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.1645, 'eval_samples_per_second': 69.206, 'eval_steps_per_second': 2.212, 'epoch': 178.0}\n",
            " 89% 11036/12400 [1:46:43<08:49,  2.57it/s]\n",
            "100% 7/7 [00:00<00:00,  8.01it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:53:54,981 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11036\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:53:54,984 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11036/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:53:56,709 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11036/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:53:56,710 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11036/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:54:00,552 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10912] due to args.save_total_limit\n",
            " 89% 11040/12400 [1:46:52<35:16,  1.56s/it]{'loss': 0.0871, 'learning_rate': 2.1935483870967744e-05, 'epoch': 178.06}\n",
            " 89% 11050/12400 [1:46:56<10:51,  2.07it/s]{'loss': 0.0541, 'learning_rate': 2.1774193548387097e-05, 'epoch': 178.23}\n",
            "{'loss': 0.0609, 'learning_rate': 2.1612903225806454e-05, 'epoch': 178.39}\n",
            "                                           {'loss': 0.0702, 'learning_rate': 2.1451612903225807e-05, 'epoch': 178.55}\n",
            " 89% 11080/12400 [1:47:10<09:24,  2.34it/s]{'loss': 0.0432, 'learning_rate': 2.129032258064516e-05, 'epoch': 178.71}\n",
            "                                           {'loss': 0.058, 'learning_rate': 2.112903225806452e-05, 'epoch': 178.87}\n",
            " 90% 11098/12400 [1:47:17<08:26,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 05:54:29,318 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:54:29,319 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:54:29,319 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.50it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.70it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.48it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.01it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.38it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.6966514587402344, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.2012, 'eval_samples_per_second': 68.412, 'eval_steps_per_second': 2.187, 'epoch': 179.0}\n",
            " 90% 11098/12400 [1:47:20<08:26,  2.57it/s]\n",
            "100% 7/7 [00:00<00:00,  7.89it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:54:32,522 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11098\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:54:32,528 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11098/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:54:34,255 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11098/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:54:34,255 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11098/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:54:38,062 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-10974] due to args.save_total_limit\n",
            " 90% 11100/12400 [1:47:28<57:46,  2.67s/it]  {'loss': 0.0632, 'learning_rate': 2.0967741935483873e-05, 'epoch': 179.03}\n",
            " 90% 11110/12400 [1:47:33<11:02,  1.95it/s]{'loss': 0.0929, 'learning_rate': 2.0806451612903226e-05, 'epoch': 179.19}\n",
            " 90% 11120/12400 [1:47:38<09:07,  2.34it/s]{'loss': 0.0829, 'learning_rate': 2.0645161290322582e-05, 'epoch': 179.35}\n",
            " 90% 11130/12400 [1:47:42<08:48,  2.40it/s]{'loss': 0.0729, 'learning_rate': 2.0483870967741936e-05, 'epoch': 179.52}\n",
            "                                           {'loss': 0.0847, 'learning_rate': 2.0322580645161292e-05, 'epoch': 179.68}\n",
            "                                           {'loss': 0.1184, 'learning_rate': 2.0161290322580645e-05, 'epoch': 179.84}\n",
            " 90% 11160/12400 [1:47:54<08:02,  2.57it/s]{'loss': 0.0603, 'learning_rate': 2e-05, 'epoch': 180.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 05:55:06,605 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:55:06,605 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:55:06,605 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.29it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.52it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.25it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.83it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.26it/s]\u001b[A\n",
            "                                           \n",
            " 90% 11160/12400 [1:47:57<08:02,  2.57it/s]\n",
            "100% 7/7 [00:01<00:00,  7.87it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:55:09,806 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11160\n",
            "{'eval_loss': 2.763531446456909, 'eval_accuracy': 0.5296803652968036, 'eval_runtime': 3.2, 'eval_samples_per_second': 68.438, 'eval_steps_per_second': 2.188, 'epoch': 180.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:55:09,810 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11160/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:55:11,486 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11160/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:55:11,487 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11160/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:55:15,228 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11036] due to args.save_total_limit\n",
            " 90% 11170/12400 [1:48:09<11:51,  1.73it/s]{'loss': 0.0685, 'learning_rate': 1.9838709677419358e-05, 'epoch': 180.16}\n",
            "{'loss': 0.0734, 'learning_rate': 1.967741935483871e-05, 'epoch': 180.32}\n",
            "{'loss': 0.066, 'learning_rate': 1.9516129032258064e-05, 'epoch': 180.48}\n",
            " 90% 11200/12400 [1:48:22<08:27,  2.36it/s]{'loss': 0.1142, 'learning_rate': 1.935483870967742e-05, 'epoch': 180.65}\n",
            "{'loss': 0.043, 'learning_rate': 1.9193548387096774e-05, 'epoch': 180.81}\n",
            "{'loss': 0.0644, 'learning_rate': 1.9032258064516127e-05, 'epoch': 180.97}\n",
            " 90% 11222/12400 [1:48:31<07:43,  2.54it/s][INFO|trainer.py:2625] 2022-05-19 05:55:43,924 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:55:43,924 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:55:43,924 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.20it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.23it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.06it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.69it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.19it/s]\u001b[A\n",
            "                                           \n",
            " 90% 11222/12400 [1:48:35<07:43,  2.54it/s]\n",
            "100% 7/7 [00:01<00:00,  7.84it/s]\u001b[A\n",
            "                                 {'eval_loss': 2.7326576709747314, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.2026, 'eval_samples_per_second': 68.382, 'eval_steps_per_second': 2.186, 'epoch': 181.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 05:55:47,128 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11222\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:55:47,133 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11222/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:55:48,830 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11222/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:55:48,831 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11222/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:55:52,653 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11098] due to args.save_total_limit\n",
            "{'loss': 0.0553, 'learning_rate': 1.8870967741935487e-05, 'epoch': 181.13}\n",
            "{'loss': 0.0793, 'learning_rate': 1.870967741935484e-05, 'epoch': 181.29}\n",
            "                                           {'loss': 0.0489, 'learning_rate': 1.8548387096774193e-05, 'epoch': 181.45}\n",
            "                                           {'loss': 0.0988, 'learning_rate': 1.838709677419355e-05, 'epoch': 181.61}\n",
            " 91% 11270/12400 [1:49:03<07:52,  2.39it/s]{'loss': 0.0415, 'learning_rate': 1.8225806451612903e-05, 'epoch': 181.77}\n",
            " 91% 11280/12400 [1:49:07<07:14,  2.58it/s]{'loss': 0.0624, 'learning_rate': 1.806451612903226e-05, 'epoch': 181.94}\n",
            " 91% 11284/12400 [1:49:08<07:16,  2.56it/s][INFO|trainer.py:2625] 2022-05-19 05:56:21,072 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:56:21,072 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:56:21,072 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.45it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.25it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.87it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.34it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.6985673904418945, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.2026, 'eval_samples_per_second': 68.381, 'eval_steps_per_second': 2.186, 'epoch': 182.0}\n",
            " 91% 11284/12400 [1:49:12<07:16,  2.56it/s]\n",
            "100% 7/7 [00:01<00:00,  7.86it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:56:24,277 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11284\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:56:24,282 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11284/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:56:25,979 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11284/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:56:25,980 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11284/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:56:30,109 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11160] due to args.save_total_limit\n",
            "{'loss': 0.0453, 'learning_rate': 1.7903225806451616e-05, 'epoch': 182.1}\n",
            " 91% 11300/12400 [1:49:27<08:30,  2.16it/s]{'loss': 0.0571, 'learning_rate': 1.774193548387097e-05, 'epoch': 182.26}\n",
            "{'loss': 0.0816, 'learning_rate': 1.7580645161290325e-05, 'epoch': 182.42}\n",
            "{'loss': 0.1138, 'learning_rate': 1.741935483870968e-05, 'epoch': 182.58}\n",
            "{'loss': 0.1133, 'learning_rate': 1.725806451612903e-05, 'epoch': 182.74}\n",
            "{'loss': 0.0356, 'learning_rate': 1.7096774193548388e-05, 'epoch': 182.9}\n",
            " 92% 11346/12400 [1:49:46<06:44,  2.61it/s][INFO|trainer.py:2625] 2022-05-19 05:56:58,617 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:56:58,617 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:56:58,617 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.29it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.42it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.22it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.81it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.28it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.7225899696350098, 'eval_accuracy': 0.54337899543379, 'eval_runtime': 3.2101, 'eval_samples_per_second': 68.223, 'eval_steps_per_second': 2.181, 'epoch': 183.0}\n",
            " 92% 11346/12400 [1:49:49<06:44,  2.61it/s]\n",
            "100% 7/7 [00:01<00:00,  7.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:57:01,831 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11346\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:57:01,834 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11346/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:57:03,533 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11346/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:57:03,534 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11346/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:57:08,828 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11222] due to args.save_total_limit\n",
            " 92% 11350/12400 [1:50:00<29:52,  1.71s/it]{'loss': 0.0725, 'learning_rate': 1.693548387096774e-05, 'epoch': 183.06}\n",
            "{'loss': 0.0746, 'learning_rate': 1.6774193548387098e-05, 'epoch': 183.23}\n",
            "{'loss': 0.1357, 'learning_rate': 1.6612903225806454e-05, 'epoch': 183.39}\n",
            " 92% 11380/12400 [1:50:14<07:16,  2.34it/s]{'loss': 0.0722, 'learning_rate': 1.6451612903225807e-05, 'epoch': 183.55}\n",
            "{'loss': 0.0903, 'learning_rate': 1.6290322580645164e-05, 'epoch': 183.71}\n",
            "{'loss': 0.0658, 'learning_rate': 1.6129032258064517e-05, 'epoch': 183.87}\n",
            " 92% 11408/12400 [1:50:25<06:26,  2.56it/s][INFO|trainer.py:2625] 2022-05-19 05:57:37,369 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:57:37,369 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:57:37,369 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.54it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.73it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.44it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.96it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.32it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.718945026397705, 'eval_accuracy': 0.5251141552511416, 'eval_runtime': 3.2137, 'eval_samples_per_second': 68.145, 'eval_steps_per_second': 2.178, 'epoch': 184.0}\n",
            " 92% 11408/12400 [1:50:28<06:26,  2.56it/s]\n",
            "100% 7/7 [00:00<00:00,  7.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:57:40,585 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11408\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:57:40,588 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11408/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:57:42,273 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11408/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:57:42,274 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11408/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:57:46,081 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11284] due to args.save_total_limit\n",
            " 92% 11410/12400 [1:50:37<43:48,  2.66s/it]{'loss': 0.1016, 'learning_rate': 1.596774193548387e-05, 'epoch': 184.03}\n",
            "                                           {'loss': 0.0959, 'learning_rate': 1.5806451612903226e-05, 'epoch': 184.19}\n",
            "                                           {'loss': 0.0747, 'learning_rate': 1.5645161290322583e-05, 'epoch': 184.35}\n",
            " 92% 11440/12400 [1:50:50<06:54,  2.32it/s]{'loss': 0.1035, 'learning_rate': 1.5483870967741936e-05, 'epoch': 184.52}\n",
            "{'loss': 0.0661, 'learning_rate': 1.5322580645161292e-05, 'epoch': 184.68}\n",
            " 92% 11460/12400 [1:50:58<06:20,  2.47it/s]{'loss': 0.0622, 'learning_rate': 1.5161290322580646e-05, 'epoch': 184.84}\n",
            " 92% 11470/12400 [1:51:02<06:03,  2.56it/s]{'loss': 0.0783, 'learning_rate': 1.5e-05, 'epoch': 185.0}\n",
            " 92% 11470/12400 [1:51:02<06:03,  2.56it/s][INFO|trainer.py:2625] 2022-05-19 05:58:14,707 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:58:14,707 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:58:14,707 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.52it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.31it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.88it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.26it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.7646567821502686, 'eval_accuracy': 0.5205479452054794, 'eval_runtime': 3.2399, 'eval_samples_per_second': 67.595, 'eval_steps_per_second': 2.161, 'epoch': 185.0}\n",
            " 92% 11470/12400 [1:51:06<06:03,  2.56it/s]\n",
            "100% 7/7 [00:01<00:00,  7.83it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:58:17,949 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11470\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:58:17,954 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11470/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:58:19,645 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11470/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:58:19,646 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11470/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:58:23,326 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11346] due to args.save_total_limit\n",
            " 93% 11480/12400 [1:51:18<08:49,  1.74it/s]{'loss': 0.0972, 'learning_rate': 1.4838709677419355e-05, 'epoch': 185.16}\n",
            "{'loss': 0.0626, 'learning_rate': 1.4677419354838708e-05, 'epoch': 185.32}\n",
            "{'loss': 0.0919, 'learning_rate': 1.4516129032258066e-05, 'epoch': 185.48}\n",
            "{'loss': 0.0634, 'learning_rate': 1.4354838709677421e-05, 'epoch': 185.65}\n",
            "{'loss': 0.0693, 'learning_rate': 1.4193548387096774e-05, 'epoch': 185.81}\n",
            " 93% 11530/12400 [1:51:39<05:39,  2.57it/s]{'loss': 0.0968, 'learning_rate': 1.403225806451613e-05, 'epoch': 185.97}\n",
            " 93% 11532/12400 [1:51:39<05:41,  2.54it/s][INFO|trainer.py:2625] 2022-05-19 05:58:51,688 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:58:51,688 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:58:51,688 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.39it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  7.04it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  7.50it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.76it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.93it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.6743624210357666, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.2469, 'eval_samples_per_second': 67.45, 'eval_steps_per_second': 2.156, 'epoch': 186.0}\n",
            " 93% 11532/12400 [1:51:43<05:41,  2.54it/s]\n",
            "100% 7/7 [00:00<00:00,  8.37it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:58:54,938 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11532\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:58:54,941 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11532/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:58:56,651 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11532/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:58:56,651 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11532/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:59:00,420 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11408] due to args.save_total_limit\n",
            "{'loss': 0.0507, 'learning_rate': 1.3870967741935484e-05, 'epoch': 186.13}\n",
            "{'loss': 0.0581, 'learning_rate': 1.3709677419354839e-05, 'epoch': 186.29}\n",
            " 93% 11560/12400 [1:52:02<06:02,  2.32it/s]{'loss': 0.0662, 'learning_rate': 1.3548387096774195e-05, 'epoch': 186.45}\n",
            "{'loss': 0.0458, 'learning_rate': 1.338709677419355e-05, 'epoch': 186.61}\n",
            " 93% 11580/12400 [1:52:11<05:45,  2.38it/s]{'loss': 0.0866, 'learning_rate': 1.3225806451612905e-05, 'epoch': 186.77}\n",
            " 93% 11590/12400 [1:52:15<05:16,  2.56it/s]{'loss': 0.1085, 'learning_rate': 1.3064516129032258e-05, 'epoch': 186.94}\n",
            " 94% 11594/12400 [1:52:16<05:15,  2.55it/s][INFO|trainer.py:2625] 2022-05-19 05:59:28,928 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 05:59:28,928 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 05:59:28,928 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.30it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.09it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.68it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.08it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.7532858848571777, 'eval_accuracy': 0.5251141552511416, 'eval_runtime': 3.2822, 'eval_samples_per_second': 66.724, 'eval_steps_per_second': 2.133, 'epoch': 187.0}\n",
            " 94% 11594/12400 [1:52:20<05:15,  2.55it/s]\n",
            "100% 7/7 [00:01<00:00,  7.66it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 05:59:32,213 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11594\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 05:59:32,217 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11594/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 05:59:33,902 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11594/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 05:59:33,903 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11594/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 05:59:37,607 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11470] due to args.save_total_limit\n",
            "{'loss': 0.0626, 'learning_rate': 1.2903225806451613e-05, 'epoch': 187.1}\n",
            "                                           {'loss': 0.0442, 'learning_rate': 1.2741935483870968e-05, 'epoch': 187.26}\n",
            "{'loss': 0.0709, 'learning_rate': 1.2580645161290322e-05, 'epoch': 187.42}\n",
            " 94% 11630/12400 [1:52:43<05:36,  2.29it/s]{'loss': 0.0795, 'learning_rate': 1.2419354838709677e-05, 'epoch': 187.58}\n",
            "                                           {'loss': 0.0667, 'learning_rate': 1.2258064516129032e-05, 'epoch': 187.74}\n",
            " 94% 11650/12400 [1:52:52<04:58,  2.51it/s]{'loss': 0.0998, 'learning_rate': 1.2096774193548388e-05, 'epoch': 187.9}\n",
            " 94% 11656/12400 [1:52:54<04:50,  2.56it/s][INFO|trainer.py:2625] 2022-05-19 06:00:06,484 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:00:06,484 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:00:06,484 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00, 13.21it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  5.79it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.14it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  6.67it/s]\u001b[A\n",
            "                                           \n",
            " 94% 11656/12400 [1:52:57<04:50,  2.56it/s]\n",
            "100% 7/7 [00:01<00:00,  7.30it/s]\u001b[A{'eval_loss': 2.7282040119171143, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.2487, 'eval_samples_per_second': 67.411, 'eval_steps_per_second': 2.155, 'epoch': 188.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 06:00:09,735 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11656\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:00:09,739 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11656/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:00:11,437 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11656/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:00:11,438 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11656/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:00:15,431 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11532] due to args.save_total_limit\n",
            "{'loss': 0.0645, 'learning_rate': 1.1935483870967743e-05, 'epoch': 188.06}\n",
            " 94% 11670/12400 [1:53:12<05:48,  2.09it/s]{'loss': 0.096, 'learning_rate': 1.1774193548387096e-05, 'epoch': 188.23}\n",
            "{'loss': 0.0887, 'learning_rate': 1.1612903225806453e-05, 'epoch': 188.39}\n",
            " 94% 11690/12400 [1:53:20<05:04,  2.33it/s]{'loss': 0.0602, 'learning_rate': 1.1451612903225808e-05, 'epoch': 188.55}\n",
            "{'loss': 0.064, 'learning_rate': 1.129032258064516e-05, 'epoch': 188.71}\n",
            "                                           {'loss': 0.035, 'learning_rate': 1.1129032258064517e-05, 'epoch': 188.87}\n",
            " 94% 11718/12400 [1:53:31<04:26,  2.56it/s][INFO|trainer.py:2625] 2022-05-19 06:00:44,065 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:00:44,065 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:00:44,065 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.46it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.60it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.90it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.28it/s]\u001b[A\n",
            "                                           \n",
            " 94% 11718/12400 [1:53:35<04:26,  2.56it/s]\n",
            "100% 7/7 [00:01<00:00,  7.82it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 06:00:47,242 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11718\n",
            "{'eval_loss': 2.692866086959839, 'eval_accuracy': 0.54337899543379, 'eval_runtime': 3.175, 'eval_samples_per_second': 68.975, 'eval_steps_per_second': 2.205, 'epoch': 189.0}\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:00:47,245 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11718/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:00:48,962 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11718/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:00:48,964 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11718/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:00:52,717 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11594] due to args.save_total_limit\n",
            "                                           {'loss': 0.0441, 'learning_rate': 1.0967741935483872e-05, 'epoch': 189.03}\n",
            "{'loss': 0.0984, 'learning_rate': 1.0806451612903227e-05, 'epoch': 189.19}\n",
            "{'loss': 0.0812, 'learning_rate': 1.064516129032258e-05, 'epoch': 189.35}\n",
            "{'loss': 0.0473, 'learning_rate': 1.0483870967741936e-05, 'epoch': 189.52}\n",
            "                                           {'loss': 0.0791, 'learning_rate': 1.0322580645161291e-05, 'epoch': 189.68}\n",
            " 95% 11770/12400 [1:54:05<04:06,  2.55it/s]{'loss': 0.052, 'learning_rate': 1.0161290322580646e-05, 'epoch': 189.84}\n",
            " 95% 11780/12400 [1:54:09<03:53,  2.65it/s]{'loss': 0.0565, 'learning_rate': 1e-05, 'epoch': 190.0}\n",
            "[INFO|trainer.py:2625] 2022-05-19 06:01:21,273 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:01:21,274 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:01:21,274 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.40it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.69it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.41it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.93it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.33it/s]\u001b[A\n",
            "                                           \n",
            " 95% 11780/12400 [1:54:12<03:53,  2.65it/s]\n",
            "100% 7/7 [00:00<00:00,  7.87it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.6889631748199463, 'eval_accuracy': 0.5525114155251142, 'eval_runtime': 3.1937, 'eval_samples_per_second': 68.573, 'eval_steps_per_second': 2.192, 'epoch': 190.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 06:01:24,469 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11780\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:01:24,473 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11780/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:01:26,158 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11780/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:01:26,159 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11780/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:01:30,082 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11656] due to args.save_total_limit\n",
            " 95% 11790/12400 [1:54:24<05:56,  1.71it/s]{'loss': 0.0956, 'learning_rate': 9.838709677419356e-06, 'epoch': 190.16}\n",
            " 95% 11800/12400 [1:54:29<04:33,  2.19it/s]{'loss': 0.0991, 'learning_rate': 9.67741935483871e-06, 'epoch': 190.32}\n",
            " 95% 11810/12400 [1:54:33<04:16,  2.30it/s]{'loss': 0.1356, 'learning_rate': 9.516129032258064e-06, 'epoch': 190.48}\n",
            "{'loss': 0.1166, 'learning_rate': 9.35483870967742e-06, 'epoch': 190.65}\n",
            " 95% 11830/12400 [1:54:41<03:47,  2.51it/s]{'loss': 0.0608, 'learning_rate': 9.193548387096775e-06, 'epoch': 190.81}\n",
            " 95% 11840/12400 [1:54:46<03:38,  2.57it/s]{'loss': 0.107, 'learning_rate': 9.03225806451613e-06, 'epoch': 190.97}\n",
            " 96% 11842/12400 [1:54:46<03:39,  2.54it/s][INFO|trainer.py:2625] 2022-05-19 06:01:58,712 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:01:58,712 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:01:58,712 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.25it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.45it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.21it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.83it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.23it/s]\u001b[A\n",
            "                                           \n",
            " 96% 11842/12400 [1:54:49<03:39,  2.54it/s]\n",
            "100% 7/7 [00:01<00:00,  7.86it/s]\u001b[A\n",
            "                                 {'eval_loss': 2.724804401397705, 'eval_accuracy': 0.5525114155251142, 'eval_runtime': 3.1363, 'eval_samples_per_second': 69.828, 'eval_steps_per_second': 2.232, 'epoch': 191.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 06:02:01,850 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11842\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:02:01,853 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11842/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:02:03,547 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11842/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:02:03,548 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11842/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:02:07,188 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11718] due to args.save_total_limit\n",
            " 96% 11850/12400 [1:55:00<06:38,  1.38it/s]{'loss': 0.0483, 'learning_rate': 8.870967741935484e-06, 'epoch': 191.13}\n",
            "{'loss': 0.0482, 'learning_rate': 8.70967741935484e-06, 'epoch': 191.29}\n",
            " 96% 11870/12400 [1:55:10<03:47,  2.33it/s]{'loss': 0.0739, 'learning_rate': 8.548387096774194e-06, 'epoch': 191.45}\n",
            "{'loss': 0.094, 'learning_rate': 8.387096774193549e-06, 'epoch': 191.61}\n",
            "{'loss': 0.0987, 'learning_rate': 8.225806451612904e-06, 'epoch': 191.77}\n",
            "                                           {'loss': 0.0712, 'learning_rate': 8.064516129032258e-06, 'epoch': 191.94}\n",
            " 96% 11904/12400 [1:55:23<03:14,  2.55it/s][INFO|trainer.py:2625] 2022-05-19 06:02:36,040 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:02:36,040 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:02:36,041 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.38it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.82it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.57it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.09it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.44it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 2.7106704711914062, 'eval_accuracy': 0.547945205479452, 'eval_runtime': 3.184, 'eval_samples_per_second': 68.782, 'eval_steps_per_second': 2.199, 'epoch': 192.0}\n",
            " 96% 11904/12400 [1:55:27<03:14,  2.55it/s]\n",
            "100% 7/7 [00:00<00:00,  7.88it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 06:02:39,227 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11904\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:02:39,231 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11904/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:02:40,935 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11904/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:02:40,936 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11904/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:02:44,955 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11780] due to args.save_total_limit\n",
            " 96% 11910/12400 [1:55:37<08:19,  1.02s/it]{'loss': 0.0479, 'learning_rate': 7.903225806451613e-06, 'epoch': 192.1}\n",
            "{'loss': 0.0691, 'learning_rate': 7.741935483870968e-06, 'epoch': 192.26}\n",
            "                                           {'loss': 0.0627, 'learning_rate': 7.580645161290323e-06, 'epoch': 192.42}\n",
            " 96% 11940/12400 [1:55:51<03:15,  2.35it/s]{'loss': 0.0763, 'learning_rate': 7.419354838709678e-06, 'epoch': 192.58}\n",
            "{'loss': 0.0357, 'learning_rate': 7.258064516129033e-06, 'epoch': 192.74}\n",
            " 96% 11960/12400 [1:55:59<02:53,  2.53it/s]{'loss': 0.0646, 'learning_rate': 7.096774193548387e-06, 'epoch': 192.9}\n",
            " 96% 11966/12400 [1:56:01<02:50,  2.54it/s][INFO|trainer.py:2625] 2022-05-19 06:03:13,571 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:03:13,571 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:03:13,571 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.30it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.39it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.21it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.75it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.17it/s]\u001b[A\n",
            "                                           \n",
            " 96% 11966/12400 [1:56:04<02:50,  2.54it/s]\n",
            "{'eval_loss': 2.715763568878174, 'eval_accuracy': 0.547945205479452, 'eval_runtime': 3.285, 'eval_samples_per_second': 66.668, 'eval_steps_per_second': 2.131, 'epoch': 193.0}\n",
            "100% 7/7 [00:01<00:00,  7.68it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 06:03:16,859 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11966\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:03:16,863 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11966/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:03:18,568 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11966/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:03:18,569 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11966/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:03:22,139 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11842] due to args.save_total_limit\n",
            "{'loss': 0.0631, 'learning_rate': 6.935483870967742e-06, 'epoch': 193.06}\n",
            "{'loss': 0.1004, 'learning_rate': 6.774193548387098e-06, 'epoch': 193.23}\n",
            "{'loss': 0.1299, 'learning_rate': 6.6129032258064524e-06, 'epoch': 193.39}\n",
            "                                           {'loss': 0.0534, 'learning_rate': 6.451612903225806e-06, 'epoch': 193.55}\n",
            "{'loss': 0.0642, 'learning_rate': 6.290322580645161e-06, 'epoch': 193.71}\n",
            "{'loss': 0.0729, 'learning_rate': 6.129032258064516e-06, 'epoch': 193.87}\n",
            " 97% 12028/12400 [1:56:38<02:22,  2.60it/s][INFO|trainer.py:2625] 2022-05-19 06:03:50,675 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:03:50,675 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:03:50,675 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.59it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.85it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.56it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.07it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.46it/s]\u001b[A\n",
            "                                           \n",
            "{'eval_loss': 2.7245676517486572, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.1604, 'eval_samples_per_second': 69.296, 'eval_steps_per_second': 2.215, 'epoch': 194.0}\n",
            " 97% 12028/12400 [1:56:41<02:22,  2.60it/s]\n",
            "100% 7/7 [00:00<00:00,  7.98it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 06:03:53,838 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12028\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:03:53,841 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12028/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:03:55,549 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12028/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:03:55,550 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12028/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:03:59,569 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11904] due to args.save_total_limit\n",
            "{'loss': 0.0969, 'learning_rate': 5.967741935483872e-06, 'epoch': 194.03}\n",
            "{'loss': 0.038, 'learning_rate': 5.806451612903226e-06, 'epoch': 194.19}\n",
            " 97% 12050/12400 [1:56:59<02:32,  2.30it/s]{'loss': 0.0844, 'learning_rate': 5.64516129032258e-06, 'epoch': 194.35}\n",
            " 97% 12060/12400 [1:57:04<02:26,  2.31it/s]{'loss': 0.0574, 'learning_rate': 5.483870967741936e-06, 'epoch': 194.52}\n",
            "{'loss': 0.0801, 'learning_rate': 5.32258064516129e-06, 'epoch': 194.68}\n",
            "{'loss': 0.039, 'learning_rate': 5.161290322580646e-06, 'epoch': 194.84}\n",
            "{'loss': 0.0858, 'learning_rate': 5e-06, 'epoch': 195.0}\n",
            " 98% 12090/12400 [1:57:16<01:56,  2.67it/s][INFO|trainer.py:2625] 2022-05-19 06:04:28,004 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:04:28,004 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:04:28,004 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.47it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.73it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.48it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.97it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.31it/s]\u001b[A\n",
            "{'eval_loss': 2.7208468914031982, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.1855, 'eval_samples_per_second': 68.749, 'eval_steps_per_second': 2.197, 'epoch': 195.0}\n",
            "\n",
            " 98% 12090/12400 [1:57:19<01:56,  2.67it/s]\n",
            "100% 7/7 [00:00<00:00,  7.85it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 06:04:31,192 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12090\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:04:31,195 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12090/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:04:32,897 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12090/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:04:32,898 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12090/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:04:36,829 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-11966] due to args.save_total_limit\n",
            "                                           {'loss': 0.031, 'learning_rate': 4.838709677419355e-06, 'epoch': 195.16}\n",
            "                                           {'loss': 0.0914, 'learning_rate': 4.67741935483871e-06, 'epoch': 195.32}\n",
            " 98% 12120/12400 [1:57:40<02:01,  2.31it/s]{'loss': 0.0487, 'learning_rate': 4.516129032258065e-06, 'epoch': 195.48}\n",
            "{'loss': 0.0323, 'learning_rate': 4.35483870967742e-06, 'epoch': 195.65}\n",
            "{'loss': 0.0426, 'learning_rate': 4.193548387096774e-06, 'epoch': 195.81}\n",
            "{'loss': 0.05, 'learning_rate': 4.032258064516129e-06, 'epoch': 195.97}\n",
            " 98% 12152/12400 [1:57:53<01:36,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 06:05:05,539 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:05:05,540 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:05:05,540 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.25it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.47it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.26it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.84it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.27it/s]\u001b[A\n",
            "                                           \n",
            " 98% 12152/12400 [1:57:56<01:36,  2.57it/s]\n",
            "100% 7/7 [00:01<00:00,  7.84it/s]{'eval_loss': 2.7158117294311523, 'eval_accuracy': 0.5342465753424658, 'eval_runtime': 3.207, 'eval_samples_per_second': 68.287, 'eval_steps_per_second': 2.183, 'epoch': 196.0}\n",
            "\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 06:05:08,749 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12152\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:05:08,752 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12152/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:05:10,451 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12152/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:05:10,452 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12152/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:05:14,424 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12028] due to args.save_total_limit\n",
            "{'loss': 0.0596, 'learning_rate': 3.870967741935484e-06, 'epoch': 196.13}\n",
            "                                           {'loss': 0.0509, 'learning_rate': 3.709677419354839e-06, 'epoch': 196.29}\n",
            " 98% 12180/12400 [1:58:17<01:34,  2.33it/s]{'loss': 0.0648, 'learning_rate': 3.5483870967741936e-06, 'epoch': 196.45}\n",
            " 98% 12190/12400 [1:58:21<01:29,  2.34it/s]{'loss': 0.1033, 'learning_rate': 3.387096774193549e-06, 'epoch': 196.61}\n",
            " 98% 12200/12400 [1:58:25<01:23,  2.39it/s]{'loss': 0.1232, 'learning_rate': 3.225806451612903e-06, 'epoch': 196.77}\n",
            " 98% 12210/12400 [1:58:29<01:14,  2.56it/s]{'loss': 0.0401, 'learning_rate': 3.064516129032258e-06, 'epoch': 196.94}\n",
            " 98% 12214/12400 [1:58:30<01:12,  2.56it/s][INFO|trainer.py:2625] 2022-05-19 06:05:43,087 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:05:43,087 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:05:43,087 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.46it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.73it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.44it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.98it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.32it/s]\u001b[A\n",
            "                                           \n",
            " 98% 12214/12400 [1:58:34<01:12,  2.56it/s]\n",
            "100% 7/7 [00:00<00:00,  7.85it/s]\u001b[A{'eval_loss': 2.6989500522613525, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.1753, 'eval_samples_per_second': 68.969, 'eval_steps_per_second': 2.204, 'epoch': 197.0}\n",
            "\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 06:05:46,264 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12214\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:05:46,267 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12214/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:05:48,004 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12214/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:05:48,005 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12214/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:05:51,785 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12090] due to args.save_total_limit\n",
            "{'loss': 0.1099, 'learning_rate': 2.903225806451613e-06, 'epoch': 197.1}\n",
            "{'loss': 0.0705, 'learning_rate': 2.741935483870968e-06, 'epoch': 197.26}\n",
            "{'loss': 0.0428, 'learning_rate': 2.580645161290323e-06, 'epoch': 197.42}\n",
            " 99% 12250/12400 [1:58:57<01:05,  2.30it/s]{'loss': 0.0864, 'learning_rate': 2.4193548387096776e-06, 'epoch': 197.58}\n",
            " 99% 12260/12400 [1:59:02<01:01,  2.27it/s]{'loss': 0.0812, 'learning_rate': 2.2580645161290324e-06, 'epoch': 197.74}\n",
            " 99% 12270/12400 [1:59:06<00:51,  2.53it/s]{'loss': 0.1003, 'learning_rate': 2.096774193548387e-06, 'epoch': 197.9}\n",
            " 99% 12276/12400 [1:59:08<00:46,  2.66it/s][INFO|trainer.py:2625] 2022-05-19 06:06:20,346 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:06:20,346 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:06:20,346 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.33it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.19it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.04it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.68it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.12it/s]\u001b[A\n",
            "                                           \n",
            " 99% 12276/12400 [1:59:11<00:46,  2.66it/s]\n",
            "100% 7/7 [00:01<00:00,  7.74it/s]\u001b[A\n",
            "                                 {'eval_loss': 2.694842576980591, 'eval_accuracy': 0.5388127853881278, 'eval_runtime': 3.2449, 'eval_samples_per_second': 67.491, 'eval_steps_per_second': 2.157, 'epoch': 198.0}\n",
            "\u001b[A[INFO|trainer.py:2345] 2022-05-19 06:06:23,593 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12276\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:06:23,596 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12276/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:06:25,289 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12276/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:06:25,289 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12276/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:06:29,230 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12152] due to args.save_total_limit\n",
            "                                           {'loss': 0.0684, 'learning_rate': 1.935483870967742e-06, 'epoch': 198.06}\n",
            "{'loss': 0.0575, 'learning_rate': 1.7741935483870968e-06, 'epoch': 198.23}\n",
            "{'loss': 0.0485, 'learning_rate': 1.6129032258064516e-06, 'epoch': 198.39}\n",
            "{'loss': 0.0317, 'learning_rate': 1.4516129032258066e-06, 'epoch': 198.55}\n",
            " 99% 12320/12400 [1:59:38<00:34,  2.35it/s]{'loss': 0.1148, 'learning_rate': 1.2903225806451614e-06, 'epoch': 198.71}\n",
            "                                           {'loss': 0.0728, 'learning_rate': 1.1290322580645162e-06, 'epoch': 198.87}\n",
            "100% 12338/12400 [1:59:45<00:24,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 06:06:57,774 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:06:57,774 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:06:57,774 >>   Batch size = 32\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.03it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  6.17it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.83it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  7.27it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.56it/s]\u001b[A\n",
            "                                           \n",
            "                                 {'eval_loss': 2.693960189819336, 'eval_accuracy': 0.5525114155251142, 'eval_runtime': 3.1291, 'eval_samples_per_second': 69.989, 'eval_steps_per_second': 2.237, 'epoch': 199.0}\n",
            "100% 12338/12400 [1:59:49<00:24,  2.57it/s]\n",
            "100% 7/7 [00:00<00:00,  8.10it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 06:07:00,905 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12338\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:07:00,909 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12338/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:07:02,595 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12338/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:07:02,596 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12338/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:07:06,384 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12214] due to args.save_total_limit\n",
            "{'loss': 0.0341, 'learning_rate': 9.67741935483871e-07, 'epoch': 199.03}\n",
            "100% 12350/12400 [2:00:02<00:25,  1.94it/s]{'loss': 0.1025, 'learning_rate': 8.064516129032258e-07, 'epoch': 199.19}\n",
            "                                           {'loss': 0.0746, 'learning_rate': 6.451612903225807e-07, 'epoch': 199.35}\n",
            "                                           {'loss': 0.0315, 'learning_rate': 4.838709677419355e-07, 'epoch': 199.52}\n",
            "{'loss': 0.0432, 'learning_rate': 3.2258064516129035e-07, 'epoch': 199.68}\n",
            "                                           {'loss': 0.0443, 'learning_rate': 1.6129032258064518e-07, 'epoch': 199.84}\n",
            "100% 12400/12400 [2:00:23<00:00,  2.57it/s][INFO|trainer.py:2625] 2022-05-19 06:07:34,989 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:07:34,989 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:07:34,989 >>   Batch size = 32\n",
            "{'loss': 0.1019, 'learning_rate': 0.0, 'epoch': 200.0}\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:00,  6.38it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:00,  5.35it/s]\u001b[A\n",
            " 57% 4/7 [00:00<00:00,  6.12it/s]\u001b[A\n",
            " 71% 5/7 [00:00<00:00,  6.73it/s]\u001b[A\n",
            " 86% 6/7 [00:00<00:00,  7.17it/s]\u001b[A\n",
            "                                           \n",
            "100% 12400/12400 [2:00:26<00:00,  2.57it/s]\n",
            "100% 7/7 [00:01<00:00,  7.73it/s]\u001b[A\n",
            "                                 \u001b[A{'eval_loss': 2.69675612449646, 'eval_accuracy': 0.5570776255707762, 'eval_runtime': 3.2941, 'eval_samples_per_second': 66.482, 'eval_steps_per_second': 2.125, 'epoch': 200.0}\n",
            "[INFO|trainer.py:2345] 2022-05-19 06:07:38,286 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12400\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:07:38,289 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12400/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:07:40,006 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12400/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:07:40,007 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12400/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 06:07:44,047 >> Deleting older checkpoint [orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-12276] due to args.save_total_limit\n",
            "[INFO|trainer.py:1671] 2022-05-19 06:07:44,174 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1744] 2022-05-19 06:07:44,174 >> Loading best model from orchid219_ft_pretrain_data2vec-vision-base-mae/checkpoint-1550 (score: 2.048746347427368).\n",
            "100% 12400/12400 [2:00:40<00:00,  2.57it/s]{'train_runtime': 7243.8149, 'train_samples_per_second': 54.419, 'train_steps_per_second': 1.712, 'train_loss': 0.5143826528999114, 'epoch': 200.0}\n",
            "100% 12400/12400 [2:00:40<00:00,  1.71it/s]\n",
            "[INFO|trainer.py:2345] 2022-05-19 06:07:51,956 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:07:51,959 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:07:54,306 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:07:54,307 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/preprocessor_config.json\n",
            "[INFO|trainer.py:2345] 2022-05-19 06:07:54,307 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:07:54,310 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:07:57,693 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:07:57,956 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/preprocessor_config.json\n",
            "Several commits (5) will be pushed upstream.\n",
            "WARNING:huggingface_hub.repository:Several commits (5) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n",
            "WARNING:huggingface_hub.repository:The progress bars may be unreliable.\n",
            "Upload file runs/May19_04-06-44_217e4d200061/events.out.tfevents.1652933228.217e4d200061.19131.0:   1% 3.34k/265k [00:00<?, ?B/s]\n",
            "Upload file runs/May19_04-06-44_217e4d200061/events.out.tfevents.1652933228.217e4d200061.19131.0:  90% 239k/265k [00:01<00:00, 241kB/s]\n",
            "Upload file pytorch_model.bin:   0% 239k/328M [00:01<23:43, 241kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   0% 1.13M/328M [00:02<08:47, 650kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   1% 2.02M/328M [00:03<07:18, 779kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   1% 2.91M/328M [00:04<06:46, 839kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   1% 3.85M/328M [00:05<06:21, 890kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   1% 4.73M/328M [00:06<06:15, 901kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   2% 5.61M/328M [00:07<06:12, 907kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   2% 6.54M/328M [00:08<06:02, 928kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   2% 7.50M/328M [00:09<05:53, 951kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   3% 8.44M/328M [00:10<05:47, 964kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   3% 9.36M/328M [00:11<05:47, 962kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   3% 10.3M/328M [00:12<05:45, 962kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   3% 11.2M/328M [00:13<05:45, 961kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   4% 12.1M/328M [00:14<05:49, 948kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   4% 13.0M/328M [00:15<05:49, 946kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   4% 13.9M/328M [00:16<05:47, 948kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   5% 14.8M/328M [00:17<05:48, 943kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   5% 15.7M/328M [00:18<05:48, 939kB/s]\u001b[A\n",
            "Upload file runs/May19_04-06-44_217e4d200061/events.out.tfevents.1652933228.217e4d200061.19131.0: 100% 265k/265k [00:19<00:00, 241kB/s]\n",
            "Upload file pytorch_model.bin:   5% 17.4M/328M [00:20<05:52, 922kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   6% 18.3M/328M [00:21<05:55, 913kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   6% 19.1M/328M [00:22<05:53, 915kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   6% 20.0M/328M [00:23<05:49, 924kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   6% 21.0M/328M [00:24<05:43, 937kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   7% 21.8M/328M [00:25<05:44, 932kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   7% 22.7M/328M [00:26<05:41, 936kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   7% 23.7M/328M [00:27<05:38, 942kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   7% 24.5M/328M [00:28<05:39, 936kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   8% 25.4M/328M [00:29<05:38, 936kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   8% 26.3M/328M [00:30<05:37, 936kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   8% 27.2M/328M [00:31<05:37, 934kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   9% 28.1M/328M [00:32<05:38, 928kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   9% 29.0M/328M [00:33<05:39, 922kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   9% 29.8M/328M [00:34<05:41, 914kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:   9% 30.7M/328M [00:35<05:43, 908kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  10% 31.5M/328M [00:36<05:44, 902kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  10% 32.4M/328M [00:37<05:35, 922kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  10% 33.4M/328M [00:38<05:31, 931kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  10% 34.3M/328M [00:39<05:25, 945kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  11% 35.2M/328M [00:40<05:22, 952kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  11% 36.2M/328M [00:41<05:17, 963kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  11% 37.1M/328M [00:42<05:14, 970kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  12% 38.0M/328M [00:43<05:12, 972kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  12% 39.0M/328M [00:44<05:08, 982kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  12% 39.9M/328M [00:45<05:07, 983kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  12% 40.9M/328M [00:46<05:05, 985kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  13% 41.8M/328M [00:47<05:06, 980kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  13% 42.7M/328M [00:48<05:04, 982kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  13% 43.7M/328M [00:49<05:04, 978kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  14% 44.6M/328M [00:50<05:04, 975kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  14% 45.5M/328M [00:51<05:06, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  14% 46.4M/328M [00:52<05:10, 952kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  14% 47.3M/328M [00:53<05:09, 951kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  15% 48.2M/328M [00:54<05:06, 957kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  15% 49.2M/328M [00:55<05:03, 963kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  15% 50.1M/328M [00:56<04:58, 974kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  16% 51.1M/328M [00:57<04:55, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  16% 52.0M/328M [00:58<04:52, 988kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  16% 53.0M/328M [00:59<04:52, 986kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  16% 53.9M/328M [01:00<04:52, 983kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  17% 54.8M/328M [01:01<04:52, 980kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  17% 55.8M/328M [01:02<04:49, 985kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  17% 56.8M/328M [01:03<04:44, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  18% 57.7M/328M [01:04<04:42, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  18% 58.7M/328M [01:05<04:41, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  18% 59.6M/328M [01:06<04:42, 995kB/s] \u001b[A\n",
            "Upload file pytorch_model.bin:  18% 60.6M/328M [01:07<04:37, 1.01MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  19% 61.6M/328M [01:08<04:33, 1.02MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  19% 62.6M/328M [01:09<04:33, 1.02MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  19% 63.5M/328M [01:10<04:36, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  20% 64.4M/328M [01:11<04:42, 979kB/s] \u001b[A\n",
            "Upload file pytorch_model.bin:  20% 65.3M/328M [01:12<04:42, 974kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  20% 66.2M/328M [01:13<04:42, 972kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  20% 67.2M/328M [01:14<04:42, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  21% 68.1M/328M [01:15<04:41, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  21% 69.0M/328M [01:16<04:41, 965kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  21% 69.9M/328M [01:17<04:38, 971kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  22% 70.9M/328M [01:18<04:38, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  22% 71.8M/328M [01:19<04:33, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  22% 72.8M/328M [01:20<04:30, 987kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  22% 73.7M/328M [01:21<04:32, 979kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  23% 74.7M/328M [01:22<04:29, 986kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  23% 75.6M/328M [01:23<04:28, 987kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  23% 76.5M/328M [01:24<04:32, 968kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  24% 77.4M/328M [01:25<04:35, 952kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  24% 78.2M/328M [01:26<04:36, 947kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  24% 79.2M/328M [01:27<04:35, 946kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  24% 80.0M/328M [01:28<04:36, 939kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  25% 80.9M/328M [01:29<04:39, 925kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  25% 81.8M/328M [01:30<04:39, 922kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  25% 82.6M/328M [01:31<04:40, 916kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  25% 83.5M/328M [01:32<04:40, 915kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  26% 84.4M/328M [01:33<04:37, 921kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  26% 85.3M/328M [01:34<04:36, 918kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  26% 86.2M/328M [01:35<04:31, 932kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  27% 87.1M/328M [01:36<04:28, 939kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  27% 88.0M/328M [01:37<04:28, 937kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  27% 88.9M/328M [01:38<04:28, 934kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  27% 89.8M/328M [01:39<04:24, 942kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  28% 90.7M/328M [01:40<04:23, 945kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  28% 91.6M/328M [01:41<04:21, 946kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  28% 92.5M/328M [01:42<04:19, 952kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  29% 93.5M/328M [01:43<04:12, 973kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  29% 94.5M/328M [01:44<04:07, 987kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  29% 95.4M/328M [01:45<04:10, 972kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  29% 96.3M/328M [01:46<04:11, 965kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  30% 97.2M/328M [01:47<04:10, 963kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  30% 98.1M/328M [01:48<04:12, 954kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  30% 99.0M/328M [01:49<04:11, 952kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  30% 99.9M/328M [01:50<04:15, 935kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  31% 101M/328M [01:51<04:20, 914kB/s] \u001b[A\n",
            "Upload file pytorch_model.bin:  31% 102M/328M [01:52<04:21, 906kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  31% 102M/328M [01:53<04:18, 915kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  32% 103M/328M [01:54<04:17, 913kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  32% 104M/328M [01:55<04:18, 909kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  32% 105M/328M [01:56<04:18, 903kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  32% 106M/328M [01:57<04:15, 909kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  33% 107M/328M [01:58<04:10, 923kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  33% 108M/328M [01:59<04:08, 928kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  33% 109M/328M [02:00<04:05, 937kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  33% 110M/328M [02:01<04:02, 945kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  34% 110M/328M [02:02<03:56, 964kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  34% 111M/328M [02:03<03:54, 968kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  34% 112M/328M [02:04<03:53, 968kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  35% 113M/328M [02:05<03:51, 973kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  35% 114M/328M [02:06<03:49, 977kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  35% 115M/328M [02:07<03:45, 990kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  35% 116M/328M [02:08<03:43, 992kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  36% 117M/328M [02:09<03:43, 990kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  36% 118M/328M [02:10<03:43, 984kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  36% 119M/328M [02:11<03:44, 977kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  37% 120M/328M [02:12<03:42, 978kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  37% 121M/328M [02:13<03:40, 984kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  37% 122M/328M [02:14<03:39, 983kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  37% 123M/328M [02:15<03:37, 990kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  38% 124M/328M [02:16<03:35, 992kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  38% 125M/328M [02:17<03:32, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  38% 126M/328M [02:18<03:30, 1.01MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  39% 127M/328M [02:19<03:30, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  39% 128M/328M [02:20<03:29, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  39% 129M/328M [02:21<03:29, 998kB/s] \u001b[A\n",
            "Upload file pytorch_model.bin:  39% 129M/328M [02:22<03:32, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  40% 130M/328M [02:23<03:34, 966kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  40% 131M/328M [02:24<03:34, 963kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  40% 132M/328M [02:25<03:31, 971kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  41% 133M/328M [02:26<03:32, 959kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  41% 134M/328M [02:27<03:30, 966kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  41% 135M/328M [02:28<03:29, 965kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  41% 136M/328M [02:29<03:28, 968kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  42% 137M/328M [02:30<03:25, 976kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  42% 138M/328M [02:31<03:25, 972kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  42% 139M/328M [02:32<03:22, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  43% 140M/328M [02:33<03:20, 982kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  43% 141M/328M [02:34<03:18, 990kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  43% 142M/328M [02:35<03:16, 996kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  43% 143M/328M [02:36<03:13, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  44% 143M/328M [02:37<03:12, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  44% 144M/328M [02:38<03:14, 991kB/s] \u001b[A\n",
            "Upload file pytorch_model.bin:  44% 145M/328M [02:39<03:14, 983kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  45% 146M/328M [02:40<03:14, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  45% 147M/328M [02:41<03:13, 978kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  45% 148M/328M [02:42<03:13, 974kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  45% 149M/328M [02:43<03:15, 962kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  46% 150M/328M [02:44<03:15, 954kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  46% 151M/328M [02:45<03:18, 936kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  46% 152M/328M [02:46<03:19, 924kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  47% 152M/328M [02:47<03:19, 921kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  47% 153M/328M [02:48<03:18, 923kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  47% 154M/328M [02:49<03:13, 939kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  47% 155M/328M [02:50<03:13, 937kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  48% 156M/328M [02:51<03:09, 948kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  48% 157M/328M [02:52<03:08, 951kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  48% 158M/328M [02:53<03:08, 947kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  48% 159M/328M [02:54<03:08, 940kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  49% 160M/328M [02:55<03:08, 936kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  49% 161M/328M [02:56<03:07, 935kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  49% 161M/328M [02:57<03:06, 936kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  50% 162M/328M [02:58<03:04, 939kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  50% 163M/328M [02:59<03:02, 944kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  50% 164M/328M [03:00<03:00, 951kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  50% 165M/328M [03:01<03:01, 942kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  51% 166M/328M [03:02<02:58, 950kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  51% 167M/328M [03:03<02:55, 960kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  51% 168M/328M [03:04<02:54, 963kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  52% 169M/328M [03:05<02:52, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  52% 170M/328M [03:06<02:51, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  52% 171M/328M [03:07<02:48, 976kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  52% 172M/328M [03:08<02:47, 975kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  53% 173M/328M [03:09<02:47, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  53% 173M/328M [03:10<02:46, 972kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  53% 174M/328M [03:11<02:46, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  53% 175M/328M [03:12<02:45, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  54% 176M/328M [03:13<02:43, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  54% 177M/328M [03:14<02:43, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  54% 178M/328M [03:15<02:40, 976kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  55% 179M/328M [03:16<02:39, 977kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  55% 180M/328M [03:17<02:37, 984kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  55% 181M/328M [03:18<02:36, 985kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  55% 182M/328M [03:19<02:36, 976kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  56% 183M/328M [03:20<02:36, 970kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  56% 184M/328M [03:21<02:36, 966kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  56% 185M/328M [03:22<02:34, 973kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  57% 186M/328M [03:23<02:31, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  57% 187M/328M [03:24<02:30, 984kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  57% 188M/328M [03:25<02:28, 988kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  57% 188M/328M [03:26<02:30, 971kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  58% 189M/328M [03:27<02:32, 953kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  58% 190M/328M [03:28<02:34, 933kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  58% 191M/328M [03:29<02:33, 932kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  59% 192M/328M [03:30<02:30, 947kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  59% 193M/328M [03:31<02:27, 956kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  59% 194M/328M [03:32<02:26, 957kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  59% 195M/328M [03:33<02:26, 951kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  60% 196M/328M [03:34<02:26, 948kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  60% 197M/328M [03:35<02:24, 954kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  60% 197M/328M [03:36<02:22, 962kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  61% 198M/328M [03:37<02:19, 970kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  61% 199M/328M [03:38<02:16, 984kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  61% 200M/328M [03:39<02:17, 972kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  61% 201M/328M [03:40<02:18, 956kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  62% 202M/328M [03:41<02:21, 934kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  62% 203M/328M [03:42<02:21, 925kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  62% 204M/328M [03:43<02:20, 927kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  62% 205M/328M [03:44<02:18, 931kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  63% 206M/328M [03:45<02:16, 942kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  63% 207M/328M [03:46<02:12, 962kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  63% 208M/328M [03:47<02:08, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  64% 208M/328M [03:48<02:06, 987kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  64% 209M/328M [03:49<02:07, 976kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  64% 210M/328M [03:50<02:04, 986kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  64% 211M/328M [03:51<02:04, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  65% 212M/328M [03:52<02:04, 976kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  65% 213M/328M [03:53<02:04, 970kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  65% 214M/328M [03:54<02:00, 990kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  66% 215M/328M [03:55<01:58, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  66% 216M/328M [03:56<01:57, 999kB/s] \u001b[A\n",
            "Upload file pytorch_model.bin:  66% 217M/328M [03:57<01:57, 993kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  66% 218M/328M [03:58<01:55, 996kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  67% 219M/328M [03:59<01:54, 997kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  67% 220M/328M [04:00<01:55, 984kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  67% 221M/328M [04:01<01:56, 966kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  68% 222M/328M [04:02<01:56, 957kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  68% 222M/328M [04:03<01:55, 955kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  68% 223M/328M [04:04<01:54, 953kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  68% 224M/328M [04:05<01:54, 946kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  69% 225M/328M [04:06<01:53, 949kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  69% 226M/328M [04:07<01:49, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  69% 227M/328M [04:08<01:48, 975kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  70% 228M/328M [04:09<01:46, 979kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  70% 229M/328M [04:10<01:45, 985kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  70% 230M/328M [04:11<01:44, 979kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  70% 231M/328M [04:12<01:44, 977kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  71% 232M/328M [04:13<01:43, 976kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  71% 233M/328M [04:14<01:42, 973kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  71% 234M/328M [04:15<01:41, 970kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  72% 235M/328M [04:16<01:40, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  72% 235M/328M [04:17<01:39, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  72% 236M/328M [04:18<01:38, 977kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  72% 237M/328M [04:19<01:34, 998kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  73% 238M/328M [04:20<01:33, 998kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  73% 239M/328M [04:21<01:32, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  73% 240M/328M [04:22<01:31, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  74% 241M/328M [04:23<01:30, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  74% 242M/328M [04:24<01:30, 993kB/s] \u001b[A\n",
            "Upload file pytorch_model.bin:  74% 243M/328M [04:25<01:30, 979kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  74% 244M/328M [04:26<01:30, 972kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  75% 245M/328M [04:27<01:31, 955kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  75% 246M/328M [04:28<01:31, 942kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  75% 247M/328M [04:29<01:30, 941kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  76% 248M/328M [04:30<01:29, 945kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  76% 248M/328M [04:31<01:27, 952kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  76% 249M/328M [04:32<01:25, 957kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  76% 250M/328M [04:33<01:24, 957kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  77% 251M/328M [04:34<01:23, 960kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  77% 252M/328M [04:35<01:22, 958kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  77% 253M/328M [04:36<01:21, 963kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  78% 254M/328M [04:37<01:19, 971kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  78% 255M/328M [04:38<01:17, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  78% 256M/328M [04:39<01:15, 993kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  78% 257M/328M [04:40<01:14, 999kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  79% 258M/328M [04:41<01:12, 1.01MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  79% 259M/328M [04:42<01:11, 1.01MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  79% 260M/328M [04:43<01:09, 1.02MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  80% 261M/328M [04:44<01:09, 1.01MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  80% 262M/328M [04:45<01:07, 1.02MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  80% 263M/328M [04:46<01:06, 1.02MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  80% 264M/328M [04:47<01:06, 1.01MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  81% 265M/328M [04:48<01:05, 1.02MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  81% 266M/328M [04:49<01:05, 1.00MB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  81% 267M/328M [04:50<01:05, 983kB/s] \u001b[A\n",
            "Upload file pytorch_model.bin:  82% 267M/328M [04:51<01:05, 963kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  82% 268M/328M [04:52<01:05, 958kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  82% 269M/328M [04:53<01:04, 953kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  82% 270M/328M [04:54<01:04, 937kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  83% 271M/328M [04:55<01:04, 923kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  83% 272M/328M [04:56<01:04, 904kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  83% 273M/328M [04:57<01:04, 900kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  83% 274M/328M [04:58<01:03, 903kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  84% 274M/328M [04:59<01:01, 909kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  84% 275M/328M [05:00<00:59, 924kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  84% 276M/328M [05:01<00:57, 933kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  85% 277M/328M [05:02<00:56, 943kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  85% 278M/328M [05:03<00:54, 954kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  85% 279M/328M [05:04<00:53, 962kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  85% 280M/328M [05:05<00:51, 965kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  86% 281M/328M [05:06<00:50, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  86% 282M/328M [05:07<00:49, 980kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  86% 283M/328M [05:08<00:48, 978kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  87% 284M/328M [05:09<00:46, 987kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  87% 285M/328M [05:10<00:46, 982kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  87% 286M/328M [05:11<00:45, 970kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  87% 286M/328M [05:12<00:45, 962kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  88% 287M/328M [05:13<00:44, 957kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  88% 288M/328M [05:14<00:43, 956kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  88% 289M/328M [05:15<00:42, 957kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  89% 290M/328M [05:16<00:41, 959kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  89% 291M/328M [05:17<00:39, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  89% 292M/328M [05:18<00:39, 960kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  89% 293M/328M [05:19<00:38, 959kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  90% 294M/328M [05:20<00:37, 957kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  90% 295M/328M [05:21<00:36, 948kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  90% 296M/328M [05:22<00:35, 943kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  90% 296M/328M [05:23<00:34, 940kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  91% 297M/328M [05:24<00:33, 950kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  91% 298M/328M [05:25<00:32, 949kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  91% 299M/328M [05:26<00:31, 952kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  92% 300M/328M [05:27<00:30, 950kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  92% 301M/328M [05:28<00:29, 949kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  92% 302M/328M [05:29<00:28, 954kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  92% 303M/328M [05:30<00:27, 956kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  93% 304M/328M [05:31<00:26, 966kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  93% 305M/328M [05:32<00:24, 974kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  93% 306M/328M [05:33<00:23, 981kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  94% 307M/328M [05:34<00:22, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  94% 308M/328M [05:35<00:22, 965kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  94% 308M/328M [05:36<00:20, 970kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  94% 309M/328M [05:37<00:20, 962kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  95% 310M/328M [05:38<00:19, 951kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  95% 311M/328M [05:39<00:18, 946kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  95% 312M/328M [05:40<00:17, 938kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  95% 313M/328M [05:41<00:16, 923kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  96% 314M/328M [05:42<00:16, 919kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  96% 315M/328M [05:43<00:14, 935kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  96% 316M/328M [05:44<00:13, 938kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  97% 316M/328M [05:45<00:12, 939kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  97% 317M/328M [05:46<00:11, 954kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  97% 318M/328M [05:47<00:10, 964kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  97% 319M/328M [05:48<00:09, 966kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  98% 320M/328M [05:49<00:08, 969kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  98% 321M/328M [05:50<00:07, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  98% 322M/328M [05:51<00:06, 967kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  99% 323M/328M [05:52<00:05, 963kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  99% 324M/328M [05:53<00:04, 951kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  99% 325M/328M [05:54<00:03, 943kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin:  99% 326M/328M [05:55<00:02, 943kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin: 100% 327M/328M [05:56<00:01, 948kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin: 100% 327M/328M [05:57<00:00, 943kB/s]\u001b[Aremote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "   72c3ed6..30793c9  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "   72c3ed6..30793c9  main -> main\n",
            "\n",
            "Upload file runs/May19_04-06-44_217e4d200061/events.out.tfevents.1652933228.217e4d200061.19131.0: 100% 265k/265k [05:59<00:00, 744B/s]\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 328M/328M [05:59<00:00, 593kB/s]\u001b[A\n",
            "Upload file pytorch_model.bin: 100% 328M/328M [05:59<00:00, 955kB/s]\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "   30793c9..380c16a  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "   30793c9..380c16a  main -> main\n",
            "\n",
            "***** train metrics *****\n",
            "  epoch                    =      200.0\n",
            "  train_loss               =     0.5144\n",
            "  train_runtime            = 2:00:43.81\n",
            "  train_samples_per_second =     54.419\n",
            "  train_steps_per_second   =      1.712\n",
            "[INFO|trainer.py:2625] 2022-05-19 06:14:19,540 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 06:14:19,540 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 06:14:19,540 >>   Batch size = 32\n",
            "100% 7/7 [00:00<00:00,  8.17it/s]***** eval metrics *****\n",
            "  epoch                   =      200.0\n",
            "  eval_accuracy           =     0.4429\n",
            "  eval_loss               =     2.0487\n",
            "  eval_runtime            = 0:00:03.28\n",
            "  eval_samples_per_second =     66.669\n",
            "  eval_steps_per_second   =      2.131\n",
            "100% 7/7 [00:00<00:00,  7.13it/s]\n",
            "[INFO|trainer.py:2345] 2022-05-19 06:14:22,873 >> Saving model checkpoint to orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 06:14:22,877 >> Configuration saved in orchid219_ft_pretrain_data2vec-vision-base-mae/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 06:14:24,845 >> Model weights saved in orchid219_ft_pretrain_data2vec-vision-base-mae/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 06:14:24,846 >> Feature extractor saved in orchid219_ft_pretrain_data2vec-vision-base-mae/preprocessor_config.json\n",
            "Upload file pytorch_model.bin:   0% 3.34k/328M [00:00<?, ?B/s]\n",
            "Upload file pytorch_model.bin: 100% 327M/328M [06:03<00:00, 1.01MB/s]remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "   380c16a..b341452  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "   380c16a..b341452  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 328M/328M [06:05<00:00, 940kB/s]\n",
            "\n",
            "Upload file runs/May19_04-06-44_217e4d200061/events.out.tfevents.1652940862.217e4d200061.19131.2: 100% 363/363 [06:05<?, ?B/s]\u001b[A\n",
            "Upload file runs/May19_04-06-44_217e4d200061/events.out.tfevents.1652940862.217e4d200061.19131.2: 100% 363/363 [06:05<?, ?B/s]\n",
            "remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "   b341452..91e6f1d  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:remote: Enforcing permissions...        \n",
            "remote: Allowed refs: all        \n",
            "To https://huggingface.co/gary109/orchid219_ft_pretrain_data2vec-vision-base-mae\n",
            "   b341452..91e6f1d  main -> main\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy ▁▃▄▅▆▇▇▆▇▇▆▇▇▇▇█▇▇▇▇█▇▇▇▇▇▇█▇▇▇▇███▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss █▄▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▃▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▃▂▃▁▂▂▄▃▃▅▅▄▄▅▄▄▄▃▄▃▃▃▃▅▃▃▃▄▄▃▄▅▆▆▆▅▆▇▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▆▇▆█▇▇▄▆▅▄▄▄▅▄▅▄▅▆▅▅▆▅▆▃▆▆▆▅▅▅▅▄▂▂▃▃▃▂▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▆▇▆█▇▇▄▆▅▄▄▄▅▄▅▄▅▆▅▅▆▅▆▃▆▆▆▅▅▅▅▄▂▂▃▃▃▂▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  eval/accuracy 0.44292\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 2.04875\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 3.2849\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 66.669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 2.131\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 200.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 12400\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 3.0559296685949174e+19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.51438\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 7243.8149\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 54.419\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 1.712\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33morchid219_ft_pretrain_data2vec-vision-base-mae\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/gary109/huggingface/runs/1rlyymxz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220519_040708-1rlyymxz/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gary109/orchid219_ft_data2vec-vision-large\n",
        "---"
      ],
      "metadata": {
        "id": "aogw2ll1JOZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"facebook/data2vec-vision-large\" \\\n",
        "    --output_dir=\"orchid219_ft_data2vec-vision-large\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train --do_eval --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_ft_data2vec-vision-large\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 400 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \n",
        "\n",
        "# --gradient_accumulation_steps 8 \\\n",
        "# --gradient_checkpointing"
      ],
      "metadata": {
        "id": "5oHZ5KVYJOZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gary109/orchid219_ft_data2vec-vision-base-ft1k \n",
        "---"
      ],
      "metadata": {
        "id": "-dooC6Sb7pvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification_data2vec.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"facebook/data2vec-vision-base-ft1k\" \\\n",
        "    --output_dir=\"orchid219_ft_data2vec-vision-base-ft1k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train --do_eval --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_ft_data2vec-vision-base-ft1k\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 400 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \n",
        "\n",
        "# --gradient_accumulation_steps 8 \\\n",
        "# --gradient_checkpointing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "176hLBOi7pvf",
        "outputId": "b440ba2c-63c0-4f7d-c168-cf39be27fce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO|trainer.py:2453] 2022-05-19 12:54:00,352 >> Deleting older checkpoint [orchid219_ft_data2vec-vision-base-ft1k/checkpoint-5270] due to args.save_total_limit\n",
            " 43% 5366/12400 [1:57:07<5:31:02,  2.82s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gary109/orchid219_ft_data2vec-vision-large-ft1k \n",
        "---"
      ],
      "metadata": {
        "id": "HSEy5xLu-sgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch run_image_classification_data2vec.py \\\n",
        "    --dataset_name \"gary109/orchid219\" \\\n",
        "    --model_name_or_path \"facebook/data2vec-vision-large-ft1k\" \\\n",
        "    --output_dir=\"orchid219_ft_data2vec-vision-large-ft1k\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train --do_eval --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"orchid219_ft_data2vec-vision-large-ft1k\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 400 \\\n",
        "    --per_device_train_batch_size 32 \\\n",
        "    --per_device_eval_batch_size 32 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \n",
        "\n",
        "# --gradient_accumulation_steps 8 \\\n",
        "# --gradient_checkpointing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f1a5a0-6aba-4056-f23b-77d680e8d89b",
        "id": "b-MBzH-L-sg-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.25it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.51it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 5.367639064788818, 'eval_accuracy': 0.0136986301369863, 'eval_runtime': 4.1237, 'eval_samples_per_second': 53.107, 'eval_steps_per_second': 0.97, 'epoch': 2.0}\n",
            "  0% 62/12400 [03:02<2:44:24,  1.25it/s]\n",
            "100% 4/4 [00:00<00:00,  7.19it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 11:00:00,368 >> Saving model checkpoint to orchid219_ft_data2vec-vision-base-ft1k/checkpoint-62\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 11:00:00,371 >> Configuration saved in orchid219_ft_data2vec-vision-base-ft1k/checkpoint-62/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 11:00:02,091 >> Model weights saved in orchid219_ft_data2vec-vision-base-ft1k/checkpoint-62/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 11:00:02,092 >> Feature extractor saved in orchid219_ft_data2vec-vision-base-ft1k/checkpoint-62/preprocessor_config.json\n",
            "                                        {'loss': 5.3631, 'learning_rate': 1.9887096774193552e-05, 'epoch': 2.26}\n",
            "{'loss': 5.3588, 'learning_rate': 1.9870967741935484e-05, 'epoch': 2.58}\n",
            "                                        {'loss': 5.3545, 'learning_rate': 1.9854838709677423e-05, 'epoch': 2.9}\n",
            "  1% 93/12400 [03:36<2:46:16,  1.23it/s][INFO|trainer.py:2625] 2022-05-19 11:00:35,474 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 11:00:35,474 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 11:00:35,474 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.34it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.57it/s]\u001b[A\n",
            "                                        \n",
            "{'eval_loss': 5.347695350646973, 'eval_accuracy': 0.0319634703196347, 'eval_runtime': 4.1575, 'eval_samples_per_second': 52.676, 'eval_steps_per_second': 0.962, 'epoch': 3.0}\n",
            "  1% 93/12400 [03:41<2:46:16,  1.23it/s]\n",
            "100% 4/4 [00:00<00:00,  7.25it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 11:00:39,634 >> Saving model checkpoint to orchid219_ft_data2vec-vision-base-ft1k/checkpoint-93\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 11:00:39,639 >> Configuration saved in orchid219_ft_data2vec-vision-base-ft1k/checkpoint-93/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 11:00:41,370 >> Model weights saved in orchid219_ft_data2vec-vision-base-ft1k/checkpoint-93/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 11:00:41,371 >> Feature extractor saved in orchid219_ft_data2vec-vision-base-ft1k/checkpoint-93/preprocessor_config.json\n",
            "{'loss': 5.3484, 'learning_rate': 1.9838709677419358e-05, 'epoch': 3.23}\n",
            "  1% 110/12400 [04:05<2:58:20,  1.15it/s]{'loss': 5.3357, 'learning_rate': 1.982258064516129e-05, 'epoch': 3.55}\n",
            "  1% 120/12400 [04:13<2:46:22,  1.23it/s]{'loss': 5.3325, 'learning_rate': 1.980645161290323e-05, 'epoch': 3.87}\n",
            "  1% 124/12400 [04:15<2:44:52,  1.24it/s][INFO|trainer.py:2625] 2022-05-19 11:01:14,785 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2627] 2022-05-19 11:01:14,785 >>   Num examples = 219\n",
            "[INFO|trainer.py:2630] 2022-05-19 11:01:14,786 >>   Batch size = 64\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:00<00:00,  9.13it/s]\u001b[A\n",
            " 75% 3/4 [00:00<00:00,  6.47it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 5.329884052276611, 'eval_accuracy': 0.0547945205479452, 'eval_runtime': 4.0809, 'eval_samples_per_second': 53.664, 'eval_steps_per_second': 0.98, 'epoch': 4.0}\n",
            "  1% 124/12400 [04:20<2:44:52,  1.24it/s]\n",
            "100% 4/4 [00:00<00:00,  7.01it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2345] 2022-05-19 11:01:18,868 >> Saving model checkpoint to orchid219_ft_data2vec-vision-base-ft1k/checkpoint-124\n",
            "[INFO|configuration_utils.py:446] 2022-05-19 11:01:18,872 >> Configuration saved in orchid219_ft_data2vec-vision-base-ft1k/checkpoint-124/config.json\n",
            "[INFO|modeling_utils.py:1546] 2022-05-19 11:01:20,605 >> Model weights saved in orchid219_ft_data2vec-vision-base-ft1k/checkpoint-124/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-19 11:01:20,606 >> Feature extractor saved in orchid219_ft_data2vec-vision-base-ft1k/checkpoint-124/preprocessor_config.json\n",
            "[INFO|trainer.py:2453] 2022-05-19 11:01:24,479 >> Deleting older checkpoint [orchid219_ft_data2vec-vision-base-ft1k/checkpoint-31] due to args.save_total_limit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nMaBzIBLWWs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 製作 Public Test Datasets with Datasets"
      ],
      "metadata": {
        "id": "5l6FGTXVYA6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "tCaeOQ4PfwO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget -O /content/drive/MyDrive/datasets/Orchid219/Public_Test/Public_Test.zip http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip /content/drive/MyDrive/datasets/Orchid219/Public_Test/Public_Test.zip -d /content/drive/MyDrive/datasets/Orchid219/Public_Test/"
      ],
      "metadata": {
        "id": "TAdNczP0zwGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# # dataset = load_dataset(\"/content/ai-cup-2022-crop_classification/datasets/crop14.py\", 'crop14-small')\n",
        "# dataset = load_dataset(\"/content/crop14.py\", 'crop14-balance')\n",
        "# dataset = load_dataset(\"gary109/crop14_balance\", use_auth_token=True)\n",
        "# dataset = load_dataset(\"gary109/crop14-small\", use_auth_token=True)\n",
        "# dataset = load_dataset(\"gary109/crop14-pretrain\", use_auth_token=True, cache_dir='/content/drive/MyDrive/datasets/crop14-pretrain')\n",
        "# dataset = load_dataset(\"gary109/crop14_balance\", use_auth_token=True, cache_dir='/content/drive/MyDrive/datasets/cache_crop14-balance')\n",
        "# dataset = load_dataset(\"STAS_colab.py\",\"public-test\", use_auth_token=True, cache_dir='/content/drive/MyDrive/datasets/cache_STAS')\n",
        "dataset = load_dataset(\"orchid219.py\",\"public-test\", use_auth_token=True, cache_dir='/content/drive/MyDrive/datasets/cache_orchid219_public_test')\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "fab48cb629a64f208ddd24a8c474ec5f",
            "44f50a1d2a004a6bb902c14d81fbd8a0",
            "93e8a6547d734774a36cd913b0f6fdb0",
            "cccbb8b7ebf34225ab9a755ea6a7a03a",
            "1083296862674595b8df6fdc8935d6e7",
            "dbe6cb98578c4063925dcd24ca324780",
            "6734fa5fc09b4b3fb33a7a4aa1932eb9",
            "deaa1644f6cc48608a2aa5c8b8afa042",
            "d1061773125c40ac9e96b91cf7f57a4e",
            "35ff918111534bec94451232df24d865",
            "93b1f128307b41cdb8521604fee47749"
          ]
        },
        "id": "3mN4Btl-X_fZ",
        "outputId": "6d2fa65c-3181-4c15-df60-034148bb3222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset orchid219 (/content/drive/MyDrive/datasets/cache_orchid219_public_test/orchid219/public-test/1.0.0/8f8444a00f455cca182e267fafef70db843b3dd0d3ddb264f27c2accbf34d75e)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fab48cb629a64f208ddd24a8c474ec5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    test: Dataset({\n",
              "        features: ['filename', 'image'],\n",
              "        num_rows: 19\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"gary109/orchid219_public-test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "e4700aa07706413d83aa8df13b595fe2",
            "85b37051b722460fb7383bba9742eca8",
            "731922d9715e41ce91c138e3d807e597",
            "440937b6a4ed48c9afa80e6f6e2986e4",
            "71634fbe99ff482dbd047e29f0a29e5e",
            "0f46d8305ba243f6a244fd20ab968689",
            "92f9f4f265a0433da167d3cda5d05472",
            "e8128bfc884e4d5ab91410d89e3bff65",
            "059c6d26c3374d50ac30fd82083d2f7d",
            "458c6977e6bf40b6ba5560637e53d3ce",
            "80762b77e4834fb7946329cd1d2d2d7c",
            "8322dfac04394d27ba63467f8a523bfe",
            "c3b723bcb0004d56be9f6aea022c086c",
            "be7080e2feff46bf8aeaf0445ab182c6",
            "fd610c20fe35496cbd36f92d653fbff9",
            "5931c1d8c6ed463f83f71554d81942ca",
            "cc82eac98ac949d7957181cf23bff4e0",
            "374237e61278446188a0a3a7d0edb43e",
            "45ec73e18be142a18aa2bc51a3aa5889",
            "711d3a19242644ce8116a951c8027840",
            "43cf40feb1af4e46adb68bb1fb8547d8",
            "ee059f6125074996b5ac1facdd447f8b"
          ]
        },
        "id": "yn73Ba0Medmw",
        "outputId": "b1df2891-6010-47cf-d040-56786c063978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pushing split test to the Hub.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4700aa07706413d83aa8df13b595fe2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8322dfac04394d27ba63467f8a523bfe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 預測 Public Test Dataset Results using Fine-Tune Models\n",
        "---\n",
        "- gary109/orchid219_ft_vit-base-patch16-224-in21k\n",
        "- gary109/orchid219_ft_vit-large-patch16-224-in21k\n",
        "- gary109/orchid219_ft_vit-huge-patch14-224-in21k\n",
        "- gary109/orchid219_ft_data2vec-vision-base\n",
        "- gary109/orchid219_ft_vit-base-patch32-224-in21k\n",
        "- gary109/orchid219_ft_vit-large-patch32-224-in21k\n",
        "- gary109/orchid219_ft_vit-base-mae\n",
        "- gary109/orchid219_ft_vit-mae-large"
      ],
      "metadata": {
        "id": "p3nqwLgWbKU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification, BeitFeatureExtractor, Data2VecVisionForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "ft_models = [\n",
        "            #  'gary109/orchid219_ft_vit-base-patch16-224-in21k',\n",
        "            #  'gary109/orchid219_ft_vit-large-patch16-224-in21k',\n",
        "            #  'gary109/orchid219_ft_vit-huge-patch14-224-in21k'\n",
        "             'gary109/orchid219_ft_data2vec-vision-base',\n",
        "            #  'gary109/orchid219_ft_vit-base-patch32-224-in21k',\n",
        "            #  'gary109/orchid219_ft_vit-large-patch32-224-in21k',\n",
        "             'gary109/orchid219_ft_vit-base-mae',\n",
        "            #  'gary109/orchid219_ft_vit-mae-large'\n",
        "]\n",
        "\n",
        "root_results = './results/'\n",
        "Path(root_results).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for m in ft_models:\n",
        "    print(f'Using {m} ===> Predicting ...')\n",
        "    if 'data2vec-vision-base' in m:\n",
        "        feature_extractor = BeitFeatureExtractor.from_pretrained(m)\n",
        "        model = Data2VecVisionForImageClassification.from_pretrained(m)\n",
        "    else:\n",
        "        feature_extractor = ViTFeatureExtractor.from_pretrained(m, use_auth_token=True)\n",
        "        model = ViTForImageClassification.from_pretrained(m, use_auth_token=True)\n",
        "\n",
        "    \n",
        "    file_list = []\n",
        "    label_list = []\n",
        "    for ds in tqdm(dataset['test']):\n",
        "        # print(ds['filename'])\n",
        "        file_list.append(os.path.basename(ds['filename']))\n",
        "\n",
        "        inputs = feature_extractor(images=ds['image'], return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        # model predicts one of the 219 Orchid classes\n",
        "        predicted_class_idx = logits.argmax(-1).item()\n",
        "        # print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
        "        label_list.append(model.config.id2label[predicted_class_idx])\n",
        "\n",
        "    # Generate CSV file\n",
        "    df = pd.DataFrame()\n",
        "    modelName = os.path.basename(m)\n",
        "    dateStr = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "    csv_path = os.path.join(root_results, f'{modelName}_{dateStr}.csv')\n",
        "    df['file'] = file_list\n",
        "    df['label'] = label_list\n",
        "    df.to_csv(csv_path, index=False, encoding=\"utf8\")\n",
        "    del model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "c3ece1c021ee4045acdbbdd26caf0eeb",
            "b477b885f26a44fc9ff0c8a5d7bb3206",
            "b32f4272f1f04d76b99d4ca7dbacfba2",
            "2027317bc21d49968c816c2c4f909533",
            "daccf9a3441d4d2d8c17a0e7f3f4d1a8",
            "cda33a487c0b4ed89460c880e53a8a24",
            "c899aa6007b04e66b3aa4a2b49916670",
            "f3365dfe4c154249b72bfc615c31f3ba",
            "13acb81c371a412291490ca73d7f640b",
            "5ca173fadc2b4f84ae55d360e4de607e",
            "a1478aa9dc1148369e2b2457856159f2",
            "dac47f2197b948ae9f1426bb7df19a8b",
            "be02ea367c3540c6bc0ff27244aedb60",
            "6aa3b7e8557d43a5b35f988addfa229d",
            "aef697538da441948968508353f85e72",
            "d27b42c5ed344829a46b8a55eb2b1dd3",
            "bc01fd7a1b754e73a9d28ce4bd34ff0b",
            "3272340a2de740a6b80dc70bc2c6f645",
            "8923b23b2f59411cb5df57f011aed7d1",
            "94e56179352444eb9a757fba5ee7c8de",
            "c90dc962ec1f4be4b2475395e1fb874d",
            "d6987f0f33824b60865495cf653e7804",
            "cab36b8cc28248fba430fde50f7f13fd",
            "3b30a3a31e05416bb43ceb38e9520b7d",
            "d6d6bf67d4ae4c0cbadb4b1bbb5a4624",
            "c5927cbe3e3c4ac9ad1d0bf77feacf09",
            "2c82a67d16d740efba36cf967ba6c736",
            "237644cfc20141708406aefc32cb9e64",
            "0513eddefae941ea99a3f0d42cf4ac99",
            "7987a41e0f3742389b6ae77570094b80",
            "0ddbb449c7e441efb3122e3db8698906",
            "6a6d1b864f184162a62c6ec59a97ea79",
            "c92176ba1ee142cca9d3ff2d5eb3f0c7",
            "b59fa663bdf542d3ab7444d174c130c3",
            "da42b1b09ad7485abfe6a74032177097",
            "bbd4b2e9a96846be8b5b6c6ab8f148b1",
            "37e0f188836b4ccead445f559289dd1c",
            "aabfeb44239349eca112e237d0127aec",
            "123763be6ef244f4af84a7d49a72d7f2",
            "cc7b3d15ee5146139ff47cf821c0a334",
            "a4e7f729be8c485791c96596cd4f55d0",
            "06fea7c460614a86969568ad087a2100",
            "fb1344571cd647488c419d750e8b0b66",
            "da724f5356fe4990871c79d6c95b7c6a",
            "02bd2ab7886b495b99fa1919452be503",
            "e23c150ac0894b2eb4f6878aef099069",
            "e709c0aeb7c44f389384d043c6c04324",
            "b18f324f0c4145bd9296b5513052f03a",
            "69097a583c884e95801d4a2b4a235d08",
            "cbde0163c3cf4bea9b0793ff741a6e4b",
            "25aadd10c3584c1cb89f554f7a351381",
            "5859a0c8b93d4b1c916720c246e0bbdd",
            "89371b7e8c8a42f5b56848a15c072733",
            "8e7e5754037e485ca134cb9afd973629",
            "0906d8334594404a8ca897140569599f",
            "1d7219d9c4b44de78cb62af940ac6192",
            "457090bdf30b4ccf8528225947d512e6",
            "aeb00f75f9cc493da565eb4e204b9e06",
            "71ab873c5753406bb69445b01105c882",
            "b3a9288db84441fcbe4ab14bc38647e1",
            "6673fb5450bf43168bdcddbab70bd8ae",
            "b1b678e912084dc58970cb4e7fa912f8",
            "22649f76d0c248f691a184425623039a",
            "93ce7a1bbeba489cbcf76f03d1ceb563",
            "796ed23619af43dfb57c231ee06469e3",
            "5d68aa66b3604c0ba5ef89feba16e4b8"
          ]
        },
        "id": "QTFdgRvxWb5L",
        "outputId": "33db8362-3a51-4e98-b32c-df4ab0def6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using gary109/orchid219_ft_data2vec-vision-base ===> Predicting ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/302 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3ece1c021ee4045acdbbdd26caf0eeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/8.05k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dac47f2197b948ae9f1426bb7df19a8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/328M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cab36b8cc28248fba430fde50f7f13fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "100%|██████████| 19/19 [00:09<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using gary109/orchid219_ft_vit-base-mae ===> Predicting ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/228 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b59fa663bdf542d3ab7444d174c130c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/7.71k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02bd2ab7886b495b99fa1919452be503"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type vit_mae to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/328M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d7219d9c4b44de78cb62af940ac6192"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:08<00:00,  2.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Git Push"
      ],
      "metadata": {
        "id": "r8lwxM4JXSa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git add .\n",
        "! git commit -am \"submit function ready\"\n",
        "! git push"
      ],
      "metadata": {
        "id": "scYjAZcmrjWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00568a15-928a-448c-a521-32877d991328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 845c131] submit function ready\n",
            " 4 files changed, 80 insertions(+)\n",
            " create mode 100644 results/orchid219_ft_data2vec-vision-base_2022-05-10-04-40-44.csv\n",
            " create mode 100644 results/orchid219_ft_vit-base-mae_2022-05-10-04-41-02.csv\n",
            " create mode 100644 results/orchid219_ft_vit-base-patch16-224-in21k_2022-05-10-04-35-31.csv\n",
            " create mode 100644 results/orchid219_ft_vit-large-patch16-224-in21k_2022-05-10-04-36-36.csv\n",
            "Counting objects: 6, done.\n",
            "Delta compression using up to 4 threads.\n",
            "Compressing objects: 100% (6/6), done.\n",
            "Writing objects: 100% (6/6), 962 bytes | 962.00 KiB/s, done.\n",
            "Total 6 (delta 2), reused 0 (delta 0)\n",
            "To https://gitlab.com/gary109/orchid219_classification.git\n",
            "   d3f3779..845c131  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pyFA4ahMrjZ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}